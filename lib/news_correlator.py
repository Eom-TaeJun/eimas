"""
News Correlator - ì´ìƒ íƒì§€ ì‹œê°„ ê¸°ë°˜ ë‰´ìŠ¤ ê²€ìƒ‰ ë° ê·€ì¸

ì´ìƒ íƒì§€ â†’ ì‹œê°„ í´ëŸ¬ìŠ¤í„°ë§ â†’ ë‰´ìŠ¤ ê²€ìƒ‰ â†’ ì›ì¸ ê·€ì¸
"""

import os
import json
import sqlite3
from datetime import datetime, timedelta
from typing import Optional
from dataclasses import dataclass, field
from openai import OpenAI


@dataclass
class Anomaly:
    """ë‹¨ì¼ ì´ìƒ íƒì§€ ê¸°ë¡"""
    timestamp: datetime  # UTC
    asset: str
    anomaly_type: str  # volume_explosion, volatility_spike, price_shock
    value: float
    details: dict = field(default_factory=dict)


@dataclass
class AnomalyCluster:
    """ì‹œê°„ì ìœ¼ë¡œ ê·¼ì ‘í•œ ì´ìƒë“¤ì˜ í´ëŸ¬ìŠ¤í„°"""
    cluster_id: str
    start_time: datetime
    end_time: datetime
    anomalies: list[Anomaly]
    severity_score: float = 0.0
    affected_assets: list[str] = field(default_factory=list)

    def __post_init__(self):
        self.affected_assets = list(set(a.asset for a in self.anomalies))
        self._calculate_severity()

    def _calculate_severity(self):
        """ì‹¬ê°ë„ ì ìˆ˜ ê³„ì‚°"""
        score = 0.0
        for a in self.anomalies:
            if a.anomaly_type == 'volume_explosion':
                score += min(a.value / 5.0, 3.0)  # 5ë°° = 1ì , ìµœëŒ€ 3ì 
            elif a.anomaly_type == 'volatility_spike':
                score += min(a.value / 2.0, 3.0)  # 2Ïƒ = 1ì , ìµœëŒ€ 3ì 
            elif a.anomaly_type == 'price_shock':
                score += min(abs(a.value) / 2.0, 3.0)  # 2% = 1ì , ìµœëŒ€ 3ì 

        # ë‹¤ì¤‘ ìì‚° ë³´ë„ˆìŠ¤
        score *= (1 + 0.2 * (len(self.affected_assets) - 1))
        self.severity_score = round(score, 2)


@dataclass
class NewsResult:
    """ë‰´ìŠ¤ ê²€ìƒ‰ ê²°ê³¼"""
    headline: str
    source: str
    language: str
    search_query: str
    relevance_score: float = 0.0


@dataclass
class EventAttribution:
    """ì´ìƒ-ë‰´ìŠ¤ ê·€ì¸ ê²°ê³¼"""
    cluster: AnomalyCluster
    news: list[NewsResult]
    confidence_score: float
    summary: str


class NewsCorrelator:
    """ì´ìƒ íƒì§€ì™€ ë‰´ìŠ¤ë¥¼ ì‹œê°„ ê¸°ë°˜ìœ¼ë¡œ ì—°ê²°"""

    # êµ­ê°€ë³„ í‚¤ì›Œë“œ ë§¤í•‘
    COUNTRY_KEYWORDS = {
        'ko': ['korea', 'korean', 'samsung', 'kospi', 'kosdaq', 'hyundai', 'sk', 'seoul',
               'north korea', 'pyongyang', 'kim jong'],
        'zh': ['china', 'chinese', 'taiwan', 'xi jinping', 'beijing', 'shanghai',
               'hong kong', 'alibaba', 'tencent', 'csi', 'hang seng'],
        'ja': ['japan', 'japanese', 'nikkei', 'tokyo', 'yen', 'boj', 'kishida',
               'sony', 'toyota', 'softbank'],
        'de': ['germany', 'german', 'dax', 'bundesbank', 'ecb', 'frankfurt'],
        'es': ['venezuela', 'maduro', 'mexico', 'brazil', 'latin america', 'peso']
    }

    # ì–¸ì–´ë³„ ê²€ìƒ‰ ì¿¼ë¦¬ í…œí”Œë¦¿
    LANGUAGE_TEMPLATES = {
        'en': {
            'market': '{asset} market news {date}',
            'breaking': 'breaking news financial markets {date}',
            'geopolitical': 'geopolitical news world events {date}'
        },
        'ko': {
            'market': '{asset} ì‹œì¥ ë‰´ìŠ¤ {date}',
            'breaking': 'ì†ë³´ ê¸ˆìœµì‹œì¥ {date}',
            'geopolitical': 'êµ­ì œ ì •ì„¸ ë‰´ìŠ¤ {date}'
        },
        'zh': {
            'market': '{asset} å¸‚åœºæ–°é—» {date}',
            'breaking': 'çªå‘æ–°é—» é‡‘èå¸‚åœº {date}',
            'geopolitical': 'å›½é™…å±€åŠ¿ æ–°é—» {date}'
        },
        'ja': {
            'market': '{asset} å¸‚å ´ãƒ‹ãƒ¥ãƒ¼ã‚¹ {date}',
            'breaking': 'é€Ÿå ± é‡‘èå¸‚å ´ {date}',
            'geopolitical': 'å›½éš›æƒ…å‹¢ ãƒ‹ãƒ¥ãƒ¼ã‚¹ {date}'
        }
    }

    # ìì‚°ë³„ í‘œì‹œ ì´ë¦„ (ê²€ìƒ‰ìš©)
    ASSET_NAMES = {
        'BTC': {'en': 'Bitcoin', 'ko': 'ë¹„íŠ¸ì½”ì¸', 'zh': 'æ¯”ç‰¹å¸', 'ja': 'ãƒ“ãƒƒãƒˆã‚³ã‚¤ãƒ³'},
        'ETH': {'en': 'Ethereum', 'ko': 'ì´ë”ë¦¬ì›€', 'zh': 'ä»¥å¤ªåŠ', 'ja': 'ã‚¤ãƒ¼ã‚µãƒªã‚¢ãƒ '},
        'SPY': {'en': 'S&P 500', 'ko': 'S&P 500', 'zh': 'æ ‡æ™®500', 'ja': 'S&P 500'},
        'QQQ': {'en': 'Nasdaq', 'ko': 'ë‚˜ìŠ¤ë‹¥', 'zh': 'çº³æ–¯è¾¾å…‹', 'ja': 'ãƒŠã‚¹ãƒ€ãƒƒã‚¯'},
        'GLD': {'en': 'Gold', 'ko': 'ê¸ˆ', 'zh': 'é»„é‡‘', 'ja': 'é‡‘'},
        'CL=F': {'en': 'Oil', 'ko': 'ì›ìœ ', 'zh': 'åŸæ²¹', 'ja': 'åŸæ²¹'},
        'GC=F': {'en': 'Gold futures', 'ko': 'ê¸ˆ ì„ ë¬¼', 'zh': 'é»„é‡‘æœŸè´§', 'ja': 'é‡‘å…ˆç‰©'},
        'DX-Y.NYB': {'en': 'US Dollar', 'ko': 'ë‹¬ëŸ¬', 'zh': 'ç¾å…ƒ', 'ja': 'ãƒ‰ãƒ«'},
    }

    # ì‹¬ê°ë„ ì„ê³„ê°’ (ì´ ì´ìƒì´ì–´ì•¼ ë‰´ìŠ¤ ê²€ìƒ‰)
    SEVERITY_THRESHOLD = 1.5

    # í´ëŸ¬ìŠ¤í„°ë§ ìœˆë„ìš° (ë¶„)
    CLUSTER_WINDOW_MINUTES = 30

    def __init__(self, db_path: str = None):
        self.db_path = db_path or os.path.join(
            os.path.dirname(os.path.dirname(__file__)),
            'data', 'volatile', 'realtime.db'
        )

        # Perplexity API í´ë¼ì´ì–¸íŠ¸
        api_key = os.getenv('PERPLEXITY_API_KEY')
        if api_key:
            self.perplexity_client = OpenAI(
                api_key=api_key,
                base_url="https://api.perplexity.ai"
            )
        else:
            self.perplexity_client = None
            print("âš ï¸ PERPLEXITY_API_KEY not set")

        self._init_db()

    def _init_db(self):
        """ê·€ì¸ ê²°ê³¼ ì €ì¥ í…Œì´ë¸” ìƒì„±"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()

        cursor.execute('''
            CREATE TABLE IF NOT EXISTS event_attribution (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                cluster_id TEXT UNIQUE,
                anomaly_start TEXT,
                anomaly_end TEXT,
                affected_assets TEXT,
                severity_score REAL,
                news_results TEXT,
                confidence_score REAL,
                summary TEXT,
                languages_searched TEXT,
                created_at TEXT DEFAULT CURRENT_TIMESTAMP
            )
        ''')

        # ê²€ìƒ‰ ìºì‹œ í…Œì´ë¸” (ì¤‘ë³µ ê²€ìƒ‰ ë°©ì§€)
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS search_cache (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                search_key TEXT UNIQUE,
                result TEXT,
                searched_at TEXT DEFAULT CURRENT_TIMESTAMP
            )
        ''')

        conn.commit()
        conn.close()

    def cluster_anomalies(self, anomalies: list[Anomaly]) -> list[AnomalyCluster]:
        """ì‹œê°„ì ìœ¼ë¡œ ê·¼ì ‘í•œ ì´ìƒë“¤ì„ í´ëŸ¬ìŠ¤í„°ë§"""
        if not anomalies:
            return []

        # ì‹œê°„ìˆœ ì •ë ¬
        sorted_anomalies = sorted(anomalies, key=lambda x: x.timestamp)

        clusters = []
        current_cluster = [sorted_anomalies[0]]

        for anomaly in sorted_anomalies[1:]:
            # ì´ì „ ì´ìƒê³¼ì˜ ì‹œê°„ ì°¨ì´
            time_diff = (anomaly.timestamp - current_cluster[-1].timestamp).total_seconds() / 60

            if time_diff <= self.CLUSTER_WINDOW_MINUTES:
                current_cluster.append(anomaly)
            else:
                # ìƒˆ í´ëŸ¬ìŠ¤í„° ì‹œì‘
                if current_cluster:
                    cluster_id = f"cluster_{current_cluster[0].timestamp.strftime('%Y%m%d_%H%M')}"
                    clusters.append(AnomalyCluster(
                        cluster_id=cluster_id,
                        start_time=current_cluster[0].timestamp,
                        end_time=current_cluster[-1].timestamp,
                        anomalies=current_cluster
                    ))
                current_cluster = [anomaly]

        # ë§ˆì§€ë§‰ í´ëŸ¬ìŠ¤í„°
        if current_cluster:
            cluster_id = f"cluster_{current_cluster[0].timestamp.strftime('%Y%m%d_%H%M')}"
            clusters.append(AnomalyCluster(
                cluster_id=cluster_id,
                start_time=current_cluster[0].timestamp,
                end_time=current_cluster[-1].timestamp,
                anomalies=current_cluster
            ))

        return clusters

    def detect_relevant_languages(self, news_text: str) -> list[str]:
        """ë‰´ìŠ¤ í…ìŠ¤íŠ¸ì—ì„œ ê´€ë ¨ êµ­ê°€/ì–¸ì–´ ê°ì§€"""
        languages = ['en']  # ì˜ì–´ëŠ” í•­ìƒ í¬í•¨
        text_lower = news_text.lower()

        for lang, keywords in self.COUNTRY_KEYWORDS.items():
            for keyword in keywords:
                if keyword in text_lower:
                    if lang not in languages:
                        languages.append(lang)
                    break

        return languages

    def _get_cache_key(self, query: str, time_window: str) -> str:
        """ìºì‹œ í‚¤ ìƒì„±"""
        return f"{query}_{time_window}"

    def _check_cache(self, cache_key: str) -> Optional[str]:
        """ìºì‹œ í™•ì¸"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        cursor.execute(
            'SELECT result FROM search_cache WHERE search_key = ?',
            (cache_key,)
        )
        row = cursor.fetchone()
        conn.close()
        return row[0] if row else None

    def _save_cache(self, cache_key: str, result: str):
        """ìºì‹œ ì €ì¥"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        cursor.execute(
            'INSERT OR REPLACE INTO search_cache (search_key, result) VALUES (?, ?)',
            (cache_key, result)
        )
        conn.commit()
        conn.close()

    def search_news(self, query: str, time_window: str, language: str = 'en') -> Optional[str]:
        """Perplexity APIë¡œ ë‰´ìŠ¤ ê²€ìƒ‰"""
        if not self.perplexity_client:
            return None

        cache_key = self._get_cache_key(query, time_window)
        cached = self._check_cache(cache_key)
        if cached:
            print(f"  ğŸ“¦ ìºì‹œ ì‚¬ìš©: {query[:50]}...")
            return cached

        try:
            # ì–¸ì–´ë³„ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
            system_prompts = {
                'en': "You are a financial news analyst. Find and summarize breaking news related to the query. Focus on events that could impact financial markets. Be concise.",
                'ko': "ë‹¹ì‹ ì€ ê¸ˆìœµ ë‰´ìŠ¤ ë¶„ì„ê°€ì…ë‹ˆë‹¤. ì¿¼ë¦¬ì™€ ê´€ë ¨ëœ ì†ë³´ë¥¼ ì°¾ì•„ ìš”ì•½í•˜ì„¸ìš”. ê¸ˆìœµ ì‹œì¥ì— ì˜í–¥ì„ ì¤„ ìˆ˜ ìˆëŠ” ì´ë²¤íŠ¸ì— ì§‘ì¤‘í•˜ì„¸ìš”. ê°„ê²°í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”.",
                'zh': "ä½ æ˜¯ä¸€åé‡‘èæ–°é—»åˆ†æå¸ˆã€‚æŸ¥æ‰¾å¹¶æ€»ç»“ä¸æŸ¥è¯¢ç›¸å…³çš„çªå‘æ–°é—»ã€‚å…³æ³¨å¯èƒ½å½±å“é‡‘èå¸‚åœºçš„äº‹ä»¶ã€‚è¯·ç®€æ´ã€‚",
                'ja': "ã‚ãªãŸã¯é‡‘èãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚¢ãƒŠãƒªã‚¹ãƒˆã§ã™ã€‚ã‚¯ã‚¨ãƒªã«é–¢é€£ã™ã‚‹ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’è¦‹ã¤ã‘ã¦è¦ç´„ã—ã¦ãã ã•ã„ã€‚é‡‘èå¸‚å ´ã«å½±éŸ¿ã‚’ä¸ãˆã‚‹å¯èƒ½æ€§ã®ã‚ã‚‹ã‚¤ãƒ™ãƒ³ãƒˆã«ç„¦ç‚¹ã‚’å½“ã¦ã¦ãã ã•ã„ã€‚ç°¡æ½”ã«ã€‚"
            }

            response = self.perplexity_client.chat.completions.create(
                model="sonar",
                messages=[
                    {"role": "system", "content": system_prompts.get(language, system_prompts['en'])},
                    {"role": "user", "content": f"{query}\n\nTime window: {time_window}"}
                ],
                max_tokens=500
            )

            result = response.choices[0].message.content
            self._save_cache(cache_key, result)
            return result

        except Exception as e:
            print(f"  âŒ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return None

    def correlate_cluster(self, cluster: AnomalyCluster) -> Optional[EventAttribution]:
        """í´ëŸ¬ìŠ¤í„°ì— ëŒ€í•´ ë‰´ìŠ¤ ê²€ìƒ‰ ë° ê·€ì¸"""

        if cluster.severity_score < self.SEVERITY_THRESHOLD:
            print(f"  â­ï¸ ì‹¬ê°ë„ ë¶€ì¡± ({cluster.severity_score} < {self.SEVERITY_THRESHOLD})")
            return None

        print(f"\nğŸ” í´ëŸ¬ìŠ¤í„° ë¶„ì„: {cluster.cluster_id}")
        print(f"   ì‹œê°„: {cluster.start_time} ~ {cluster.end_time} UTC")
        print(f"   ìì‚°: {cluster.affected_assets}")
        print(f"   ì‹¬ê°ë„: {cluster.severity_score}")

        # ê²€ìƒ‰ ì‹œê°„ ìœˆë„ìš° (ë¹„ëŒ€ì¹­: ì „ 1ì‹œê°„ ~ í›„ 3ì‹œê°„)
        search_start = cluster.start_time - timedelta(hours=1)
        search_end = cluster.end_time + timedelta(hours=3)
        time_window = f"{search_start.strftime('%Y-%m-%d %H:%M')} to {search_end.strftime('%Y-%m-%d %H:%M')} UTC"
        date_str = cluster.start_time.strftime('%Y-%m-%d')

        news_results = []
        languages_searched = []

        # Phase 1: ì˜ì–´ë¡œ ê¸€ë¡œë²Œ ê°œìš” ê²€ìƒ‰
        print("\n  ğŸ“° Phase 1: ì˜ì–´ ê¸€ë¡œë²Œ ê²€ìƒ‰")

        # ìì‚° íŠ¹ì • ê²€ìƒ‰
        for asset in cluster.affected_assets[:3]:  # ìµœëŒ€ 3ê°œ ìì‚°
            asset_name = self.ASSET_NAMES.get(asset, {}).get('en', asset)
            query = f"{asset_name} market news breaking {date_str}"
            result = self.search_news(query, time_window, 'en')
            if result:
                news_results.append(NewsResult(
                    headline=result[:200],
                    source='perplexity',
                    language='en',
                    search_query=query
                ))

        # ì§€ì •í•™ì  ê²€ìƒ‰
        query = f"breaking news geopolitical events financial markets {date_str}"
        result = self.search_news(query, time_window, 'en')
        if result:
            news_results.append(NewsResult(
                headline=result[:200],
                source='perplexity',
                language='en',
                search_query=query
            ))

        languages_searched.append('en')

        # Phase 2: ê´€ë ¨ êµ­ê°€ ê°ì§€ ë° í•´ë‹¹ ì–¸ì–´ë¡œ ìƒì„¸ ê²€ìƒ‰
        combined_news = ' '.join(n.headline for n in news_results)
        relevant_langs = self.detect_relevant_languages(combined_news)

        for lang in relevant_langs:
            if lang == 'en':
                continue

            print(f"\n  ğŸŒ Phase 2: {lang.upper()} ìƒì„¸ ê²€ìƒ‰")

            templates = self.LANGUAGE_TEMPLATES.get(lang, self.LANGUAGE_TEMPLATES['en'])

            # ì‹œì¥ ë‰´ìŠ¤ ê²€ìƒ‰
            for asset in cluster.affected_assets[:2]:
                asset_name = self.ASSET_NAMES.get(asset, {}).get(lang, asset)
                query = templates['market'].format(asset=asset_name, date=date_str)
                result = self.search_news(query, time_window, lang)
                if result:
                    news_results.append(NewsResult(
                        headline=result[:200],
                        source='perplexity',
                        language=lang,
                        search_query=query
                    ))

            # ì§€ì •í•™ ê²€ìƒ‰
            query = templates['geopolitical'].format(date=date_str)
            result = self.search_news(query, time_window, lang)
            if result:
                news_results.append(NewsResult(
                    headline=result[:200],
                    source='perplexity',
                    language=lang,
                    search_query=query
                ))

            languages_searched.append(lang)

        # ì‹ ë¢°ë„ ê³„ì‚°
        confidence = self._calculate_confidence(cluster, news_results)

        # ìš”ì•½ ìƒì„±
        summary = self._generate_summary(cluster, news_results)

        attribution = EventAttribution(
            cluster=cluster,
            news=news_results,
            confidence_score=confidence,
            summary=summary
        )

        # DB ì €ì¥
        self._save_attribution(attribution, languages_searched)

        return attribution

    def _calculate_confidence(self, cluster: AnomalyCluster, news: list[NewsResult]) -> float:
        """ê·€ì¸ ì‹ ë¢°ë„ ê³„ì‚°"""
        if not news:
            return 0.0

        # ê¸°ë³¸ ì ìˆ˜: ë‰´ìŠ¤ ê°œìˆ˜
        score = min(len(news) * 0.15, 0.6)

        # ë‹¤êµ­ì–´ ë³´ë„ˆìŠ¤
        languages = set(n.language for n in news)
        score += len(languages) * 0.1

        # ì‹¬ê°ë„ ë³´ë„ˆìŠ¤
        score += min(cluster.severity_score * 0.05, 0.2)

        return min(round(score, 2), 1.0)

    def _generate_summary(self, cluster: AnomalyCluster, news: list[NewsResult]) -> str:
        """ê·€ì¸ ê²°ê³¼ ìš”ì•½ ìƒì„±"""
        if not news:
            return "ë‰´ìŠ¤ë¥¼ ì°¾ì§€ ëª»í•¨"

        assets = ', '.join(cluster.affected_assets)
        time_str = cluster.start_time.strftime('%Y-%m-%d %H:%M UTC')

        summary_parts = [
            f"[{time_str}] {assets} ì´ìƒ ê°ì§€",
            f"ì‹¬ê°ë„: {cluster.severity_score}",
            f"ê´€ë ¨ ë‰´ìŠ¤ {len(news)}ê±´ ë°œê²¬:",
        ]

        for i, n in enumerate(news[:3], 1):
            summary_parts.append(f"  {i}. [{n.language.upper()}] {n.headline[:100]}...")

        return '\n'.join(summary_parts)

    def _save_attribution(self, attr: EventAttribution, languages: list[str]):
        """ê·€ì¸ ê²°ê³¼ DB ì €ì¥"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()

        cursor.execute('''
            INSERT OR REPLACE INTO event_attribution
            (cluster_id, anomaly_start, anomaly_end, affected_assets,
             severity_score, news_results, confidence_score, summary, languages_searched)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
        ''', (
            attr.cluster.cluster_id,
            attr.cluster.start_time.isoformat(),
            attr.cluster.end_time.isoformat(),
            json.dumps(attr.cluster.affected_assets),
            attr.cluster.severity_score,
            json.dumps([{
                'headline': n.headline,
                'source': n.source,
                'language': n.language,
                'query': n.search_query
            } for n in attr.news]),
            attr.confidence_score,
            attr.summary,
            json.dumps(languages)
        ))

        conn.commit()
        conn.close()
        print(f"  ğŸ’¾ ì €ì¥ë¨: {attr.cluster.cluster_id}")

    def process_recent_anomalies(self, hours_back: int = 24) -> list[EventAttribution]:
        """ìµœê·¼ ì´ìƒë“¤ì„ ì²˜ë¦¬í•˜ê³  ë‰´ìŠ¤ì™€ ì—°ê²°"""

        # volatile DBì—ì„œ ìµœê·¼ ì´ìƒ ë¡œë“œ
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()

        cutoff = (datetime.utcnow() - timedelta(hours=hours_back)).isoformat()

        cursor.execute('''
            SELECT timestamp, ticker, event_type, value, metadata_json
            FROM detected_events
            WHERE timestamp > ?
            ORDER BY timestamp
        ''', (cutoff,))

        rows = cursor.fetchall()
        conn.close()

        if not rows:
            print("ìµœê·¼ ì´ìƒ ì—†ìŒ")
            return []

        # Anomaly ê°ì²´ë¡œ ë³€í™˜
        anomalies = []
        for row in rows:
            try:
                anomalies.append(Anomaly(
                    timestamp=datetime.fromisoformat(row[0]),
                    asset=row[1],
                    anomaly_type=row[2],
                    value=float(row[3]) if row[3] else 0,
                    details=json.loads(row[4]) if row[4] else {}
                ))
            except Exception as e:
                print(f"  âš ï¸ íŒŒì‹± ì˜¤ë¥˜: {e}")

        print(f"\nğŸ“Š {len(anomalies)}ê°œ ì´ìƒ ë¡œë“œë¨")

        # í´ëŸ¬ìŠ¤í„°ë§
        clusters = self.cluster_anomalies(anomalies)
        print(f"ğŸ“¦ {len(clusters)}ê°œ í´ëŸ¬ìŠ¤í„° ìƒì„±")

        # ê° í´ëŸ¬ìŠ¤í„°ì— ëŒ€í•´ ë‰´ìŠ¤ ê²€ìƒ‰
        attributions = []
        for cluster in clusters:
            attr = self.correlate_cluster(cluster)
            if attr:
                attributions.append(attr)

        return attributions

    def generate_report(self, attributions: list[EventAttribution]) -> str:
        """ê·€ì¸ ê²°ê³¼ ë¦¬í¬íŠ¸ ìƒì„±"""
        if not attributions:
            return "ê·€ì¸ ê²°ê³¼ ì—†ìŒ"

        lines = [
            "# ì´ìƒ íƒì§€-ë‰´ìŠ¤ ê·€ì¸ ë¦¬í¬íŠ¸",
            f"> ìƒì„±: {datetime.now().strftime('%Y-%m-%d %H:%M')}",
            "",
            "---",
            ""
        ]

        for i, attr in enumerate(attributions, 1):
            lines.extend([
                f"## {i}. {attr.cluster.cluster_id}",
                "",
                f"**ì‹œê°„**: {attr.cluster.start_time} ~ {attr.cluster.end_time} UTC",
                f"**ìì‚°**: {', '.join(attr.cluster.affected_assets)}",
                f"**ì‹¬ê°ë„**: {attr.cluster.severity_score}",
                f"**ì‹ ë¢°ë„**: {attr.confidence_score}",
                "",
                "### ê´€ë ¨ ë‰´ìŠ¤",
                ""
            ])

            for j, news in enumerate(attr.news, 1):
                lines.append(f"{j}. [{news.language.upper()}] {news.headline}")

            lines.extend(["", "---", ""])

        return '\n'.join(lines)


# ì£¼ë§ìš© ì¶”ê°€ ìì‚° ìˆ˜ì§‘ê¸°
class WeekendAssetCollector:
    """ì¼ìš”ì¼ ì €ë…ë¶€í„° ê±°ë˜ë˜ëŠ” ì„ ë¬¼/FX ìˆ˜ì§‘"""

    WEEKEND_ASSETS = {
        'CL=F': 'WTI Crude Oil Futures',
        'GC=F': 'Gold Futures',
        'SI=F': 'Silver Futures',
        'DX-Y.NYB': 'US Dollar Index',
        'EURUSD=X': 'EUR/USD',
        'USDJPY=X': 'USD/JPY',
        'GBPUSD=X': 'GBP/USD',
    }

    # ì´ìƒ íƒì§€ ì„ê³„ê°’
    THRESHOLDS = {
        'price_change_pct': 1.5,  # 1.5% ì´ìƒ ë³€ë™
        'volume_ratio': 3.0,      # í‰ê·  ëŒ€ë¹„ 3ë°° ì´ìƒ
    }

    def __init__(self):
        import yfinance as yf
        self.yf = yf

    def collect_and_detect(self) -> list[Anomaly]:
        """ì£¼ë§ ìì‚° ìˆ˜ì§‘ ë° ì´ìƒ íƒì§€"""
        anomalies = []

        for symbol, name in self.WEEKEND_ASSETS.items():
            try:
                ticker = self.yf.Ticker(symbol)

                # ìµœê·¼ 5ì¼ ë°ì´í„°
                hist = ticker.history(period='5d', interval='1h')
                if hist.empty:
                    continue

                latest = hist.iloc[-1]
                prev_close = hist['Close'].iloc[-2] if len(hist) > 1 else latest['Close']

                # ê°€ê²© ë³€í™”ìœ¨
                pct_change = ((latest['Close'] - prev_close) / prev_close) * 100

                # ê±°ë˜ëŸ‰ ë¹„ìœ¨
                avg_volume = hist['Volume'].mean()
                volume_ratio = latest['Volume'] / avg_volume if avg_volume > 0 else 0

                # ì´ìƒ íƒì§€
                if abs(pct_change) >= self.THRESHOLDS['price_change_pct']:
                    anomalies.append(Anomaly(
                        timestamp=datetime.utcnow(),
                        asset=symbol,
                        anomaly_type='price_shock',
                        value=pct_change,
                        details={'name': name, 'close': latest['Close']}
                    ))
                    print(f"  ğŸš¨ {symbol}: ê°€ê²© ë³€ë™ {pct_change:+.2f}%")

                if volume_ratio >= self.THRESHOLDS['volume_ratio']:
                    anomalies.append(Anomaly(
                        timestamp=datetime.utcnow(),
                        asset=symbol,
                        anomaly_type='volume_explosion',
                        value=volume_ratio,
                        details={'name': name, 'volume': latest['Volume']}
                    ))
                    print(f"  ğŸš¨ {symbol}: ê±°ë˜ëŸ‰ {volume_ratio:.1f}ë°°")

            except Exception as e:
                print(f"  âš ï¸ {symbol} ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")

        return anomalies


if __name__ == '__main__':
    print("=" * 60)
    print("News Correlator - ì´ìƒ íƒì§€-ë‰´ìŠ¤ ê·€ì¸ ì‹œìŠ¤í…œ")
    print("=" * 60)

    # í…ŒìŠ¤íŠ¸: ìƒ˜í”Œ ì´ìƒ ìƒì„±
    test_anomalies = [
        Anomaly(
            timestamp=datetime(2026, 1, 3, 6, 15),
            asset='BTC',
            anomaly_type='volume_explosion',
            value=9.2
        ),
        Anomaly(
            timestamp=datetime(2026, 1, 3, 6, 30),
            asset='ETH',
            anomaly_type='volume_explosion',
            value=12.5
        ),
        Anomaly(
            timestamp=datetime(2026, 1, 3, 7, 0),
            asset='BTC',
            anomaly_type='volatility_spike',
            value=6.4
        ),
    ]

    correlator = NewsCorrelator()

    # í´ëŸ¬ìŠ¤í„°ë§
    clusters = correlator.cluster_anomalies(test_anomalies)
    print(f"\ní´ëŸ¬ìŠ¤í„° {len(clusters)}ê°œ ìƒì„±")

    for cluster in clusters:
        print(f"\ní´ëŸ¬ìŠ¤í„°: {cluster.cluster_id}")
        print(f"  ì‹œê°„: {cluster.start_time} ~ {cluster.end_time}")
        print(f"  ìì‚°: {cluster.affected_assets}")
        print(f"  ì‹¬ê°ë„: {cluster.severity_score}")

        # ë‰´ìŠ¤ ê²€ìƒ‰ (API í‚¤ ìˆìœ¼ë©´)
        if correlator.perplexity_client:
            attr = correlator.correlate_cluster(cluster)
            if attr:
                print(f"\n{attr.summary}")
