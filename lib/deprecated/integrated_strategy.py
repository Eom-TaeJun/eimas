"""
Integrated Strategy Engine
===========================
Graph-Clustered Portfolio + Shock Propagationì„ í†µí•©í•œ íˆ¬ì ì „ëµ ì—”ì§„

ê²½ì œí•™ì  ì² í•™:
- Whitebox AI: ì„¤ëª… ê°€ëŠ¥í•œ ì¸ê³¼ê´€ê³„ ê¸°ë°˜ ì „ëµ
- Volume > Price: ê±°ë˜ëŸ‰ ê¸‰ì¦ = ì •ë³´ ë¹„ëŒ€ì¹­ ì‹ í˜¸
- M = B + SÂ·B*: í™•ì¥ëœ ìœ ë™ì„± ê³µì‹ ê³ ë ¤
- Impulse Response: ì¶©ê²© ì „íŒŒ ê²½ë¡œ ê¸°ë°˜ í—¤ì§€

í•µì‹¬ ê¸°ëŠ¥:
1. Leading Indicator Tilt: ì„ í–‰ì§€í‘œì— ê°€ì¤‘ì¹˜ ë¶€ì—¬
2. Shock Early Warning: ìƒìœ„ ë ˆì´ì–´ ì¶©ê²© ê°ì§€ â†’ í•˜ìœ„ ë ˆì´ì–´ ê²½ê³ 
3. Causal Risk Budget: ì¸ê³¼ê´€ê³„ ê¸°ë°˜ ë¦¬ìŠ¤í¬ ë°°ë¶„
4. Volume Anomaly Detection: ì •ë³´ ë¹„ëŒ€ì¹­ ì‹ í˜¸ íƒì§€
"""

import numpy as np
import pandas as pd
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass, field, asdict
from datetime import datetime
from enum import Enum
import warnings

warnings.filterwarnings('ignore')

# Local imports
from lib.graph_clustered_portfolio import (
    GraphClusteredPortfolio,
    PortfolioAllocation,
    ClusteringMethod,
    RepresentativeMethod
)
from lib.shock_propagation_graph import (
    ShockPropagationGraph,
    PropagationAnalysis,
    NodeLayer,
    get_node_layer,
    ShockPath
)


class SignalType(Enum):
    """ì‹œê·¸ë„ ìœ í˜•"""
    LEADING_TILT = "leading_tilt"       # ì„ í–‰ì§€í‘œ ê¸°ë°˜ í‹¸íŒ…
    SHOCK_WARNING = "shock_warning"     # ì¶©ê²© ì „íŒŒ ê²½ê³ 
    VOLUME_SPIKE = "volume_spike"       # ê±°ë˜ëŸ‰ ê¸‰ì¦
    REGIME_SHIFT = "regime_shift"       # ë ˆì§ ë³€í™”
    REBALANCE = "rebalance"             # ë¦¬ë°¸ëŸ°ì‹± í•„ìš”


class ActionType(Enum):
    """í–‰ë™ ìœ í˜•"""
    BUY = "buy"
    SELL = "sell"
    HOLD = "hold"
    HEDGE = "hedge"
    REDUCE = "reduce"
    INCREASE = "increase"


@dataclass
class Signal:
    """íˆ¬ì ì‹œê·¸ë„"""
    timestamp: str
    signal_type: SignalType
    source: str                    # ì‹œê·¸ë„ ë°œìƒ ì†ŒìŠ¤
    affected_assets: List[str]     # ì˜í–¥ ë°›ëŠ” ìì‚°
    confidence: float              # ì‹ ë¢°ë„ (0-1)
    urgency: str                   # "HIGH", "MEDIUM", "LOW"
    description: str
    action_suggested: ActionType
    metadata: Dict = field(default_factory=dict)


@dataclass
class StrategyRecommendation:
    """ì „ëµ ê¶Œê³ """
    timestamp: str
    portfolio_weights: Dict[str, float]
    tilted_weights: Dict[str, float]     # í‹¸íŒ… ì ìš© í›„ ê°€ì¤‘ì¹˜
    tilt_factors: Dict[str, float]       # ìì‚°ë³„ í‹¸íŒ… íŒ©í„°
    signals: List[Signal]
    risk_metrics: Dict[str, float]

    # ê²½ì œí•™ì  í•´ì„
    leading_exposure: float              # ì„ í–‰ì§€í‘œ ë…¸ì¶œë„
    lagging_exposure: float              # í›„í–‰ì§€í‘œ ë…¸ì¶œë„
    shock_vulnerability: float           # ì¶©ê²© ì·¨ì•½ë„

    # ì‹¤í–‰ ê°€ì´ë“œ
    actions: List[Dict]
    warnings: List[str]

    def to_dict(self) -> Dict:
        result = asdict(self)
        result['signals'] = [asdict(s) for s in self.signals]
        return result


@dataclass
class VolumeAnomaly:
    """ê±°ë˜ëŸ‰ ì´ìƒì¹˜"""
    asset: str
    timestamp: str
    volume: float
    volume_ma20: float
    surge_ratio: float             # volume / MA20
    interpretation: str            # "NEW_INFORMATION", "EXHAUSTION", "MANIPULATION"
    confidence: float


# ============================================================================
# Integrated Strategy Engine
# ============================================================================

class IntegratedStrategy:
    """
    í†µí•© íˆ¬ì ì „ëµ ì—”ì§„

    Foundation (GC-HRP) + Intelligence (SPG) = Application
    """

    def __init__(
        self,
        # Portfolio params
        correlation_threshold: float = 0.3,
        clustering_method: ClusteringMethod = ClusteringMethod.KMEANS,

        # Causality params
        significance_level: float = 0.05,
        max_lag: int = 20,

        # Strategy params
        leading_tilt_factor: float = 0.15,      # ì„ í–‰ì§€í‘œ í‹¸íŒ… ê°•ë„
        volume_surge_threshold: float = 3.0,    # ê±°ë˜ëŸ‰ ê¸‰ì¦ ì„ê³„ê°’ (MA20 ëŒ€ë¹„)
        shock_window: int = 5                   # ì¶©ê²© ê°ì§€ ìœˆë„ìš° (ì¼)
    ):
        # Portfolio engine
        self.portfolio_engine = GraphClusteredPortfolio(
            correlation_threshold=correlation_threshold,
            clustering_method=clustering_method,
            representative_method=RepresentativeMethod.CENTRALITY
        )

        # Causality engine
        self.causality_engine = ShockPropagationGraph(
            significance_level=significance_level,
            max_lag=max_lag,
            enforce_layer_order=True
        )

        # Strategy params
        self.leading_tilt_factor = leading_tilt_factor
        self.volume_surge_threshold = volume_surge_threshold
        self.shock_window = shock_window

        # Results
        self.portfolio_allocation: Optional[PortfolioAllocation] = None
        self.causality_analysis: Optional[PropagationAnalysis] = None
        self.signals: List[Signal] = []

    def fit(
        self,
        returns: pd.DataFrame,
        macro_data: pd.DataFrame,
        volumes: Optional[pd.DataFrame] = None
    ) -> StrategyRecommendation:
        """
        ì „ëµ ìˆ˜ë¦½

        Args:
            returns: ìì‚° ìˆ˜ìµë¥ 
            macro_data: ê±°ì‹œì§€í‘œ ë°ì´í„° (Fed Funds, VIX, etc.)
            volumes: ê±°ë˜ëŸ‰ ë°ì´í„°

        Returns:
            StrategyRecommendation
        """
        print("[Strategy] Starting integrated strategy...")
        self.signals = []

        # Step 1: í¬íŠ¸í´ë¦¬ì˜¤ êµ¬ì¶• (Foundation)
        print("[Strategy] Step 1: Building portfolio allocation...")
        self.portfolio_allocation = self.portfolio_engine.fit(returns, volumes)
        base_weights = self.portfolio_allocation.weights

        # Step 2: ì¸ê³¼ê´€ê³„ ë¶„ì„ (Intelligence)
        print("[Strategy] Step 2: Analyzing causal relationships...")
        self.causality_analysis = self.causality_engine.run_full_analysis(macro_data)

        # Step 3: ì„ í–‰ì§€í‘œ í‹¸íŒ…
        print("[Strategy] Step 3: Calculating leading indicator tilt...")
        tilt_factors = self._calculate_leading_tilt(returns.columns.tolist())
        tilted_weights = self._apply_tilt(base_weights, tilt_factors)

        # Step 4: ì¶©ê²© ê²½ê³  ìƒì„±
        print("[Strategy] Step 4: Generating shock warnings...")
        shock_signals = self._detect_shock_warnings(macro_data)
        self.signals.extend(shock_signals)

        # Step 5: ê±°ë˜ëŸ‰ ì´ìƒì¹˜ íƒì§€
        print("[Strategy] Step 5: Detecting volume anomalies...")
        if volumes is not None:
            volume_signals = self._detect_volume_anomalies(volumes, returns)
            self.signals.extend(volume_signals)

        # Step 6: ë¦¬ìŠ¤í¬ ë©”íŠ¸ë¦­ ê³„ì‚°
        risk_metrics = self._calculate_risk_metrics(returns, tilted_weights)

        # Step 7: ì‹¤í–‰ ì•¡ì…˜ ìƒì„±
        actions = self._generate_actions(base_weights, tilted_weights, self.signals)
        warnings = self._generate_warnings(self.signals)

        # ë…¸ì¶œë„ ê³„ì‚°
        leading_exp = self._calculate_leading_exposure(tilted_weights)
        lagging_exp = self._calculate_lagging_exposure(tilted_weights)
        shock_vuln = self._calculate_shock_vulnerability(tilted_weights)

        return StrategyRecommendation(
            timestamp=datetime.now().isoformat(),
            portfolio_weights=base_weights,
            tilted_weights=tilted_weights,
            tilt_factors=tilt_factors,
            signals=self.signals,
            risk_metrics=risk_metrics,
            leading_exposure=leading_exp,
            lagging_exposure=lagging_exp,
            shock_vulnerability=shock_vuln,
            actions=actions,
            warnings=warnings
        )

    def _calculate_leading_tilt(self, assets: List[str]) -> Dict[str, float]:
        """
        ì„ í–‰ì§€í‘œ ê¸°ë°˜ í‹¸íŒ… íŒ©í„° ê³„ì‚°

        ê²½ì œí•™ì  ê·¼ê±°:
        - Out-degree ë†’ì€ ìì‚°: ì‹œì¥ì„ ë¦¬ë“œ â†’ ì˜¤ë²„ì›¨ì´íŠ¸
        - In-degree ë†’ì€ ìì‚°: í›„í–‰ â†’ ë¦¬ë°¸ëŸ°ì‹± ì‹ í˜¸ë¡œ í™œìš©
        """
        tilt_factors = {}

        if self.causality_analysis is None:
            return {a: 1.0 for a in assets}

        # ë…¸ë“œ ë¶„ì„ ê²°ê³¼ì—ì„œ ì„ í–‰ ì ìˆ˜ ì¶”ì¶œ
        node_scores = {}
        for node in self.causality_analysis.nodes:
            node_scores[node.node] = {
                'leading_score': node.leading_score,
                'role': node.role,
                'layer': node.layer.value
            }

        for asset in assets:
            if asset in node_scores:
                score = node_scores[asset]

                # ì„ í–‰ ì§€í‘œ: í‹¸íŒ… ì¦ê°€
                if score['role'] == 'LEADING':
                    tilt_factors[asset] = 1.0 + self.leading_tilt_factor

                    self.signals.append(Signal(
                        timestamp=datetime.now().isoformat(),
                        signal_type=SignalType.LEADING_TILT,
                        source=asset,
                        affected_assets=[asset],
                        confidence=0.7,
                        urgency="LOW",
                        description=f"{asset}ëŠ” ì„ í–‰ì§€í‘œë¡œ ì‹ë³„ë¨. ë¹„ì¤‘ í™•ëŒ€ ê¶Œê³ .",
                        action_suggested=ActionType.INCREASE
                    ))

                # í›„í–‰ ì§€í‘œ: í‹¸íŒ… ê°ì†Œ
                elif score['role'] == 'LAGGING':
                    tilt_factors[asset] = 1.0 - self.leading_tilt_factor * 0.5

                # ë¸Œë¦¿ì§€: í—¤ì§€ ëª©ì  ìœ ì§€
                elif score['role'] == 'BRIDGE':
                    tilt_factors[asset] = 1.0  # ìœ ì§€

                else:
                    tilt_factors[asset] = 1.0
            else:
                tilt_factors[asset] = 1.0

        return tilt_factors

    def _apply_tilt(
        self,
        weights: Dict[str, float],
        tilt_factors: Dict[str, float]
    ) -> Dict[str, float]:
        """í‹¸íŒ… ì ìš© ë° ì •ê·œí™”"""
        tilted = {}

        for asset, weight in weights.items():
            tilt = tilt_factors.get(asset, 1.0)
            tilted[asset] = weight * tilt

        # ì •ê·œí™”
        total = sum(tilted.values())
        if total > 0:
            tilted = {k: v / total for k, v in tilted.items()}

        return tilted

    def _detect_shock_warnings(self, macro_data: pd.DataFrame) -> List[Signal]:
        """
        ì¶©ê²© ì „íŒŒ ê²½ê³  ìƒì„±

        ìƒìœ„ ë ˆì´ì–´(POLICY, LIQUIDITY)ì—ì„œ ê¸‰ê²©í•œ ë³€í™” ê°ì§€ ì‹œ
        í•˜ìœ„ ë ˆì´ì–´(ASSET_PRICE) ê²½ê³  ë°œìƒ
        """
        signals = []

        if self.causality_analysis is None:
            return signals

        # ìµœê·¼ Nì¼ ë³€í™”ìœ¨ ê³„ì‚°
        recent = macro_data.tail(self.shock_window)

        for col in macro_data.columns:
            layer = get_node_layer(col)

            # ìƒìœ„ ë ˆì´ì–´ë§Œ ëª¨ë‹ˆí„°ë§
            if layer not in [NodeLayer.POLICY, NodeLayer.LIQUIDITY, NodeLayer.RISK_PREMIUM]:
                continue

            # ë³€í™”ìœ¨ ê³„ì‚°
            if len(recent[col].dropna()) < 2:
                continue

            first_val = recent[col].dropna().iloc[0]
            last_val = recent[col].dropna().iloc[-1]

            if first_val == 0:
                continue

            change_pct = (last_val - first_val) / abs(first_val) * 100

            # ê¸‰ê²©í•œ ë³€í™” ê°ì§€ (2% ì´ìƒ)
            if abs(change_pct) > 2:
                # Critical Pathì—ì„œ ì˜í–¥ë°›ëŠ” ìì‚° íƒìƒ‰
                affected = self._find_affected_assets(col)

                urgency = "HIGH" if abs(change_pct) > 5 else "MEDIUM"

                direction = "ìƒìŠ¹" if change_pct > 0 else "í•˜ë½"

                signals.append(Signal(
                    timestamp=datetime.now().isoformat(),
                    signal_type=SignalType.SHOCK_WARNING,
                    source=col,
                    affected_assets=affected,
                    confidence=min(0.9, abs(change_pct) / 10),
                    urgency=urgency,
                    description=f"{col} {self.shock_window}ì¼ê°„ {change_pct:.1f}% {direction}. "
                               f"ì˜í–¥ ì˜ˆìƒ ìì‚°: {', '.join(affected[:3])}",
                    action_suggested=ActionType.HEDGE if change_pct > 0 else ActionType.HOLD,
                    metadata={'change_pct': change_pct, 'layer': layer.name}
                ))

        return signals

    def _find_affected_assets(self, source: str) -> List[str]:
        """ì¶©ê²© ì†ŒìŠ¤ì—ì„œ ì˜í–¥ë°›ëŠ” ìì‚° íƒìƒ‰"""
        affected = []

        path = self.causality_engine.find_critical_path(source)
        if path:
            # Critical Pathì˜ ë§ˆì§€ë§‰ ë…¸ë“œë“¤ (ìì‚°)
            for node in path.path:
                if get_node_layer(node) == NodeLayer.ASSET_PRICE:
                    affected.append(node)

        # Direct successors ì¶”ê°€
        if source in self.causality_engine.graph:
            for successor in self.causality_engine.graph.successors(source):
                if successor not in affected:
                    affected.append(successor)

        return affected

    def _detect_volume_anomalies(
        self,
        volumes: pd.DataFrame,
        returns: pd.DataFrame
    ) -> List[Signal]:
        """
        ê±°ë˜ëŸ‰ ì´ìƒì¹˜ íƒì§€

        ê²½ì œí•™ì  ì˜ë¯¸:
        - ê±°ë˜ëŸ‰ ê¸‰ì¦ = ì°¸ì—¬ì ê°„ ê¸°ëŒ€ ë¶ˆì¼ì¹˜ ë˜ëŠ” ìƒˆë¡œìš´ ì •ë³´ ìœ ì…
        - ê°€ê²© ìƒìŠ¹ + ê±°ë˜ëŸ‰ ê¸‰ì¦ = ê°•í•œ ë§¤ìˆ˜ ì‹ í˜¸
        - ê°€ê²© í•˜ë½ + ê±°ë˜ëŸ‰ ê¸‰ì¦ = íŒ¨ë‹‰ ë˜ëŠ” ë§¤ë„ ì‹ í˜¸
        """
        signals = []

        for asset in volumes.columns:
            if asset not in returns.columns:
                continue

            vol_series = volumes[asset].dropna()
            if len(vol_series) < 20:
                continue

            # 20ì¼ ì´ë™í‰ê· 
            vol_ma20 = vol_series.rolling(20).mean()

            # ìµœê·¼ ê±°ë˜ëŸ‰
            recent_vol = vol_series.iloc[-1]
            recent_ma = vol_ma20.iloc[-1]

            if recent_ma == 0:
                continue

            surge_ratio = recent_vol / recent_ma

            # ê±°ë˜ëŸ‰ ê¸‰ì¦ íƒì§€
            if surge_ratio >= self.volume_surge_threshold:
                # ê°€ê²© ë°©í–¥ í™•ì¸
                recent_return = returns[asset].iloc[-1] if len(returns[asset]) > 0 else 0

                if recent_return > 0.01:  # 1% ì´ìƒ ìƒìŠ¹
                    interpretation = "ê°•í•œ ë§¤ìˆ˜ì„¸ ìœ ì… (NEW_INFORMATION)"
                    action = ActionType.HOLD  # ì¶”ì„¸ ì¶”ì¢…
                elif recent_return < -0.01:  # 1% ì´ìƒ í•˜ë½
                    interpretation = "íŒ¨ë‹‰ ë§¤ë„ ë˜ëŠ” ê³ ì  ì‹ í˜¸ (EXHAUSTION)"
                    action = ActionType.REDUCE
                else:
                    interpretation = "ë°©í–¥ì„± ë¶ˆëª…í™• (ACCUMULATION)"
                    action = ActionType.HOLD

                signals.append(Signal(
                    timestamp=datetime.now().isoformat(),
                    signal_type=SignalType.VOLUME_SPIKE,
                    source=asset,
                    affected_assets=[asset],
                    confidence=min(0.9, surge_ratio / 10),
                    urgency="MEDIUM" if surge_ratio < 5 else "HIGH",
                    description=f"{asset} ê±°ë˜ëŸ‰ ê¸‰ì¦ (MA20 ëŒ€ë¹„ {surge_ratio:.1f}ë°°). {interpretation}",
                    action_suggested=action,
                    metadata={
                        'surge_ratio': surge_ratio,
                        'recent_return': recent_return,
                        'interpretation': interpretation
                    }
                ))

        return signals

    def _calculate_risk_metrics(
        self,
        returns: pd.DataFrame,
        weights: Dict[str, float]
    ) -> Dict[str, float]:
        """ë¦¬ìŠ¤í¬ ë©”íŠ¸ë¦­ ê³„ì‚°"""
        assets = [a for a in weights.keys() if a in returns.columns]
        w = np.array([weights[a] for a in assets])

        # ê³µë¶„ì‚°
        cov = returns[assets].cov().values

        # í¬íŠ¸í´ë¦¬ì˜¤ ë³€ë™ì„±
        port_var = np.dot(w, np.dot(cov, w))
        port_vol = np.sqrt(port_var) * np.sqrt(252)

        # VaR (95%)
        port_returns = returns[assets].dot(pd.Series(weights)[assets])
        var_95 = np.percentile(port_returns.dropna(), 5) * np.sqrt(252)

        # CVaR (Expected Shortfall)
        es_95 = port_returns[port_returns <= np.percentile(port_returns, 5)].mean() * np.sqrt(252)

        # ìµœëŒ€ ë“œë¡œìš°ë‹¤ìš´
        cumulative = (1 + port_returns).cumprod()
        peak = cumulative.expanding().max()
        drawdown = (cumulative - peak) / peak
        max_dd = drawdown.min()

        return {
            'volatility': port_vol,
            'var_95': var_95,
            'cvar_95': es_95 if not np.isnan(es_95) else var_95,
            'max_drawdown': max_dd,
            'sharpe_estimate': port_returns.mean() * 252 / (port_vol + 1e-10)
        }

    def _calculate_leading_exposure(self, weights: Dict[str, float]) -> float:
        """ì„ í–‰ì§€í‘œ ë…¸ì¶œë„ ê³„ì‚°"""
        if self.causality_analysis is None:
            return 0.0

        leading_weight = 0.0
        for node in self.causality_analysis.nodes:
            if node.role == 'LEADING' and node.node in weights:
                leading_weight += weights[node.node]

        return leading_weight

    def _calculate_lagging_exposure(self, weights: Dict[str, float]) -> float:
        """í›„í–‰ì§€í‘œ ë…¸ì¶œë„ ê³„ì‚°"""
        if self.causality_analysis is None:
            return 0.0

        lagging_weight = 0.0
        for node in self.causality_analysis.nodes:
            if node.role == 'LAGGING' and node.node in weights:
                lagging_weight += weights[node.node]

        return lagging_weight

    def _calculate_shock_vulnerability(self, weights: Dict[str, float]) -> float:
        """
        ì¶©ê²© ì·¨ì•½ë„ ê³„ì‚°

        Critical Path ìƒì—ì„œ ì˜í–¥ë°›ëŠ” ìì‚°ì˜ ê°€ì¤‘ì¹˜ í•©
        """
        if self.causality_analysis is None:
            return 0.0

        vulnerable_assets = set()

        for path in self.causality_analysis.critical_paths:
            # ê²½ë¡œ ëì— ìˆëŠ” ìì‚°ë“¤ (ìµœì¢… ì˜í–¥ë°›ëŠ” ìì‚°)
            for node in path.path[-3:]:  # ë§ˆì§€ë§‰ 3ê°œ
                if get_node_layer(node) == NodeLayer.ASSET_PRICE:
                    vulnerable_assets.add(node)

        vulnerability = sum(weights.get(a, 0) for a in vulnerable_assets)
        return vulnerability

    def _generate_actions(
        self,
        base_weights: Dict[str, float],
        tilted_weights: Dict[str, float],
        signals: List[Signal]
    ) -> List[Dict]:
        """ì‹¤í–‰ ì•¡ì…˜ ìƒì„±"""
        actions = []

        # 1. í‹¸íŒ…ìœ¼ë¡œ ì¸í•œ ë¦¬ë°¸ëŸ°ì‹±
        for asset in tilted_weights:
            base = base_weights.get(asset, 0)
            tilted = tilted_weights.get(asset, 0)
            diff = tilted - base

            if abs(diff) > 0.01:  # 1% ì´ìƒ ë³€í™”
                action_type = "INCREASE" if diff > 0 else "DECREASE"
                actions.append({
                    'asset': asset,
                    'action': action_type,
                    'from_weight': f"{base:.2%}",
                    'to_weight': f"{tilted:.2%}",
                    'change': f"{diff:+.2%}",
                    'reason': 'Leading indicator tilt'
                })

        # 2. ì‹œê·¸ë„ ê¸°ë°˜ ì•¡ì…˜
        for signal in signals:
            if signal.urgency in ["HIGH", "MEDIUM"]:
                for asset in signal.affected_assets[:3]:
                    actions.append({
                        'asset': asset,
                        'action': signal.action_suggested.value.upper(),
                        'reason': signal.description[:100],
                        'urgency': signal.urgency,
                        'confidence': f"{signal.confidence:.0%}"
                    })

        return actions

    def _generate_warnings(self, signals: List[Signal]) -> List[str]:
        """ê²½ê³  ë©”ì‹œì§€ ìƒì„±"""
        warnings = []

        high_urgency = [s for s in signals if s.urgency == "HIGH"]

        for signal in high_urgency:
            warnings.append(f"âš ï¸ [{signal.signal_type.value}] {signal.description}")

        # ì¶©ê²© ì·¨ì•½ë„ ê²½ê³ 
        if self.causality_analysis:
            critical_paths = self.causality_analysis.critical_paths
            if critical_paths:
                longest = max(critical_paths, key=lambda p: len(p.path))
                if len(longest.path) > 4:
                    warnings.append(
                        f"ğŸ”— ê¸´ ì¶©ê²© ì „íŒŒ ê²½ë¡œ ê°ì§€: {' â†’ '.join(longest.path[:5])}... "
                        f"(ì´ {longest.total_lag}ì¼ ì†Œìš”)"
                    )

        return warnings

    def get_summary(self) -> str:
        """ì „ëµ ìš”ì•½ í…ìŠ¤íŠ¸"""
        if self.portfolio_allocation is None:
            return "Strategy not yet fitted."

        lines = [
            "=" * 60,
            "INTEGRATED STRATEGY SUMMARY",
            "=" * 60,
            "",
            f"Portfolio: {self.portfolio_allocation.methodology}",
            f"  - Clusters: {len(self.portfolio_allocation.clusters)}",
            f"  - Diversification Ratio: {self.portfolio_allocation.diversification_ratio:.2f}",
            f"  - Effective N: {self.portfolio_allocation.effective_n:.1f}",
            ""
        ]

        if self.causality_analysis:
            lines.extend([
                "Causality Analysis:",
                f"  - Leading Indicators: {', '.join(self.causality_analysis.leading_indicators[:3]) or 'None'}",
                f"  - Bridge Nodes: {', '.join(self.causality_analysis.bridge_nodes[:3]) or 'None'}",
                ""
            ])

            if self.causality_analysis.critical_paths:
                path = self.causality_analysis.critical_paths[0]
                lines.append(f"  - Critical Path: {' â†’ '.join(path.path)}")
                lines.append(f"    (Total lag: {path.total_lag} days)")
                lines.append("")

        lines.append(f"Signals Generated: {len(self.signals)}")
        high_signals = [s for s in self.signals if s.urgency == "HIGH"]
        if high_signals:
            lines.append(f"  âš ï¸ HIGH urgency signals: {len(high_signals)}")

        return "\n".join(lines)


# ============================================================================
# Utility Functions
# ============================================================================

def create_integrated_sample_data(
    n_assets: int = 50,
    n_days: int = 500
) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:
    """í†µí•© í…ŒìŠ¤íŠ¸ìš© ìƒ˜í”Œ ë°ì´í„° ìƒì„±"""
    np.random.seed(42)
    dates = pd.date_range(end=datetime.now(), periods=n_days, freq='D')

    # 1. ê±°ì‹œ ë°ì´í„° (ì¸ê³¼ê´€ê³„ í¬í•¨)
    fed_funds = np.cumsum(np.random.randn(n_days) * 0.001) + 4.5
    dxy = pd.Series(fed_funds).shift(2).fillna(method='bfill').values * 20 + 100
    vix = pd.Series(dxy).shift(3).fillna(method='bfill').values * 0.2 + 15

    macro_data = pd.DataFrame({
        'FED_FUNDS': fed_funds,
        'DXY': dxy + np.random.randn(n_days) * 0.5,
        'VIX': vix + np.abs(np.random.randn(n_days)) * 2
    }, index=dates)

    # 2. ìì‚° ìˆ˜ìµë¥  (íŒ©í„° ê¸°ë°˜)
    n_factors = 3
    factor_returns = np.random.randn(n_days, n_factors) * 0.01
    loadings = np.random.randn(n_assets, n_factors)
    idiosyncratic = np.random.randn(n_days, n_assets) * 0.02

    asset_returns = np.dot(factor_returns, loadings.T) + idiosyncratic

    assets = [f'ASSET_{i:02d}' for i in range(n_assets)]
    returns = pd.DataFrame(asset_returns, index=dates, columns=assets)

    # 3. ê±°ë˜ëŸ‰ (ì¼ë¶€ ê¸‰ì¦ í¬í•¨)
    volumes = np.exp(np.random.randn(n_days, n_assets) + 10)

    # íŠ¹ì • ì‹œì  ê±°ë˜ëŸ‰ ê¸‰ì¦
    spike_idx = np.random.choice(range(50, n_days), size=10, replace=False)
    spike_assets = np.random.choice(range(n_assets), size=10, replace=False)
    for idx, asset_idx in zip(spike_idx, spike_assets):
        volumes[idx, asset_idx] *= 5  # 5ë°° ê¸‰ì¦

    volumes_df = pd.DataFrame(volumes, index=dates, columns=assets)

    return returns, macro_data, volumes_df


# ============================================================================
# CLI Test
# ============================================================================

if __name__ == "__main__":
    print("=" * 60)
    print("Integrated Strategy Test")
    print("=" * 60)

    # ìƒ˜í”Œ ë°ì´í„° ìƒì„±
    print("\n1. Generating sample data...")
    returns, macro_data, volumes = create_integrated_sample_data(
        n_assets=50, n_days=500
    )
    print(f"   Returns: {returns.shape}")
    print(f"   Macro: {macro_data.shape}")
    print(f"   Volumes: {volumes.shape}")

    # í†µí•© ì „ëµ ì‹¤í–‰
    print("\n2. Running integrated strategy...")
    strategy = IntegratedStrategy(
        correlation_threshold=0.3,
        clustering_method=ClusteringMethod.KMEANS,
        leading_tilt_factor=0.15,
        volume_surge_threshold=3.0
    )

    recommendation = strategy.fit(returns, macro_data, volumes)

    # ê²°ê³¼ ì¶œë ¥
    print("\n3. Strategy Summary:")
    print(strategy.get_summary())

    print("\n4. Risk Metrics:")
    for metric, value in recommendation.risk_metrics.items():
        print(f"   {metric}: {value:.4f}")

    print("\n5. Exposure Analysis:")
    print(f"   Leading Exposure: {recommendation.leading_exposure:.2%}")
    print(f"   Lagging Exposure: {recommendation.lagging_exposure:.2%}")
    print(f"   Shock Vulnerability: {recommendation.shock_vulnerability:.2%}")

    print("\n6. Signals Generated:")
    for signal in recommendation.signals[:5]:
        print(f"   [{signal.urgency}] {signal.signal_type.value}: {signal.description[:60]}...")

    print("\n7. Top Actions:")
    for action in recommendation.actions[:5]:
        print(f"   {action.get('action', 'N/A')}: {action.get('asset', 'N/A')} - {action.get('reason', '')[:50]}")

    print("\n8. Warnings:")
    for warning in recommendation.warnings:
        print(f"   {warning}")

    print("\n9. Top 10 Tilted Weights:")
    sorted_weights = sorted(
        recommendation.tilted_weights.items(),
        key=lambda x: x[1],
        reverse=True
    )
    for asset, weight in sorted_weights[:10]:
        base = recommendation.portfolio_weights.get(asset, 0)
        tilt = recommendation.tilt_factors.get(asset, 1.0)
        print(f"   {asset}: {weight:.2%} (base: {base:.2%}, tilt: {tilt:.2f})")

    print("\n" + "=" * 60)
    print("Test completed successfully!")
