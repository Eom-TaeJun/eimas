#!/usr/bin/env python3
"""
Market Anomaly Detector - Risk Appetite & Uncertainty Index
============================================================
Bekaert et al. ì—°êµ¬ ê¸°ë°˜ ë¦¬ìŠ¤í¬ ì„ í˜¸ë„ì™€ ë¶ˆí™•ì‹¤ì„±ì„ ë¶„ë¦¬ ì¸¡ì •í•˜ëŠ” ëª¨ë“ˆ

ê²½ì œí•™ì  ë°°ê²½:
- ë¶ˆí™•ì‹¤ì„±(Uncertainty): ì‹œì¥ì˜ ì˜ˆì¸¡ ë¶ˆê°€ëŠ¥ì„±, ë³€ë™ì„± ì±„ë„ë¡œ ì‘ìš©
- ë¦¬ìŠ¤í¬ ì• í¼íƒ€ì´íŠ¸(Risk Appetite): íˆ¬ììë“¤ì˜ ìœ„í—˜ ê°ìˆ˜ ì˜ì§€, í• ì¸ìœ¨ ì±„ë„ë¡œ ì‘ìš©
- VIXë§Œìœ¼ë¡œëŠ” ë‘ ê°œë…ì´ ì„ì—¬ì„œ í•´ì„ ì˜¤ë¥˜ ë°œìƒ
- ë¶„ì‚° ë¦¬ìŠ¤í¬ í”„ë¦¬ë¯¸ì—„ = VIXÂ² - ì‹¤í˜„ë¶„ì‚° (ë¦¬ìŠ¤í¬ ì„ í˜¸ì˜ í”„ë¡ì‹œ)
"""

import pandas as pd
import numpy as np
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass, asdict, field
from datetime import datetime
import warnings

warnings.filterwarnings('ignore')

# Granger Causality ê²€ì •ì„ ìœ„í•œ statsmodels ì„í¬íŠ¸
try:
    from statsmodels.tsa.stattools import grangercausalitytests
    STATSMODELS_AVAILABLE = True
except ImportError:
    STATSMODELS_AVAILABLE = False
    print("Warning: statsmodels not available. Granger Causality tests will be skipped.")


@dataclass
class RiskAppetiteUncertaintyResult:
    """
    ë¦¬ìŠ¤í¬ ì• í¼íƒ€ì´íŠ¸ì™€ ë¶ˆí™•ì‹¤ì„± ë¶„ì„ ê²°ê³¼
    
    ê²½ì œí•™ì  ì˜ë¯¸:
    - risk_appetite_score: 0-100, ë†’ì„ìˆ˜ë¡ ìœ„í—˜ ì„ í˜¸ (íˆ¬ììë“¤ì´ ìœ„í—˜ì„ ê°ìˆ˜í•˜ë ¤ëŠ” ì˜ì§€)
    - uncertainty_score: 0-100, ë†’ì„ìˆ˜ë¡ ë¶ˆí™•ì‹¤ (ì‹œì¥ì˜ ì˜ˆì¸¡ ë¶ˆê°€ëŠ¥ì„±)
    - market_state: ë‘ ì§€ìˆ˜ì˜ ì¡°í•©ìœ¼ë¡œ ì‹œì¥ ìƒíƒœ í•´ì„
    """
    timestamp: str
    risk_appetite_score: float      # 0-100, ë†’ì„ìˆ˜ë¡ ìœ„í—˜ ì„ í˜¸
    uncertainty_score: float        # 0-100, ë†’ì„ìˆ˜ë¡ ë¶ˆí™•ì‹¤
    risk_appetite_level: str        # "LOW", "MEDIUM", "HIGH"
    uncertainty_level: str          # "LOW", "MEDIUM", "HIGH"
    market_state: str               # "NORMAL", "SPECULATIVE", "STAGNANT", "CRISIS", "MIXED"
    components: Dict                 # ê°œë³„ ì§€í‘œ ê°’ë“¤
    interpretation: str           # í•´ì„ í…ìŠ¤íŠ¸
    
    def to_dict(self) -> Dict:
        """ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜ (JSON ì§ë ¬í™”ìš©)"""
        return asdict(self)


def calculate_rolling_zscore(series: pd.Series, window: int = 20) -> pd.Series:
    """
    ë¡¤ë§ Z-score ê³„ì‚°
    
    ê²½ì œí•™ì  ì˜ë¯¸:
    - Z-score = (í˜„ì¬ê°’ - Nì¼ í‰ê· ) / Nì¼ í‘œì¤€í¸ì°¨
    - |Z| > 2: í†µê³„ì ìœ¼ë¡œ ì´ìƒì¹˜ (95% ì‹ ë¢°êµ¬ê°„ ë²—ì–´ë‚¨)
    - Mean Reversion: ê·¹ë‹¨ì  Z-scoreëŠ” í‰ê·  íšŒê·€ ê²½í–¥
    """
    mean = series.rolling(window=window, min_periods=1).mean()
    std = series.rolling(window=window, min_periods=1).std()
    z_score = (series - mean) / std.replace(0, np.nan)
    return z_score.fillna(0)


def calculate_realized_volatility(prices: pd.Series, window: int = 20) -> float:
    """
    ì‹¤í˜„ ë³€ë™ì„± ê³„ì‚° (ì—°ìœ¨í™”)
    
    ê²½ì œí•™ì  ì˜ë¯¸:
    - ì‹¤í˜„ ë³€ë™ì„± = ê³¼ê±° Nì¼ê°„ ìˆ˜ìµë¥ ì˜ í‘œì¤€í¸ì°¨ Ã— âˆš252
    - ë†’ì€ ë³€ë™ì„± = ë†’ì€ ë¶ˆí™•ì‹¤ì„± = ë†’ì€ ë¦¬ìŠ¤í¬
    """
    returns = prices.pct_change().dropna()
    if len(returns) < window:
        window = len(returns)
    if window == 0:
        return 0.0
    return returns.tail(window).std() * np.sqrt(252) * 100


def normalize_to_score(value: float, min_val: float, max_val: float) -> float:
    """
    ê°’ì„ 0-100 ìŠ¤ì½”ì–´ë¡œ ì •ê·œí™”
    
    Args:
        value: ì •ê·œí™”í•  ê°’
        min_val: ìµœì†Œê°’ (0ì )
        max_val: ìµœëŒ€ê°’ (100ì )
    
    Returns:
        0-100 ì‚¬ì´ì˜ ì •ê·œí™”ëœ ìŠ¤ì½”ì–´
    """
    if max_val == min_val:
        return 50.0  # ê¸°ë³¸ê°’
    normalized = (value - min_val) / (max_val - min_val) * 100
    return max(0.0, min(100.0, normalized))


class RiskAppetiteUncertaintyIndex:
    """
    ë¦¬ìŠ¤í¬ ì• í¼íƒ€ì´íŠ¸ì™€ ë¶ˆí™•ì‹¤ì„±ì„ ë¶„ë¦¬ ì¸¡ì •í•˜ëŠ” ì¸ë±ìŠ¤
    
    Bekaert et al. ì—°êµ¬ì— ê¸°ë°˜í•˜ì—¬ ì‹œì¥ì˜ "ë¦¬ìŠ¤í¬ ì„ í˜¸ë„"ì™€ "ë¶ˆí™•ì‹¤ì„±"ì„
    ë¶„ë¦¬ ì¸¡ì •í•©ë‹ˆë‹¤. ì´ ë‘ ì§€ìˆ˜ëŠ” ì„œë¡œ ë‹¤ë¥¸ ì±„ë„ë¡œ ìì‚°ê°€ê²©ì— ì˜í–¥ì„ ë¯¸ì¹˜ë©°,
    ì¡°í•©ì— ë”°ë¼ ì‹œì¥ ìƒíƒœ í•´ì„ì´ ë‹¬ë¼ì§‘ë‹ˆë‹¤.
    
    ê²½ì œí•™ì  ë°°ê²½:
    - ë¶ˆí™•ì‹¤ì„±(Uncertainty): ì‹œì¥ì˜ ì˜ˆì¸¡ ë¶ˆê°€ëŠ¥ì„±, ë³€ë™ì„± ì±„ë„ë¡œ ì‘ìš©
    - ë¦¬ìŠ¤í¬ ì• í¼íƒ€ì´íŠ¸(Risk Appetite): íˆ¬ììë“¤ì˜ ìœ„í—˜ ê°ìˆ˜ ì˜ì§€, í• ì¸ìœ¨ ì±„ë„ë¡œ ì‘ìš©
    - VIXë§Œìœ¼ë¡œëŠ” ë‘ ê°œë…ì´ ì„ì—¬ì„œ í•´ì„ ì˜¤ë¥˜ ë°œìƒ
    """
    
    def __init__(self, lookback: int = 20):
        """
        Args:
            lookback: ë¡¤ë§ ìœˆë„ìš° ê¸°ê°„ (ê¸°ë³¸ê°’ 20ì¼)
        """
        self.lookback = lookback
    
    def calculate_uncertainty_index(self, market_data: Dict[str, pd.DataFrame]) -> Dict:
        """
        ë¶ˆí™•ì‹¤ì„± ì§€ìˆ˜ ê³„ì‚°
        
        êµ¬ì„±ìš”ì†Œ:
        1. VIX ë ˆë²¨ (ì •ê·œí™”)
           - VIX < 15: ë‚®ìŒ, 15-25: ë³´í†µ, > 25: ë†’ìŒ
        
        2. ì‹¤í˜„ë³€ë™ì„± (20ì¼)
           - SPY ì¼ê°„ ìˆ˜ìµë¥ ì˜ í‘œì¤€í¸ì°¨ * sqrt(252) * 100
        
        3. VIX-ì‹¤í˜„ë³€ë™ì„± ê´´ë¦¬
           - ê´´ë¦¬ = VIX - ì‹¤í˜„ë³€ë™ì„±
           - ê´´ë¦¬ê°€ í´ìˆ˜ë¡ ë¶ˆí™•ì‹¤ì„±ì— ëŒ€í•œ í”„ë¦¬ë¯¸ì—„ ë†’ìŒ
        
        4. ì„¹í„°ê°„ ìƒê´€ê´€ê³„ ë¶„ì‚°
           - 11ê°œ ì„¹í„° ETF ê°„ ìƒê´€ê´€ê³„ì˜ ë¶„ì‚°
           - ë¶„ì‚°ì´ ë‚®ìœ¼ë©´(ìƒê´€ê´€ê³„ ìˆ˜ë ´) ë¶ˆí™•ì‹¤ì„± ë†’ìŒ (ìœ„ê¸° ì‹œ ë™ì¡°í™”)
        
        Returns:
            Dict with 'score' (0-100), 'level', 'components'
        """
        components = {}
        
        # 1. VIX ë ˆë²¨
        vix_data = market_data.get('^VIX')
        if vix_data is None or (hasattr(vix_data, 'empty') and vix_data.empty):
            vix_data = market_data.get('VIX')
        if vix_data is None or (hasattr(vix_data, 'empty') and vix_data.empty) or (vix_data is not None and 'Close' not in vix_data.columns):
            vix_value = 20.0  # ê¸°ë³¸ê°’
        else:
            vix_value = float(vix_data['Close'].iloc[-1])
        
        components['vix_level'] = vix_value
        # VIX ì •ê·œí™”: 10-40 ë²”ìœ„ë¥¼ 0-100ìœ¼ë¡œ ë§¤í•‘
        vix_score = normalize_to_score(vix_value, min_val=10.0, max_val=40.0)
        components['vix_score'] = vix_score
        
        # 2. ì‹¤í˜„ë³€ë™ì„±
        spy_data = market_data.get('SPY')
        if spy_data is None or (hasattr(spy_data, 'empty') and spy_data.empty) or (spy_data is not None and 'Close' not in spy_data.columns):
            realized_vol = 15.0  # ê¸°ë³¸ê°’
        else:
            realized_vol = calculate_realized_volatility(
                spy_data['Close'], 
                window=self.lookback
            )
        
        components['realized_volatility'] = realized_vol
        # ì‹¤í˜„ë³€ë™ì„± ì •ê·œí™”: 5-35 ë²”ìœ„ë¥¼ 0-100ìœ¼ë¡œ ë§¤í•‘
        realized_vol_score = normalize_to_score(realized_vol, min_val=5.0, max_val=35.0)
        components['realized_vol_score'] = realized_vol_score
        
        # 3. VIX-ì‹¤í˜„ë³€ë™ì„± ê´´ë¦¬
        vix_realized_gap = vix_value - realized_vol
        components['vix_realized_gap'] = vix_realized_gap
        # ê´´ë¦¬ ì •ê·œí™”: -10 ~ +15 ë²”ìœ„ë¥¼ 0-100ìœ¼ë¡œ ë§¤í•‘
        # ê´´ë¦¬ê°€ í´ìˆ˜ë¡ ë¶ˆí™•ì‹¤ì„± í”„ë¦¬ë¯¸ì—„ ë†’ìŒ
        gap_score = normalize_to_score(vix_realized_gap, min_val=-10.0, max_val=15.0)
        components['gap_score'] = gap_score
        
        # 4. ì„¹í„°ê°„ ìƒê´€ê´€ê³„ ë¶„ì‚°
        # ì„¹í„° ETF ëª©ë¡ (XLB, XLC, XLE, XLF, XLI, XLK, XLP, XLRE, XLU, XLV, XLY)
        sector_tickers = ['XLB', 'XLC', 'XLE', 'XLF', 'XLI', 'XLK', 'XLP', 'XLRE', 'XLU', 'XLV', 'XLY']
        sector_returns = {}
        
        for ticker in sector_tickers:
            if ticker in market_data:
                df = market_data[ticker]
                if not df.empty and 'Close' in df.columns:
                    sector_returns[ticker] = df['Close'].pct_change().dropna()
        
        if len(sector_returns) >= 3:
            # ìµœê·¼ Nì¼ê°„ ìˆ˜ìµë¥  DataFrame ìƒì„±
            returns_df = pd.DataFrame(sector_returns)
            recent_returns = returns_df.tail(self.lookback)
            
            # ìƒê´€ê´€ê³„ í–‰ë ¬ ê³„ì‚°
            corr_matrix = recent_returns.corr()
            
            # ìƒê´€ê´€ê³„ ê°’ë“¤ì˜ ë¶„ì‚° ê³„ì‚° (ëŒ€ê°ì„  ì œì™¸)
            mask = np.triu(np.ones_like(corr_matrix, dtype=bool), k=1)
            corr_values = corr_matrix.where(mask).stack().dropna()
            
            if len(corr_values) > 0:
                corr_variance = float(corr_values.var())
                components['sector_corr_variance'] = corr_variance
                # ë¶„ì‚°ì´ ë‚®ìœ¼ë©´(ìƒê´€ê´€ê³„ ìˆ˜ë ´) ë¶ˆí™•ì‹¤ì„± ë†’ìŒ
                # ë¶„ì‚° 0.01-0.1 ë²”ìœ„ë¥¼ 100-0ìœ¼ë¡œ ì—­ë§¤í•‘ (ë‚®ì€ ë¶„ì‚° = ë†’ì€ ë¶ˆí™•ì‹¤ì„±)
                corr_score = 100 - normalize_to_score(corr_variance, min_val=0.01, max_val=0.1)
                components['corr_variance_score'] = corr_score
            else:
                components['sector_corr_variance'] = 0.05
                components['corr_variance_score'] = 50.0
        else:
            components['sector_corr_variance'] = 0.05
            components['corr_variance_score'] = 50.0
        
        # ì¢…í•© ë¶ˆí™•ì‹¤ì„± ìŠ¤ì½”ì–´ (ê°€ì¤‘í‰ê· )
        # VIX: 30%, ì‹¤í˜„ë³€ë™ì„±: 30%, ê´´ë¦¬: 25%, ìƒê´€ê´€ê³„: 15%
        uncertainty_score = (
            vix_score * 0.30 +
            realized_vol_score * 0.30 +
            gap_score * 0.25 +
            components['corr_variance_score'] * 0.15
        )
        
        # ë ˆë²¨ ê²°ì •
        if uncertainty_score < 40:
            uncertainty_level = "LOW"
        elif uncertainty_score < 70:
            uncertainty_level = "MEDIUM"
        else:
            uncertainty_level = "HIGH"
        
        return {
            'score': float(uncertainty_score),
            'level': uncertainty_level,
            'components': components
        }
    
    def calculate_risk_appetite_index(self, market_data: Dict[str, pd.DataFrame]) -> Dict:
        """
        ë¦¬ìŠ¤í¬ ì• í¼íƒ€ì´íŠ¸ ì§€ìˆ˜ ê³„ì‚°
        
        êµ¬ì„±ìš”ì†Œ:
        1. HYG/LQD ë¹„ìœ¨ Z-score
           - HYG(í•˜ì´ì¼ë“œ) / LQD(íˆ¬ìë“±ê¸‰) ë¹„ìœ¨
           - ë¹„ìœ¨ ìƒìŠ¹ = ì‹ ìš© ìŠ¤í”„ë ˆë“œ ì¶•ì†Œ = ìœ„í—˜ ì„ í˜¸ ì¦ê°€
        
        2. XLY/XLP ë¹„ìœ¨ Z-score
           - XLY(ê²½ê¸°ë¯¼ê°ì†Œë¹„ì¬) / XLP(í•„ìˆ˜ì†Œë¹„ì¬) ë¹„ìœ¨
           - ë¹„ìœ¨ ìƒìŠ¹ = ê²½ê¸°ë¯¼ê° ì„ í˜¸ = ìœ„í—˜ ì„ í˜¸ ì¦ê°€
        
        3. IWM/SPY ë¹„ìœ¨ Z-score
           - IWM(ì†Œí˜•ì£¼) / SPY(ëŒ€í˜•ì£¼) ë¹„ìœ¨
           - ë¹„ìœ¨ ìƒìŠ¹ = ì†Œí˜•ì£¼(ê³ ìœ„í—˜) ì„ í˜¸ = ìœ„í—˜ ì„ í˜¸ ì¦ê°€
        
        4. ë¶„ì‚° ë¦¬ìŠ¤í¬ í”„ë¦¬ë¯¸ì—„ (ì—­ë³€í™˜)
           - VRP = VIXÂ² - ì‹¤í˜„ë¶„ì‚°
           - VRP ë†’ìŒ = ì˜µì…˜ í”„ë¦¬ë¯¸ì—„ ë†’ìŒ = ë¦¬ìŠ¤í¬ íšŒí”¼
           - VRPë¥¼ ì—­ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ë¦¬ìŠ¤í¬ ì„ í˜¸ë¡œ í•´ì„
        
        Returns:
            Dict with 'score' (0-100), 'level', 'components'
        """
        components = {}
        
        # 1. HYG/LQD ë¹„ìœ¨ Z-score
        hyg_data = market_data.get('HYG')
        lqd_data = market_data.get('LQD')
        
        if hyg_data is not None and lqd_data is not None:
            if not hyg_data.empty and not lqd_data.empty:
                if 'Close' in hyg_data.columns and 'Close' in lqd_data.columns:
                    hyg_close = hyg_data['Close']
                    lqd_close = lqd_data['Close']
                    hyg_lqd_ratio = hyg_close / lqd_close.replace(0, np.nan)
                    hyg_lqd_zscore = calculate_rolling_zscore(hyg_lqd_ratio, window=self.lookback)
                    components['hyg_lqd_ratio'] = float(hyg_lqd_ratio.iloc[-1]) if not hyg_lqd_ratio.empty else 1.0
                    components['hyg_lqd_zscore'] = float(hyg_lqd_zscore.iloc[-1]) if not hyg_lqd_zscore.empty else 0.0
                else:
                    components['hyg_lqd_zscore'] = 0.0
            else:
                components['hyg_lqd_zscore'] = 0.0
        else:
            components['hyg_lqd_zscore'] = 0.0
        
        # 2. XLY/XLP ë¹„ìœ¨ Z-score
        xly_data = market_data.get('XLY')
        xlp_data = market_data.get('XLP')
        
        if xly_data is not None and xlp_data is not None:
            if not xly_data.empty and not xlp_data.empty:
                if 'Close' in xly_data.columns and 'Close' in xlp_data.columns:
                    xly_close = xly_data['Close']
                    xlp_close = xlp_data['Close']
                    xly_xlp_ratio = xly_close / xlp_close.replace(0, np.nan)
                    xly_xlp_zscore = calculate_rolling_zscore(xly_xlp_ratio, window=self.lookback)
                    components['xly_xlp_ratio'] = float(xly_xlp_ratio.iloc[-1]) if not xly_xlp_ratio.empty else 1.0
                    components['xly_xlp_zscore'] = float(xly_xlp_zscore.iloc[-1]) if not xly_xlp_zscore.empty else 0.0
                else:
                    components['xly_xlp_zscore'] = 0.0
            else:
                components['xly_xlp_zscore'] = 0.0
        else:
            components['xly_xlp_zscore'] = 0.0
        
        # 3. IWM/SPY ë¹„ìœ¨ Z-score
        iwm_data = market_data.get('IWM')
        spy_data = market_data.get('SPY')
        
        if iwm_data is not None and spy_data is not None:
            iwm_empty = hasattr(iwm_data, 'empty') and iwm_data.empty
            spy_empty = hasattr(spy_data, 'empty') and spy_data.empty
            if not iwm_empty and not spy_empty:
                if 'Close' in iwm_data.columns and 'Close' in spy_data.columns:
                    iwm_close = iwm_data['Close']
                    spy_close = spy_data['Close']
                    iwm_spy_ratio = iwm_close / spy_close.replace(0, np.nan)
                    iwm_spy_zscore = calculate_rolling_zscore(iwm_spy_ratio, window=self.lookback)
                    components['iwm_spy_ratio'] = float(iwm_spy_ratio.iloc[-1]) if not iwm_spy_ratio.empty else 1.0
                    components['iwm_spy_zscore'] = float(iwm_spy_zscore.iloc[-1]) if not iwm_spy_zscore.empty else 0.0
                else:
                    components['iwm_spy_zscore'] = 0.0
            else:
                components['iwm_spy_zscore'] = 0.0
        else:
            components['iwm_spy_zscore'] = 0.0
        
        # 4. ë¶„ì‚° ë¦¬ìŠ¤í¬ í”„ë¦¬ë¯¸ì—„ (VRP) ì—­ë³€í™˜
        vix_data = market_data.get('^VIX')
        if vix_data is None or (hasattr(vix_data, 'empty') and vix_data.empty):
            vix_data = market_data.get('VIX')
        spy_data = market_data.get('SPY')
        
        if vix_data is not None and spy_data is not None:
            vix_empty = hasattr(vix_data, 'empty') and vix_data.empty
            spy_empty = hasattr(spy_data, 'empty') and spy_data.empty
            if not vix_empty and not spy_empty:
                if 'Close' in vix_data.columns and 'Close' in spy_data.columns:
                    vix_value = float(vix_data['Close'].iloc[-1])
                    realized_vol = calculate_realized_volatility(
                        spy_data['Close'], 
                        window=self.lookback
                    )
                    # VRP = VIXÂ² - ì‹¤í˜„ë¶„ì‚°
                    # VIXëŠ” í¼ì„¼íŠ¸ ë‹¨ìœ„ì´ë¯€ë¡œ ì œê³± ì‹œ ì£¼ì˜
                    vrp = (vix_value / 100) ** 2 - (realized_vol / 100) ** 2
                    components['variance_risk_premium'] = float(vrp)
                    # VRPë¥¼ ì—­ë³€í™˜: ë†’ì€ VRP = ë‚®ì€ ë¦¬ìŠ¤í¬ ì„ í˜¸
                    # VRP ë²”ìœ„ -0.01 ~ 0.05ë¥¼ 100-0ìœ¼ë¡œ ì—­ë§¤í•‘
                    vrp_score = 100 - normalize_to_score(vrp, min_val=-0.01, max_val=0.05)
                    components['vrp_score'] = vrp_score
                else:
                    components['variance_risk_premium'] = 0.0
                    components['vrp_score'] = 50.0
            else:
                components['variance_risk_premium'] = 0.0
                components['vrp_score'] = 50.0
        else:
            components['variance_risk_premium'] = 0.0
            components['vrp_score'] = 50.0
        
        # Z-scoreë“¤ì„ ìŠ¤ì½”ì–´ë¡œ ë³€í™˜ (-3 ~ +3 ë²”ìœ„ë¥¼ 0-100ìœ¼ë¡œ)
        hyg_score = normalize_to_score(components['hyg_lqd_zscore'], min_val=-3.0, max_val=3.0)
        xly_score = normalize_to_score(components['xly_xlp_zscore'], min_val=-3.0, max_val=3.0)
        iwm_score = normalize_to_score(components['iwm_spy_zscore'], min_val=-3.0, max_val=3.0)
        
        # ì¢…í•© ë¦¬ìŠ¤í¬ ì• í¼íƒ€ì´íŠ¸ ìŠ¤ì½”ì–´ (ê°€ì¤‘í‰ê· )
        # HYG/LQD: 30%, XLY/XLP: 25%, IWM/SPY: 25%, VRP: 20%
        risk_appetite_score = (
            hyg_score * 0.30 +
            xly_score * 0.25 +
            iwm_score * 0.25 +
            components['vrp_score'] * 0.20
        )
        
        # ë ˆë²¨ ê²°ì •
        if risk_appetite_score < 40:
            risk_appetite_level = "LOW"
        elif risk_appetite_score < 60:
            risk_appetite_level = "MEDIUM"
        else:
            risk_appetite_level = "HIGH"
        
        return {
            'score': float(risk_appetite_score),
            'level': risk_appetite_level,
            'components': components
        }
    
    def determine_market_state(self, ra_score: float, unc_score: float) -> str:
        """
        ë¦¬ìŠ¤í¬ ì„ í˜¸ì™€ ë¶ˆí™•ì‹¤ì„± ì¡°í•©ìœ¼ë¡œ ì‹œì¥ ìƒíƒœ ê²°ì •
        
        ë§¤íŠ¸ë¦­ìŠ¤:
        |                    | ë¶ˆí™•ì‹¤ì„± LOW (< 40) | ë¶ˆí™•ì‹¤ì„± HIGH (>= 40) |
        |--------------------|---------------------|----------------------|
        | ë¦¬ìŠ¤í¬ì„ í˜¸ HIGH (>=60) | NORMAL              | SPECULATIVE (ìœ„í—˜!)   |
        | ë¦¬ìŠ¤í¬ì„ í˜¸ LOW (<40)   | STAGNANT            | CRISIS               |
        | ê·¸ ì™¸                 | MIXED               | MIXED                |
        
        ê²½ì œí•™ì  í•´ì„:
        - NORMAL: ë‚®ì€ ë¶ˆí™•ì‹¤ì„± + ë†’ì€ ë¦¬ìŠ¤í¬ ì„ í˜¸ = ê±´ê°•í•œ ì‹œì¥
        - SPECULATIVE: ë†’ì€ ë¶ˆí™•ì‹¤ì„± + ë†’ì€ ë¦¬ìŠ¤í¬ ì„ í˜¸ = ìœ„í—˜í•œ íˆ¬ê¸° ìƒíƒœ
        - STAGNANT: ë‚®ì€ ë¶ˆí™•ì‹¤ì„± + ë‚®ì€ ë¦¬ìŠ¤í¬ ì„ í˜¸ = ì‹œì¥ ì¹¨ì²´
        - CRISIS: ë†’ì€ ë¶ˆí™•ì‹¤ì„± + ë‚®ì€ ë¦¬ìŠ¤í¬ ì„ í˜¸ = ìœ„ê¸° ìƒíƒœ
        
        Returns:
            str: ì‹œì¥ ìƒíƒœ
        """
        if ra_score >= 60 and unc_score < 40:
            return "NORMAL"
        elif ra_score >= 60 and unc_score >= 40:
            return "SPECULATIVE"
        elif ra_score < 40 and unc_score < 40:
            return "STAGNANT"
        elif ra_score < 40 and unc_score >= 40:
            return "CRISIS"
        else:
            return "MIXED"
    
    def generate_interpretation(
        self, 
        ra_score: float, 
        ra_level: str,
        unc_score: float,
        unc_level: str,
        market_state: str
    ) -> str:
        """
        ë¶„ì„ ê²°ê³¼ í•´ì„ í…ìŠ¤íŠ¸ ìƒì„±
        
        Returns:
            str: í•´ì„ í…ìŠ¤íŠ¸
        """
        interpretations = {
            "NORMAL": (
                f"ì‹œì¥ì€ ê±´ê°•í•œ ìƒíƒœì…ë‹ˆë‹¤. ë¶ˆí™•ì‹¤ì„±({unc_level}, {unc_score:.1f}ì )ì´ ë‚®ê³  "
                f"íˆ¬ììë“¤ì˜ ë¦¬ìŠ¤í¬ ì„ í˜¸ë„({ra_level}, {ra_score:.1f}ì )ê°€ ë†’ì•„ "
                f"ì •ìƒì ì¸ ìœ„í—˜ ìì‚° ì„ í˜¸ê°€ ê´€ì°°ë©ë‹ˆë‹¤."
            ),
            "SPECULATIVE": (
                f"âš ï¸ ìœ„í—˜í•œ íˆ¬ê¸° ìƒíƒœì…ë‹ˆë‹¤. ë¶ˆí™•ì‹¤ì„±({unc_level}, {unc_score:.1f}ì )ì´ ë†’ì€ë°ë„ "
                f"ë¦¬ìŠ¤í¬ ì„ í˜¸ë„({ra_level}, {ra_score:.1f}ì )ê°€ ë†’ì•„ "
                f"ê³¼ë„í•œ íˆ¬ê¸°ê°€ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê¸‰ê²©í•œ ì¡°ì • ê°€ëŠ¥ì„±ì„ ì£¼ì˜í•´ì•¼ í•©ë‹ˆë‹¤."
            ),
            "STAGNANT": (
                f"ì‹œì¥ì´ ì¹¨ì²´ ìƒíƒœì…ë‹ˆë‹¤. ë¶ˆí™•ì‹¤ì„±({unc_level}, {unc_score:.1f}ì )ê³¼ "
                f"ë¦¬ìŠ¤í¬ ì„ í˜¸ë„({ra_level}, {ra_score:.1f}ì ) ëª¨ë‘ ë‚®ì•„ "
                f"ì‹œì¥ ì°¸ì—¬ìë“¤ì˜ ì‹ ì¤‘í•œ ìì„¸ê°€ ê´€ì°°ë©ë‹ˆë‹¤."
            ),
            "CRISIS": (
                f"ğŸš¨ ìœ„ê¸° ìƒíƒœì…ë‹ˆë‹¤. ë¶ˆí™•ì‹¤ì„±({unc_level}, {unc_score:.1f}ì )ì´ ë†’ì€ë° "
                f"ë¦¬ìŠ¤í¬ ì„ í˜¸ë„({ra_level}, {ra_score:.1f}ì )ê°€ ë‚®ì•„ "
                f"íˆ¬ììë“¤ì´ ìœ„í—˜ì„ íšŒí”¼í•˜ê³  ìˆìŠµë‹ˆë‹¤. ìœ ë™ì„± í™•ë³´ì™€ ë°©ì–´ì  í¬ì§€ì…˜ì´ í•„ìš”í•©ë‹ˆë‹¤."
            ),
            "MIXED": (
                f"ì‹œì¥ ìƒíƒœê°€ í˜¼ì¬ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ë¶ˆí™•ì‹¤ì„±({unc_level}, {unc_score:.1f}ì )ê³¼ "
                f"ë¦¬ìŠ¤í¬ ì„ í˜¸ë„({ra_level}, {ra_score:.1f}ì )ì˜ ì¡°í•©ì´ "
                f"ëª…í™•í•œ ì‹œì¥ ìƒíƒœë¥¼ ë‚˜íƒ€ë‚´ì§€ ì•ŠìŠµë‹ˆë‹¤. ì¶”ê°€ ë¶„ì„ì´ í•„ìš”í•©ë‹ˆë‹¤."
            )
        }
        
        return interpretations.get(market_state, "ë¶„ì„ ê²°ê³¼ë¥¼ í•´ì„í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    def analyze(self, market_data: Dict[str, pd.DataFrame]) -> RiskAppetiteUncertaintyResult:
        """
        ì „ì²´ ë¶„ì„ ì‹¤í–‰
        
        Args:
            market_data: í‹°ì»¤ë³„ ê°€ê²© ë°ì´í„° ë”•ì…”ë„ˆë¦¬
                        í•„ìˆ˜ í‹°ì»¤: SPY, HYG, LQD, XLY, XLP, IWM, GLD, VIX
        
        Returns:
            RiskAppetiteUncertaintyResult ê°ì²´
        """
        # ë¶ˆí™•ì‹¤ì„± ì§€ìˆ˜ ê³„ì‚°
        uncertainty_result = self.calculate_uncertainty_index(market_data)
        
        # ë¦¬ìŠ¤í¬ ì• í¼íƒ€ì´íŠ¸ ì§€ìˆ˜ ê³„ì‚°
        risk_appetite_result = self.calculate_risk_appetite_index(market_data)
        
        # ì‹œì¥ ìƒíƒœ ê²°ì •
        market_state = self.determine_market_state(
            risk_appetite_result['score'],
            uncertainty_result['score']
        )
        
        # í•´ì„ í…ìŠ¤íŠ¸ ìƒì„±
        interpretation = self.generate_interpretation(
            risk_appetite_result['score'],
            risk_appetite_result['level'],
            uncertainty_result['score'],
            uncertainty_result['level'],
            market_state
        )
        
        # ê²°ê³¼ í†µí•©
        all_components = {
            **uncertainty_result['components'],
            **risk_appetite_result['components']
        }
        
        return RiskAppetiteUncertaintyResult(
            timestamp=datetime.now().isoformat(),
            risk_appetite_score=risk_appetite_result['score'],
            uncertainty_score=uncertainty_result['score'],
            risk_appetite_level=risk_appetite_result['level'],
            uncertainty_level=uncertainty_result['level'],
            market_state=market_state,
            components=all_components,
            interpretation=interpretation
        )


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤
    import yaml
    from collectors import DataManager
    
    # ì„¤ì • ë¡œë“œ
    with open('config/tickers.yaml', 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    
    # ë°ì´í„° ìˆ˜ì§‘
    manager = DataManager(lookback_days=60)
    market_data, _ = manager.collect_all(config)
    
    # ë¶„ì„ ì‹¤í–‰
    analyzer = RiskAppetiteUncertaintyIndex(lookback=20)
    result = analyzer.analyze(market_data)
    
    print("\n" + "="*60)
    print("Risk Appetite & Uncertainty Index ë¶„ì„ ê²°ê³¼")
    print("="*60)
    print(f"\níƒ€ì„ìŠ¤íƒ¬í”„: {result.timestamp}")
    print(f"\në¦¬ìŠ¤í¬ ì• í¼íƒ€ì´íŠ¸: {result.risk_appetite_score:.1f}ì  ({result.risk_appetite_level})")
    print(f"ë¶ˆí™•ì‹¤ì„±: {result.uncertainty_score:.1f}ì  ({result.uncertainty_level})")
    print(f"\nì‹œì¥ ìƒíƒœ: {result.market_state}")
    print(f"\ní•´ì„:\n{result.interpretation}")
    print("\nì£¼ìš” êµ¬ì„±ìš”ì†Œ:")
    for key, value in result.components.items():
        if isinstance(value, float):
            print(f"  {key}: {value:.4f}")
        else:
            print(f"  {key}: {value}")

#!/usr/bin/env python3
"""
Market Anomaly Detector - Risk Appetite & Uncertainty Index
============================================================
Bekaert et al. ì—°êµ¬ ê¸°ë°˜ ë¦¬ìŠ¤í¬ ì„ í˜¸ë„ì™€ ë¶ˆí™•ì‹¤ì„±ì„ ë¶„ë¦¬ ì¸¡ì •í•˜ëŠ” ëª¨ë“ˆ

ê²½ì œí•™ì  ë°°ê²½:
- ë¶ˆí™•ì‹¤ì„±(Uncertainty): ì‹œì¥ì˜ ì˜ˆì¸¡ ë¶ˆê°€ëŠ¥ì„±, ë³€ë™ì„± ì±„ë„ë¡œ ì‘ìš©
- ë¦¬ìŠ¤í¬ ì• í¼íƒ€ì´íŠ¸(Risk Appetite): íˆ¬ììë“¤ì˜ ìœ„í—˜ ê°ìˆ˜ ì˜ì§€, í• ì¸ìœ¨ ì±„ë„ë¡œ ì‘ìš©
- VIXë§Œìœ¼ë¡œëŠ” ë‘ ê°œë…ì´ ì„ì—¬ì„œ í•´ì„ ì˜¤ë¥˜ ë°œìƒ
- ë¶„ì‚° ë¦¬ìŠ¤í¬ í”„ë¦¬ë¯¸ì—„ = VIXÂ² - ì‹¤í˜„ë¶„ì‚° (ë¦¬ìŠ¤í¬ ì„ í˜¸ì˜ í”„ë¡ì‹œ)
"""

import pandas as pd
import numpy as np
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass, asdict, field
from datetime import datetime
import warnings

warnings.filterwarnings('ignore')

# Granger Causality ê²€ì •ì„ ìœ„í•œ statsmodels ì„í¬íŠ¸
try:
    from statsmodels.tsa.stattools import grangercausalitytests
    STATSMODELS_AVAILABLE = True
except ImportError:
    STATSMODELS_AVAILABLE = False
    print("Warning: statsmodels not available. Granger Causality tests will be skipped.")


@dataclass
class RegimeResult:
    """
    ë ˆì§ ë¶„ì„ ê²°ê³¼
    
    ê²½ì œí•™ì  ì˜ë¯¸:
    - current_regime: í˜„ì¬ ì‹œì¥ êµ­ë©´ (BULL/BEAR/TRANSITION/CRISIS)
    - regime_confidence: ë ˆì§ íŒë‹¨ì˜ í™•ì‹ ë„ (0-100%)
    - transition_probability: ë ˆì§ ì „í™˜ í™•ë¥  (0-100%)
    - thresholds: í˜„ì¬ ë ˆì§ì— ë§ëŠ” ì„ê³„ê°’ ì„¸íŠ¸ (ë ˆì§ë³„ë¡œ ë‹¤ë¦„)
    """
    timestamp: str
    current_regime: str           # "BULL", "BEAR", "TRANSITION", "CRISIS"
    regime_confidence: float      # 0-100%
    transition_probability: float  # ë ˆì§ ì „í™˜ í™•ë¥  (0-100%)
    transition_direction: str      # "BULL_TO_BEAR", "BEAR_TO_BULL", "STABLE", "UNCERTAIN"
    thresholds: Dict               # í˜„ì¬ ë ˆì§ì— ë§ëŠ” ì„ê³„ê°’ ì„¸íŠ¸
    ma_status: Dict                # MA ìƒíƒœ ì •ë³´
    interpretation: str            # í•´ì„ í…ìŠ¤íŠ¸
    
    def to_dict(self) -> Dict:
        """ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜ (JSON ì§ë ¬í™”ìš©)"""
        return asdict(self)


class EnhancedRegimeDetector:
    """
    ë ˆì§ íƒì§€ ë° ë ˆì§ë³„ ì„ê³„ê°’ ì œê³µ
    
    Maheu & McCurdy ì—°êµ¬ì— ê¸°ë°˜í•˜ì—¬ Bull/Bear/Transition ë ˆì§ì„ íƒì§€í•˜ê³ ,
    ê° ë ˆì§ë³„ë¡œ ë‹¤ë¥¸ ì„ê³„ê°’ ì„¸íŠ¸ë¥¼ ì œê³µí•©ë‹ˆë‹¤. ë ˆì§ ì „í™˜ ê°ì§€ê°€ í•µì‹¬ì…ë‹ˆë‹¤.
    
    ê²½ì œí•™ì  ë°°ê²½:
    - Bullê³¼ Bear ì‹œì¥ì€ ìˆ˜ìµë¥  ë¶„í¬ ìì²´ê°€ ë‹¤ë¦„ (Maheu & McCurdy)
    - Bull: ë‚®ì€ ë³€ë™ì„±, ì–‘ì˜ í‰ê· , ì •ê·œë¶„í¬ì— ê°€ê¹Œì›€
    - Bear: ë†’ì€ ë³€ë™ì„±, ìŒì˜ í‰ê· , fat tail
    - ê°™ì€ -3% í•˜ë½ë„ Bullì—ì„œëŠ” 2Ïƒ ì´ë²¤íŠ¸, Bearì—ì„œëŠ” 1Ïƒ ì´ë²¤íŠ¸
    - ë ˆì§ ì „í™˜ ì´ˆê¸°ì— ì‹ í˜¸ê°€ ê°€ì¥ ê°€ì¹˜ ìˆìŒ
    """
    
    # ë ˆì§ë³„ ì„ê³„ê°’ ì •ì˜
    REGIME_THRESHOLDS = {
        'BULL': {
            'volume_spike': 2.5,      # ê±°ë˜ëŸ‰ ê¸‰ì¦ ê¸°ì¤€ (í‰ê·  ëŒ€ë¹„ ë°°ìˆ˜)
            'ma_deviation': -2.5,     # MA ì´íƒˆ ê¸°ì¤€ (%)
            'zscore_alert': 2.5,      # Z-score ê²½ê³  ê¸°ì¤€
            'vix_warning': 22,        # VIX ê²½ê³  ë ˆë²¨
            'return_alert': -2.0,     # ì¼ê°„ ìˆ˜ìµë¥  ê²½ê³  (%)
        },
        'TRANSITION': {
            'volume_spike': 2.0,
            'ma_deviation': -2.0,
            'zscore_alert': 2.0,
            'vix_warning': 25,
            'return_alert': -1.5,
        },
        'BEAR': {
            'volume_spike': 1.8,
            'ma_deviation': -1.5,
            'zscore_alert': 1.5,
            'vix_warning': 30,
            'return_alert': -1.0,
        },
        'CRISIS': {
            'volume_spike': 1.5,
            'ma_deviation': -1.0,
            'zscore_alert': 1.0,
            'vix_warning': 35,
            'return_alert': -0.5,
        }
    }
    
    def __init__(self, short_ma: int = 20, long_ma: int = 120, crisis_vix: float = 30):
        """
        Args:
            short_ma: ë‹¨ê¸° ì´ë™í‰ê·  ê¸°ê°„ (ê¸°ë³¸ê°’ 20ì¼)
            long_ma: ì¥ê¸° ì´ë™í‰ê·  ê¸°ê°„ (ê¸°ë³¸ê°’ 120ì¼)
            crisis_vix: ìœ„ê¸° íŒë‹¨ VIX ì„ê³„ê°’ (ê¸°ë³¸ê°’ 30)
        """
        self.short_ma = short_ma
        self.long_ma = long_ma
        self.crisis_vix = crisis_vix
        
        # ë ˆì§ íˆìŠ¤í† ë¦¬ ì €ì¥ (ìµœê·¼ 20ì¼)
        self.regime_history: List[str] = []
        self.max_history = 20
    
    def detect_regime(self, spy_data: pd.DataFrame, vix_data: pd.DataFrame) -> str:
        """
        í˜„ì¬ ë ˆì§ íŒë‹¨
        
        ë¡œì§:
        1. CRISIS ì²´í¬ (ìš°ì„ ìˆœìœ„ ìµœê³ )
        2. BULL ì¡°ê±´
        3. BEAR ì¡°ê±´
        4. TRANSITION
        
        Returns:
            str: ë ˆì§ ì´ë¦„
        """
        if spy_data is None or (hasattr(spy_data, 'empty') and spy_data.empty) or (spy_data is not None and 'Close' not in spy_data.columns):
            return "TRANSITION"
        
        close = spy_data['Close']
        ma_short = close.rolling(window=self.short_ma, min_periods=1).mean()
        ma_long = close.rolling(window=self.long_ma, min_periods=1).mean()
        
        current_price = float(close.iloc[-1])
        current_ma_short = float(ma_short.iloc[-1])
        current_ma_long = float(ma_long.iloc[-1])
        
        # 1. CRISIS ì²´í¬ (ìš°ì„ ìˆœìœ„ ìµœê³ )
        vix_value = None
        vix_empty = hasattr(vix_data, 'empty') and vix_data.empty if vix_data is not None else True
        if vix_data is not None and not vix_empty and 'Close' in vix_data.columns:
            vix_value = float(vix_data['Close'].iloc[-1])
        
        if len(close) >= 5:
            return_5d = (current_price / close.iloc[-5] - 1) * 100
        else:
            return_5d = 0.0
        
        if (vix_value is not None and vix_value >= self.crisis_vix) or return_5d < -5.0:
            return "CRISIS"
        
        # 2. BULL ì¡°ê±´
        if not pd.isna(current_ma_long):
            price_above_long = current_price > current_ma_long
            ma_short_above_long = current_ma_short > current_ma_long if not pd.isna(current_ma_short) else False
            
            if price_above_long and ma_short_above_long:
                return "BULL"
        
        # 3. BEAR ì¡°ê±´
        if not pd.isna(current_ma_long):
            price_below_long = current_price < current_ma_long
            ma_short_below_long = current_ma_short < current_ma_long if not pd.isna(current_ma_short) else False
            
            if price_below_long and ma_short_below_long:
                return "BEAR"
        
        # 4. TRANSITION (ê·¸ ì™¸ ëª¨ë“  ê²½ìš°)
        return "TRANSITION"
    
    def calculate_regime_confidence(self, spy_data: pd.DataFrame) -> float:
        """
        ë ˆì§ í™•ì‹ ë„ ê³„ì‚°
        
        Returns:
            float: 0-100 ì‚¬ì´ í™•ì‹ ë„
        """
        if spy_data is None or (hasattr(spy_data, 'empty') and spy_data.empty) or (spy_data is not None and 'Close' not in spy_data.columns):
            return 50.0
        
        close = spy_data['Close']
        ma_short = close.rolling(window=self.short_ma, min_periods=1).mean()
        ma_long = close.rolling(window=self.long_ma, min_periods=1).mean()
        
        current_price = float(close.iloc[-1])
        current_ma_short = float(ma_short.iloc[-1])
        current_ma_long = float(ma_long.iloc[-1])
        
        if pd.isna(current_ma_long) or current_ma_long == 0:
            return 50.0
        
        # 1. í˜„ì¬ê°€ì™€ 120ì¼ MAì˜ ê±°ë¦¬ (0-50ì )
        price_distance = abs((current_price / current_ma_long - 1) * 100)
        price_score = min(50.0, normalize_to_score(price_distance, min_val=0.0, max_val=5.0) * 0.5)
        
        # 2. 20ì¼ MAì™€ 120ì¼ MAì˜ ê±°ë¦¬ (0-30ì )
        if not pd.isna(current_ma_short) and current_ma_long != 0:
            ma_distance = abs((current_ma_short / current_ma_long - 1) * 100)
            ma_score = min(30.0, normalize_to_score(ma_distance, min_val=0.0, max_val=3.0) * 0.3)
        else:
            ma_score = 15.0
        
        # 3. ìµœê·¼ Nì¼ê°„ ë ˆì§ ì¼ê´€ì„± (0-20ì )
        if len(self.regime_history) >= 5:
            from collections import Counter
            recent_regimes = self.regime_history[-5:]
            counter = Counter(recent_regimes)
            most_common_count = counter.most_common(1)[0][1] if counter else 0
            consistency_ratio = most_common_count / len(recent_regimes)
            consistency_score = consistency_ratio * 20.0
        else:
            consistency_score = 10.0
        
        total_confidence = price_score + ma_score + consistency_score
        return min(100.0, max(0.0, total_confidence))
    
    def calculate_transition_probability(self, spy_data: pd.DataFrame, vix_data: pd.DataFrame) -> Tuple[float, str]:
        """
        ë ˆì§ ì „í™˜ í™•ë¥  ê³„ì‚°
        
        Returns:
            Tuple[í™•ë¥ , ë°©í–¥]
        """
        if spy_data is None or (hasattr(spy_data, 'empty') and spy_data.empty) or (spy_data is not None and 'Close' not in spy_data.columns):
            return 0.0, "STABLE"
        
        close = spy_data['Close']
        ma_short = close.rolling(window=self.short_ma, min_periods=1).mean()
        ma_long = close.rolling(window=self.long_ma, min_periods=1).mean()
        
        signals = []
        
        # 1. MA ê·¼ì ‘ë„ ì²´í¬
        if len(ma_short) > 0 and len(ma_long) > 0:
            current_ma_short = float(ma_short.iloc[-1])
            current_ma_long = float(ma_long.iloc[-1])
            
            if not pd.isna(current_ma_short) and not pd.isna(current_ma_long) and current_ma_long != 0:
                ma_distance_pct = abs((current_ma_short / current_ma_long - 1) * 100)
                if ma_distance_pct < 3.0:
                    signals.append(('ma_proximity', 30.0))
        
        # 2. MA ê¸°ìš¸ê¸° ë³€í™” ì²´í¬
        if len(ma_short) >= 10:
            recent_slope = (float(ma_short.iloc[-1]) / float(ma_short.iloc[-5]) - 1) * 100 if len(ma_short) >= 5 else 0
            if len(ma_short) >= 10:
                prev_slope = (float(ma_short.iloc[-5]) / float(ma_short.iloc[-10]) - 1) * 100
                if (recent_slope > 0 and prev_slope < 0) or (recent_slope < 0 and prev_slope > 0):
                    signals.append(('ma_slope_change', 25.0))
        
        # 3. ê±°ë˜ëŸ‰ ì¦ê°€ + ê°€ê²© ì—­ë°©í–¥
        if 'Volume' in spy_data.columns and len(spy_data) >= 20:
            volume = spy_data['Volume']
            volume_ma = volume.rolling(window=20, min_periods=1).mean()
            
            if len(volume) > 0 and len(volume_ma) > 0:
                current_volume = float(volume.iloc[-1])
                avg_volume = float(volume_ma.iloc[-1])
                volume_ratio = current_volume / avg_volume if avg_volume > 0 else 1.0
                
                if len(close) >= 3:
                    return_3d = (float(close.iloc[-1]) / float(close.iloc[-3]) - 1) * 100
                    
                    if volume_ratio > 1.3 and return_3d < -1.0:
                        signals.append(('volume_price_divergence', 20.0))
        
        # 4. VIX ì¶”ì„¸ ì²´í¬
        vix_empty = hasattr(vix_data, 'empty') and vix_data.empty if vix_data is not None else True
        if vix_data is not None and not vix_empty and 'Close' in vix_data.columns:
            vix_close = vix_data['Close']
            if len(vix_close) >= 5:
                vix_trend = []
                for i in range(len(vix_close) - 4, len(vix_close)):
                    if i > 0:
                        change = (float(vix_close.iloc[i]) / float(vix_close.iloc[i-1]) - 1) * 100
                        vix_trend.append(change)
                
                if len(vix_trend) == 4:
                    all_positive = all(x > 0 for x in vix_trend)
                    all_negative = all(x < 0 for x in vix_trend)
                    if all_positive or all_negative:
                        signals.append(('vix_trend', 25.0))
        
        total_probability = min(100.0, sum(prob for _, prob in signals))
        
        current_regime = self.detect_regime(spy_data, vix_data)
        
        if total_probability < 30.0:
            direction = "STABLE"
        elif current_regime == "BULL":
            direction = "BULL_TO_BEAR"
        elif current_regime == "BEAR":
            direction = "BEAR_TO_BULL"
        else:
            direction = "UNCERTAIN"
        
        return total_probability, direction
    
    def get_thresholds_for_regime(self, regime: str) -> Dict:
        """ë ˆì§ì— ë§ëŠ” ì„ê³„ê°’ ì„¸íŠ¸ ë°˜í™˜"""
        return self.REGIME_THRESHOLDS.get(regime, self.REGIME_THRESHOLDS['TRANSITION'])
    
    def get_ma_status(self, spy_data: pd.DataFrame) -> Dict:
        """ì´ë™í‰ê·  ìƒíƒœ ì •ë³´ ë°˜í™˜"""
        if spy_data is None or (hasattr(spy_data, 'empty') and spy_data.empty) or (spy_data is not None and 'Close' not in spy_data.columns):
            return {}
        
        close = spy_data['Close']
        ma_5 = close.rolling(window=5, min_periods=1).mean()
        ma_20 = close.rolling(window=self.short_ma, min_periods=1).mean()
        ma_120 = close.rolling(window=self.long_ma, min_periods=1).mean()
        
        current_price = float(close.iloc[-1])
        current_ma_5 = float(ma_5.iloc[-1]) if not ma_5.empty else None
        current_ma_20 = float(ma_20.iloc[-1]) if not ma_20.empty else None
        current_ma_120 = float(ma_120.iloc[-1]) if not ma_120.empty else None
        
        price_vs_ma20 = ((current_price / current_ma_20 - 1) * 100) if current_ma_20 and current_ma_20 != 0 else None
        price_vs_ma120 = ((current_price / current_ma_120 - 1) * 100) if current_ma_120 and current_ma_120 != 0 else None
        ma20_vs_ma120 = ((current_ma_20 / current_ma_120 - 1) * 100) if current_ma_20 and current_ma_120 and current_ma_120 != 0 else None
        
        ma20_slope = None
        if len(ma_20) >= 5:
            ma20_slope = ((float(ma_20.iloc[-1]) / float(ma_20.iloc[-5]) - 1) * 100) if len(ma_20) >= 5 else None
        
        ma120_slope = None
        if len(ma_120) >= 20:
            ma120_slope = ((float(ma_120.iloc[-1]) / float(ma_120.iloc[-20]) - 1) * 100) if len(ma_120) >= 20 else None
        
        return {
            'ma_5': current_ma_5,
            'ma_20': current_ma_20,
            'ma_120': current_ma_120,
            'price_vs_ma20': price_vs_ma20,
            'price_vs_ma120': price_vs_ma120,
            'ma20_vs_ma120': ma20_vs_ma120,
            'ma20_slope': ma20_slope,
            'ma120_slope': ma120_slope,
        }
    
    def _apply_regime_buffer(self, new_regime: str) -> str:
        """ë ˆì§ ì „í™˜ ë²„í¼ ì ìš© (ê¸‰ê²©í•œ ìŠ¤ìœ„ì¹­ ë°©ì§€)"""
        if len(self.regime_history) == 0:
            return new_regime
        
        last_regime = self.regime_history[-1]
        
        if new_regime == last_regime:
            return new_regime
        
        if new_regime == "CRISIS":
            return new_regime
        
        if len(self.regime_history) >= 2:
            recent_regimes = self.regime_history[-2:]
            if new_regime in recent_regimes:
                return new_regime
        
        return last_regime
    
    def generate_interpretation(self, regime: str, confidence: float, transition_prob: float, transition_dir: str) -> str:
        """í•´ì„ í…ìŠ¤íŠ¸ ìƒì„±"""
        regime_names = {
            "BULL": "ê°•ì„¸ì¥",
            "BEAR": "ì•½ì„¸ì¥",
            "TRANSITION": "ì „í™˜ê¸°",
            "CRISIS": "ìœ„ê¸°"
        }
        
        regime_name = regime_names.get(regime, regime)
        base_text = f"í˜„ì¬ ì‹œì¥ì€ {regime_name} êµ­ë©´ì…ë‹ˆë‹¤. "
        
        if confidence >= 70:
            base_text += f"ë ˆì§ íŒë‹¨ í™•ì‹ ë„ê°€ ë†’ìŠµë‹ˆë‹¤ ({confidence:.1f}%). "
        elif confidence >= 50:
            base_text += f"ë ˆì§ íŒë‹¨ í™•ì‹ ë„ê°€ ë³´í†µì…ë‹ˆë‹¤ ({confidence:.1f}%). "
        else:
            base_text += f"ë ˆì§ íŒë‹¨ í™•ì‹ ë„ê°€ ë‚®ìŠµë‹ˆë‹¤ ({confidence:.1f}%). "
        
        if transition_prob >= 70:
            base_text += f"âš ï¸ ë ˆì§ ì „í™˜ ê°€ëŠ¥ì„±ì´ ë†’ìŠµë‹ˆë‹¤ ({transition_prob:.1f}%). "
            if transition_dir == "BULL_TO_BEAR":
                base_text += "ê°•ì„¸ì¥ì—ì„œ ì•½ì„¸ì¥ìœ¼ë¡œ ì „í™˜ë  ê°€ëŠ¥ì„±ì´ ìˆìŠµë‹ˆë‹¤."
            elif transition_dir == "BEAR_TO_BULL":
                base_text += "ì•½ì„¸ì¥ì—ì„œ ê°•ì„¸ì¥ìœ¼ë¡œ ì „í™˜ë  ê°€ëŠ¥ì„±ì´ ìˆìŠµë‹ˆë‹¤."
            else:
                base_text += f"ì „í™˜ ë°©í–¥: {transition_dir}"
        elif transition_prob >= 50:
            base_text += f"ë ˆì§ ì „í™˜ ê°€ëŠ¥ì„±ì´ ìˆìŠµë‹ˆë‹¤ ({transition_prob:.1f}%). "
        else:
            base_text += f"í˜„ì¬ ë ˆì§ì´ ì•ˆì •ì ì…ë‹ˆë‹¤ (ì „í™˜ í™•ë¥  {transition_prob:.1f}%). "
        
        return base_text
    
    def analyze(self, spy_data: pd.DataFrame, vix_data: pd.DataFrame) -> RegimeResult:
        """ì „ì²´ ë¶„ì„ ì‹¤í–‰"""
        detected_regime = self.detect_regime(spy_data, vix_data)
        final_regime = self._apply_regime_buffer(detected_regime)
        
        self.regime_history.append(final_regime)
        if len(self.regime_history) > self.max_history:
            self.regime_history.pop(0)
        
        confidence = self.calculate_regime_confidence(spy_data)
        transition_prob, transition_dir = self.calculate_transition_probability(spy_data, vix_data)
        thresholds = self.get_thresholds_for_regime(final_regime)
        ma_status = self.get_ma_status(spy_data)
        interpretation = self.generate_interpretation(final_regime, confidence, transition_prob, transition_dir)
        
        return RegimeResult(
            timestamp=datetime.now().isoformat(),
            current_regime=final_regime,
            regime_confidence=confidence,
            transition_probability=transition_prob,
            transition_direction=transition_dir,
            thresholds=thresholds,
            ma_status=ma_status,
            interpretation=interpretation
        )


@dataclass
class SpilloverEdge:
    """
    ìì‚°ê°„ ì¶©ê²© ì „ì´(spillover) ê²½ë¡œ
    
    ê²½ì œí•™ì  ì˜ë¯¸:
    - source: ì¶©ê²©ì´ ë°œìƒí•œ ìì‚° (ìœ„í—˜ ì§„ì›ì§€)
    - target: ì¶©ê²©ì´ ì „ì´ë  ìì‚°
    - edge_type: ì „ì´ ë°©í–¥ (POSITIVE: ê°™ì€ ë°©í–¥, NEGATIVE: ë°˜ëŒ€ ë°©í–¥)
    - adjusted_lag: ë ˆì§ì— ë”°ë¼ ì¡°ì •ëœ ì‹œì°¨ (ìœ„ê¸° ì‹œ ë‹¨ì¶•)
    """
    source: str                    # ì¶œë°œ ë…¸ë“œ (ì˜ˆ: "TLT")
    target: str                    # ë„ì°© ë…¸ë“œ (ì˜ˆ: "QQQ")
    edge_type: str                 # "POSITIVE", "NEGATIVE"
    base_lag: int                  # ê¸°ë³¸ ì‹œì°¨ (ì¼)
    adjusted_lag: int              # ë ˆì§ ì¡°ì •ëœ ì‹œì°¨
    signal_strength: float         # ì‹ í˜¸ ê°•ë„ (0-100)
    is_active: bool               # í˜„ì¬ í™œì„±í™” ì—¬ë¶€
    source_move: float            # ì†ŒìŠ¤ ìì‚° ì›€ì§ì„ (%)
    expected_target_move: str     # ì˜ˆìƒ íƒ€ê²Ÿ ë°©í–¥ ("UP", "DOWN")
    theory_note: str              # ê²½ì œí•™ì  ì„¤ëª…
    category: str = ""            # ê²½ë¡œ ì¹´í…Œê³ ë¦¬: 'liquidity', 'volatility', 'credit', 'concentration', 'rotation'
    
    def to_dict(self) -> Dict:
        """ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜ (JSON ì§ë ¬í™”ìš©)"""
        return asdict(self)


@dataclass
class SpilloverResult:
    """
    Spillover ë„¤íŠ¸ì›Œí¬ ë¶„ì„ ê²°ê³¼
    
    ê²½ì œí•™ì  ì˜ë¯¸:
    - active_paths: í˜„ì¬ í™œì„±í™”ëœ ì¶©ê²© ì „ì´ ê²½ë¡œë“¤
    - risk_score: ì „ì´ ìœ„í—˜ ì ìˆ˜ (í™œì„± ê²½ë¡œ ìˆ˜ì™€ ê°•ë„ ê¸°ë°˜)
    - primary_risk_source: ê°€ì¥ ë§ì€ ê²½ë¡œì˜ ì†ŒìŠ¤ê°€ ë˜ëŠ” ìì‚° (ìœ„í—˜ ì§„ì›ì§€)
    """
    timestamp: str
    active_paths: List[SpilloverEdge]    # í™œì„±í™”ëœ ê²½ë¡œë“¤
    risk_score: float                     # ì „ì´ ìœ„í—˜ ì ìˆ˜ (0-100)
    primary_risk_source: str              # ì£¼ìš” ìœ„í—˜ ì§„ì›ì§€
    expected_impacts: Dict[str, str]      # ìì‚°ë³„ ì˜ˆìƒ ì˜í–¥
    interpretation: str                  # í•´ì„ í…ìŠ¤íŠ¸
    
    def to_dict(self) -> Dict:
        """ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜ (JSON ì§ë ¬í™”ìš©)"""
        d = asdict(self)
        d['active_paths'] = [edge.to_dict() for edge in self.active_paths]
        return d


class SpilloverNetwork:
    """
    ìì‚°ê°„ ì¶©ê²© ì „ì´(spillover) ë„¤íŠ¸ì›Œí¬
    
    Boeckelmann ì—°êµ¬ì— ê¸°ë°˜í•˜ì—¬ ìì‚°ê°„ ì¶©ê²© ì „ì´ë¥¼ ê·¸ë˜í”„ êµ¬ì¡°ë¡œ ëª¨ë¸ë§í•˜ê³ ,
    ê²½ë¡œë³„ ì „ì´ ì‹ í˜¸ë¥¼ íƒì§€í•©ë‹ˆë‹¤.
    
    ê²½ì œí•™ì  ë°°ê²½:
    - ìì‚°ê°„ spillover ê°•ë„ì™€ ì‹œì°¨ê°€ ì‹œê°„ì— ë”°ë¼ ë³€í•¨
    - ìœ„ê¸° ì‹œ: spillover ê°•ë„ ì¦ê°€, ì‹œì°¨ ë‹¨ì¶• (ë¹ ë¥¸ ì „ì´)
    - í‰ì‹œ: spillover ì•½í•¨, ì‹œì°¨ ê¹€ (ëŠë¦° ì „ì´)
    - ê¸ˆìœµ ê²½ë¡œ(ìœ ë™ì„±) vs ì‹¤ë¬¼ ê²½ë¡œ(ê³µê¸‰ë§)ëŠ” ì‹œì°¨ê°€ ë‹¤ë¦„
    
    ë…¸ë“œ: ìì‚°/ìì‚°êµ°
    ì—£ì§€: ê²½ì œí•™ì  ì¸ê³¼ê´€ê³„
    """
    
    # ê²½ë¡œ ì •ì˜ (ê²½ì œí•™ ì´ë¡  ê¸°ë°˜)
    SPILLOVER_PATHS = [
        # === ìœ ë™ì„±/ê¸ˆë¦¬ ê²½ë¡œ ===
        {
            'source': 'TLT',
            'target': 'QQQ', 
            'edge_type': 'POSITIVE',      # TLTâ†“(ê¸ˆë¦¬â†‘) â†’ QQQâ†“
            'base_lag': 3,
            'category': 'liquidity',
            'theory': 'ê¸ˆë¦¬ ìƒìŠ¹ ì‹œ ì„±ì¥ì£¼ í• ì¸ìœ¨ ì¦ê°€ë¡œ ë°¸ë¥˜ì—ì´ì…˜ ì••ë°•'
        },
        {
            'source': 'DXY',
            'target': 'GLD',
            'edge_type': 'NEGATIVE',      # DXYâ†‘ â†’ GLDâ†“
            'base_lag': 1,
            'category': 'liquidity',
            'theory': 'ë‹¬ëŸ¬ ê°•ì„¸ ì‹œ ë‹¬ëŸ¬ í‘œì‹œ ê¸ˆ ê°€ê²© í•˜ë½ ì••ë ¥'
        },
        {
            'source': 'DXY',
            'target': 'EEM',
            'edge_type': 'NEGATIVE',      # DXYâ†‘ â†’ EEMâ†“
            'base_lag': 3,
            'category': 'liquidity',
            'theory': 'ë‹¬ëŸ¬ ê°•ì„¸ ì‹œ ì‹ í¥êµ­ ìê¸ˆ ìœ ì¶œ, ë‹¬ëŸ¬ ë¶€ì±„ ë¶€ë‹´ ì¦ê°€'
        },
        
        # === ë³€ë™ì„±/ê³µí¬ ê²½ë¡œ ===
        {
            'source': '^VIX',
            'target': 'SPY',
            'edge_type': 'NEGATIVE',      # VIXâ†‘ â†’ SPYâ†“
            'base_lag': 1,
            'category': 'volatility',
            'theory': 'VIX ê¸‰ë“± ì‹œ ì˜µì…˜ ë”œëŸ¬ ê°ë§ˆ í—·ì§€ ë§¤ë„, í•˜ë½ ê°€ì†'
        },
        {
            'source': 'VIX',
            'target': 'SPY',
            'edge_type': 'NEGATIVE',      # VIXâ†‘ â†’ SPYâ†“ (ëŒ€ì²´ í‹°ì»¤)
            'base_lag': 1,
            'category': 'volatility',
            'theory': 'VIX ê¸‰ë“± ì‹œ ì˜µì…˜ ë”œëŸ¬ ê°ë§ˆ í—·ì§€ ë§¤ë„, í•˜ë½ ê°€ì†'
        },
        
        # === ì‹ ìš© ê²½ë¡œ ===
        {
            'source': 'HYG',
            'target': 'XLF',
            'edge_type': 'POSITIVE',      # HYGâ†“ â†’ XLFâ†“
            'base_lag': 3,
            'category': 'credit',
            'theory': 'í•˜ì´ì¼ë“œ ìŠ¤í”„ë ˆë“œ í™•ëŒ€ ì‹œ ê¸ˆìœµì„¹í„° ì‹ ìš© ìš°ë ¤'
        },
        {
            'source': 'HYG',
            'target': 'IWM',
            'edge_type': 'POSITIVE',      # HYGâ†“ â†’ IWMâ†“
            'base_lag': 5,
            'category': 'credit',
            'theory': 'ì‹ ìš© ê²½ìƒ‰ ì‹œ ì†Œí˜•ì£¼ ìê¸ˆì¡°ë‹¬ ì–´ë ¤ì›€'
        },
        
        # === ë¹…í…Œí¬/ì§‘ì¤‘ë„ ê²½ë¡œ ===
        {
            'source': 'QQQ',
            'target': 'SPY',
            'edge_type': 'POSITIVE',      # QQQâ†“ â†’ SPYâ†“
            'base_lag': 1,
            'category': 'concentration',
            'theory': 'MAG7ì´ SPYì˜ 30% ì°¨ì§€, ë¹…í…Œí¬ í•˜ë½ì´ ì§€ìˆ˜ ëŒì–´ë‚´ë¦¼'
        },
        {
            'source': 'NVDA',
            'target': 'SMH',
            'edge_type': 'POSITIVE',      # NVDAâ†“ â†’ SMHâ†“
            'base_lag': 1,
            'category': 'concentration',
            'theory': 'AI ëŒ€ì¥ì£¼ ê· ì—´ì´ ë°˜ë„ì²´ ì„¹í„° ì „ì²´ë¡œ ì „ì´'
        },
        
        # === ì„¹í„° ë¡œí…Œì´ì…˜ ê²½ë¡œ ===
        {
            'source': 'XLY',
            'target': 'SPY',
            'edge_type': 'POSITIVE',      # XLYâ†“ â†’ SPYâ†“ (with lag)
            'base_lag': 5,
            'category': 'rotation',
            'theory': 'ê²½ê¸°ë¯¼ê° ì„¹í„° ì•½ì„¸ê°€ ì „ì²´ ì‹œì¥ ì•½ì„¸ë¡œ í™•ì‚°'
        },
    ]
    
    # ë ˆì§ë³„ ì‹œì°¨ ì¡°ì • ê³„ìˆ˜
    LAG_ADJUSTMENTS = {
        'BULL': 1.0,        # ê¸°ë³¸ ì‹œì°¨ ìœ ì§€
        'TRANSITION': 0.8,  # 20% ë‹¨ì¶•
        'BEAR': 0.7,        # 30% ë‹¨ì¶•
        'CRISIS': 0.5,      # 50% ë‹¨ì¶• (ë¹ ë¥¸ ì „ì´)
    }
    
    # í™œì„±í™” ì„ê³„ê°’ (ë ˆì§ë³„) - ì™„í™”ëœ ì„ê³„ê°’
    ACTIVATION_THRESHOLDS = {
        'BULL': {
            'min_move': 1.2,       # ìµœì†Œ 1.2% ì›€ì§ì„ (ê¸°ì¡´ 2.0ì—ì„œ ì™„í™”)
            'volume_ratio': 1.3,   # ê±°ë˜ëŸ‰ 1.3ë°° (ê¸°ì¡´ 1.5ì—ì„œ ì™„í™”)
        },
        'BEAR': {
            'min_move': 1.0,       # ìµœì†Œ 1.0% ì›€ì§ì„ (ê¸°ì¡´ 1.5ì—ì„œ ì™„í™”)
            'volume_ratio': 1.2,   # ê±°ë˜ëŸ‰ 1.2ë°° (ê¸°ì¡´ 1.3ì—ì„œ ì™„í™”)
        },
        'TRANSITION': {
            'min_move': 1.0,       # ìµœì†Œ 1.0% ì›€ì§ì„ (ê¸°ì¡´ 1.8ì—ì„œ ì™„í™”)
            'volume_ratio': 1.2,   # ê±°ë˜ëŸ‰ 1.2ë°° (ê¸°ì¡´ 1.4ì—ì„œ ì™„í™”)
        },
        'CRISIS': {
            'min_move': 0.8,       # ìµœì†Œ 0.8% ì›€ì§ì„ (ê¸°ì¡´ 1.0ì—ì„œ ì™„í™”)
            'volume_ratio': 1.1,   # ê±°ë˜ëŸ‰ 1.1ë°° (ê¸°ì¡´ 1.2ì—ì„œ ì™„í™”)
        }
    }
    
    def __init__(self, lookback: int = 20):
        """
        Args:
            lookback: ë¡¤ë§ ìœˆë„ìš° ê¸°ê°„ (ê¸°ë³¸ê°’ 20ì¼)
        """
        self.lookback = lookback
        self.paths = self.SPILLOVER_PATHS
    
    def adjust_lag_for_regime(self, base_lag: int, regime: str) -> int:
        """
        ë ˆì§ì— ë”°ë¼ ì‹œì°¨ ì¡°ì •
        
        ê²½ì œí•™ì  ì˜ë¯¸:
        - ìœ„ê¸° ì‹œ ì‹œì°¨ê°€ ë‹¨ì¶•ë¨ (ë¹ ë¥¸ ì „ì´)
        - í‰ì‹œì—ëŠ” ì‹œì°¨ê°€ ê¸¸ì–´ì§ (ëŠë¦° ì „ì´)
        
        Returns:
            int: ì¡°ì •ëœ ì‹œì°¨ (ìµœì†Œ 1ì¼)
        """
        adjustment = self.LAG_ADJUSTMENTS.get(regime, 1.0)
        adjusted = max(1, int(base_lag * adjustment))
        return adjusted
    
    def calculate_source_signal(
        self, 
        source_data: pd.DataFrame, 
        lag: int
    ) -> Tuple[float, float]:
        """
        ì†ŒìŠ¤ ìì‚°ì˜ ì‹ í˜¸ ê³„ì‚°
        
        ë¡œì§:
        - lagì¼ ì „ ëŒ€ë¹„ í˜„ì¬ ìˆ˜ìµë¥  ê³„ì‚°
        - ê±°ë˜ëŸ‰ ëŒ€ë¹„ ì´ìƒ ì—¬ë¶€ í™•ì¸
        - ì‹ í˜¸ ê°•ë„ = |ìˆ˜ìµë¥ | Ã— ê±°ë˜ëŸ‰ë¹„ìœ¨ (ì •ê·œí™”)
        
        Returns:
            Tuple[ì›€ì§ì„(%), ì‹ í˜¸ê°•ë„(0-100)]
        """
        if source_data.empty or 'Close' not in source_data.columns:
            return 0.0, 0.0
        
        close = source_data['Close']
        
        if len(close) < lag + 1:
            return 0.0, 0.0
        
        # lagì¼ ì „ ëŒ€ë¹„ í˜„ì¬ ìˆ˜ìµë¥ 
        move_pct = (float(close.iloc[-1]) / float(close.iloc[-lag-1]) - 1) * 100
        
        # ê±°ë˜ëŸ‰ ë¹„ìœ¨ ê³„ì‚°
        volume_ratio = 1.0
        if 'Volume' in source_data.columns and len(source_data) >= self.lookback:
            volume = source_data['Volume']
            current_volume = float(volume.iloc[-1]) if len(volume) > 0 else 0
            avg_volume = float(volume.tail(self.lookback).mean()) if len(volume) >= self.lookback else current_volume
            
            if avg_volume > 0:
                volume_ratio = current_volume / avg_volume
        
        # ì‹ í˜¸ ê°•ë„ ê³„ì‚°: |ì›€ì§ì„| Ã— ê±°ë˜ëŸ‰ë¹„ìœ¨ (ì •ê·œí™”)
        # ì›€ì§ì„ 0-5% ë²”ìœ„ë¥¼ 0-50ì ìœ¼ë¡œ, ê±°ë˜ëŸ‰ë¹„ìœ¨ 1.0-2.0 ë²”ìœ„ë¥¼ 0-50ì ìœ¼ë¡œ
        move_score = min(50.0, abs(move_pct) / 5.0 * 50.0)
        volume_score = min(50.0, (volume_ratio - 1.0) / 1.0 * 50.0) if volume_ratio >= 1.0 else 0.0
        
        signal_strength = min(100.0, move_score + volume_score)
        
        return move_pct, signal_strength
    
    def check_path_activation(
        self, 
        source_data: pd.DataFrame,
        target_data: pd.DataFrame,
        path: Dict,
        regime: str
    ) -> Optional[SpilloverEdge]:
        """
        ê°œë³„ ê²½ë¡œ í™œì„±í™” ì—¬ë¶€ í™•ì¸
        
        í™œì„±í™” ì¡°ê±´:
        1. ì†ŒìŠ¤ ìì‚°ì´ ì„ê³„ê°’ ì´ìƒ ì›€ì§ì„ (ì˜ˆ: 3ì¼ê°„ Â±2%)
        2. ê±°ë˜ëŸ‰ì´ í‰ê·  ëŒ€ë¹„ 1.5ë°° ì´ìƒ
        
        ì‹ í˜¸ ê°•ë„ ê³„ì‚°:
        - ì›€ì§ì„ í¬ê¸° Ã— ê±°ë˜ëŸ‰ ë¹„ìœ¨ Ã— ë ˆì§ ê°€ì¤‘ì¹˜
        
        Returns:
            SpilloverEdge ê°ì²´ (í™œì„±í™”ë˜ì§€ ì•Šìœ¼ë©´ None)
        """
        if source_data.empty or 'Close' not in source_data.columns:
            return None
        
        # ë ˆì§ë³„ ì„ê³„ê°’ ê°€ì ¸ì˜¤ê¸°
        thresholds = self.ACTIVATION_THRESHOLDS.get(regime, self.ACTIVATION_THRESHOLDS['BEAR'])
        min_move = thresholds['min_move']
        min_volume_ratio = thresholds['volume_ratio']
        
        # ì‹œì°¨ ì¡°ì •
        base_lag = path.get('base_lag', 3)
        adjusted_lag = self.adjust_lag_for_regime(base_lag, regime)
        
        # ì†ŒìŠ¤ ì‹ í˜¸ ê³„ì‚°
        source_move, signal_strength = self.calculate_source_signal(source_data, adjusted_lag)
        
        # ê±°ë˜ëŸ‰ ì²´í¬
        volume_ratio = 1.0
        if 'Volume' in source_data.columns and len(source_data) >= self.lookback:
            volume = source_data['Volume']
            current_volume = float(volume.iloc[-1]) if len(volume) > 0 else 0
            avg_volume = float(volume.tail(self.lookback).mean()) if len(volume) >= self.lookback else current_volume
            
            if avg_volume > 0:
                volume_ratio = current_volume / avg_volume
        
        # í™œì„±í™” ì¡°ê±´ ì²´í¬
        abs_move = abs(source_move)
        is_active = (abs_move >= min_move) and (volume_ratio >= min_volume_ratio)
        
        if not is_active:
            return None
        
        # ë ˆì§ ê°€ì¤‘ì¹˜ ì ìš© (ìœ„ê¸° ì‹œ ì‹ í˜¸ ê°•ë„ ì¦ê°€)
        regime_weights = {
            'BULL': 1.0,
            'TRANSITION': 1.1,
            'BEAR': 1.2,
            'CRISIS': 1.5
        }
        weight = regime_weights.get(regime, 1.0)
        final_signal_strength = min(100.0, signal_strength * weight)
        
        # ì˜ˆìƒ íƒ€ê²Ÿ ë°©í–¥ ê²°ì •
        edge_type = path.get('edge_type', 'POSITIVE')
        if edge_type == 'POSITIVE':
            # ê°™ì€ ë°©í–¥: sourceê°€ í•˜ë½í•˜ë©´ targetë„ í•˜ë½
            expected_direction = "DOWN" if source_move < 0 else "UP"
        else:  # NEGATIVE
            # ë°˜ëŒ€ ë°©í–¥: sourceê°€ ìƒìŠ¹í•˜ë©´ targetì€ í•˜ë½
            expected_direction = "DOWN" if source_move > 0 else "UP"
        
        return SpilloverEdge(
            source=path['source'],
            target=path['target'],
            edge_type=edge_type,
            base_lag=base_lag,
            adjusted_lag=adjusted_lag,
            signal_strength=final_signal_strength,
            is_active=True,
            source_move=source_move,
            expected_target_move=expected_direction,
            theory_note=path.get('theory', ''),
            category=path.get('category', '')
        )
    
    def get_expected_impacts(self, active_paths: List[SpilloverEdge]) -> Dict[str, str]:
        """
        í™œì„±í™”ëœ ê²½ë¡œ ê¸°ë°˜ìœ¼ë¡œ ê° ìì‚° ì˜ˆìƒ ì˜í–¥
        
        ì—¬ëŸ¬ ê²½ë¡œê°€ ê°™ì€ íƒ€ê²Ÿì„ ê°€ë¦¬í‚¬ ê²½ìš°, ì‹ í˜¸ ê°•ë„ê°€ ë†’ì€ ê²½ë¡œ ìš°ì„ 
        
        Returns:
            Dict[ticker, expected_direction]
            ì˜ˆ: {"QQQ": "DOWN", "GLD": "DOWN", "SPY": "DOWN"}
        """
        impacts = {}
        
        # íƒ€ê²Ÿë³„ë¡œ ì‹ í˜¸ ê°•ë„ê°€ ê°€ì¥ ë†’ì€ ê²½ë¡œ ì„ íƒ
        target_paths = {}
        for edge in active_paths:
            target = edge.target
            if target not in target_paths or edge.signal_strength > target_paths[target].signal_strength:
                target_paths[target] = edge
        
        # ì˜ˆìƒ ì˜í–¥ ê²°ì •
        for target, edge in target_paths.items():
            impacts[target] = edge.expected_target_move
        
        return impacts
    
    def identify_risk_source(self, active_paths: List[SpilloverEdge]) -> str:
        """
        ê°€ì¥ ë§ì€ ê²½ë¡œì˜ ì†ŒìŠ¤ê°€ ë˜ëŠ” ìì‚° = ìœ„í—˜ ì§„ì›ì§€
        
        Returns:
            str: í‹°ì»¤ ì´ë¦„ (í™œì„± ê²½ë¡œê°€ ì—†ìœ¼ë©´ "NONE")
        """
        if not active_paths:
            return "NONE"
        
        # ì†ŒìŠ¤ë³„ ê²½ë¡œ ìˆ˜ ì§‘ê³„
        from collections import Counter
        source_counts = Counter(edge.source for edge in active_paths)
        
        if not source_counts:
            return "NONE"
        
        # ê°€ì¥ ë§ì€ ê²½ë¡œë¥¼ ê°€ì§„ ì†ŒìŠ¤ ë°˜í™˜
        primary_source = source_counts.most_common(1)[0][0]
        return primary_source
    
    def calculate_risk_score(self, active_paths: List[SpilloverEdge], regime: str) -> float:
        """
        ì „ì´ ìœ„í—˜ ì ìˆ˜ ê³„ì‚° (0-100)
        
        ë¡œì§:
        - í™œì„± ê²½ë¡œ ìˆ˜ (ìµœëŒ€ 50ì )
        - í‰ê·  ì‹ í˜¸ ê°•ë„ (ìµœëŒ€ 50ì )
        - ë ˆì§ ê°€ì¤‘ì¹˜ ì ìš©
        
        Returns:
            float: ìœ„í—˜ ì ìˆ˜ (0-100)
        """
        if not active_paths:
            return 0.0
        
        # í™œì„± ê²½ë¡œ ìˆ˜ ì ìˆ˜ (ìµœëŒ€ 50ì )
        num_paths = len(active_paths)
        path_score = min(50.0, num_paths / 10.0 * 50.0)  # 10ê°œ ê²½ë¡œ = 50ì 
        
        # í‰ê·  ì‹ í˜¸ ê°•ë„ ì ìˆ˜ (ìµœëŒ€ 50ì )
        avg_strength = sum(edge.signal_strength for edge in active_paths) / len(active_paths)
        strength_score = avg_strength * 0.5  # ìµœëŒ€ 50ì 
        
        base_score = path_score + strength_score
        
        # ë ˆì§ ê°€ì¤‘ì¹˜
        regime_weights = {
            'BULL': 0.8,
            'TRANSITION': 1.0,
            'BEAR': 1.2,
            'CRISIS': 1.5
        }
        weight = regime_weights.get(regime, 1.0)
        
        final_score = min(100.0, base_score * weight)
        return final_score
    
    def generate_interpretation(
        self,
        active_paths: List[SpilloverEdge],
        risk_score: float,
        primary_source: str,
        expected_impacts: Dict[str, str]
    ) -> str:
        """
        ë¶„ì„ ê²°ê³¼ í•´ì„ í…ìŠ¤íŠ¸ ìƒì„±
        
        Returns:
            str: í•´ì„ í…ìŠ¤íŠ¸
        """
        if not active_paths:
            return "í˜„ì¬ í™œì„±í™”ëœ ì¶©ê²© ì „ì´ ê²½ë¡œê°€ ì—†ìŠµë‹ˆë‹¤. ì‹œì¥ì´ ìƒëŒ€ì ìœ¼ë¡œ ì•ˆì •ì ì…ë‹ˆë‹¤."
        
        base_text = f"ì´ {len(active_paths)}ê°œì˜ ì¶©ê²© ì „ì´ ê²½ë¡œê°€ í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤. "
        
        # ìœ„í—˜ ì ìˆ˜ í•´ì„
        if risk_score >= 70:
            base_text += f"âš ï¸ ì „ì´ ìœ„í—˜ ì ìˆ˜ê°€ ë†’ìŠµë‹ˆë‹¤ ({risk_score:.1f}ì ). "
        elif risk_score >= 50:
            base_text += f"ì „ì´ ìœ„í—˜ ì ìˆ˜ê°€ ë³´í†µì…ë‹ˆë‹¤ ({risk_score:.1f}ì ). "
        else:
            base_text += f"ì „ì´ ìœ„í—˜ ì ìˆ˜ê°€ ë‚®ìŠµë‹ˆë‹¤ ({risk_score:.1f}ì ). "
        
        # ì£¼ìš” ìœ„í—˜ ì§„ì›ì§€
        if primary_source != "NONE":
            base_text += f"ì£¼ìš” ìœ„í—˜ ì§„ì›ì§€ëŠ” {primary_source}ì…ë‹ˆë‹¤. "
        
        # ì˜ˆìƒ ì˜í–¥
        if expected_impacts:
            down_assets = [ticker for ticker, direction in expected_impacts.items() if direction == "DOWN"]
            up_assets = [ticker for ticker, direction in expected_impacts.items() if direction == "UP"]
            
            if down_assets:
                base_text += f"í•˜ë½ ì••ë ¥ì´ ì˜ˆìƒë˜ëŠ” ìì‚°: {', '.join(down_assets)}. "
            if up_assets:
                base_text += f"ìƒìŠ¹ ì••ë ¥ì´ ì˜ˆìƒë˜ëŠ” ìì‚°: {', '.join(up_assets)}. "
        
        return base_text
    
    def analyze(
        self, 
        market_data: Dict[str, pd.DataFrame],
        regime: str
    ) -> SpilloverResult:
        """
        ì „ì²´ ë„¤íŠ¸ì›Œí¬ ë¶„ì„
        
        1. ê° ê²½ë¡œë³„ í™œì„±í™” ì—¬ë¶€ í™•ì¸
        2. í™œì„±í™”ëœ ê²½ë¡œë“¤ì˜ ìœ„í—˜ë„ í•©ì‚°
        3. ì£¼ìš” ìœ„í—˜ ì§„ì›ì§€ ì‹ë³„
        4. íƒ€ê²Ÿ ìì‚°ë³„ ì˜ˆìƒ ì˜í–¥ ì •ë¦¬
        
        Args:
            market_data: í‹°ì»¤ë³„ ê°€ê²© ë°ì´í„° ë”•ì…”ë„ˆë¦¬
            regime: í˜„ì¬ ë ˆì§ ("BULL", "BEAR", "TRANSITION", "CRISIS")
        
        Returns:
            SpilloverResult ê°ì²´
        """
        active_paths = []
        
        # ê° ê²½ë¡œë³„ í™œì„±í™” ì—¬ë¶€ í™•ì¸
        for path in self.paths:
            source_ticker = path['source']
            target_ticker = path['target']
            
            # ì†ŒìŠ¤ ë°ì´í„° ê°€ì ¸ì˜¤ê¸° (í‹°ì»¤ëª… ë³€í˜• ì‹œë„)
            source_data = market_data.get(source_ticker)
            if source_data is None:
                # ëŒ€ì²´ í‹°ì»¤ ì‹œë„ (ì˜ˆ: ^VIX -> VIX)
                alt_source = source_ticker.replace('^', '')
                source_data = market_data.get(alt_source)
            
            # íƒ€ê²Ÿ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
            target_data = market_data.get(target_ticker)
            
            if source_data is None or target_data is None:
                continue
            
            # ê²½ë¡œ í™œì„±í™” ì²´í¬
            edge = self.check_path_activation(source_data, target_data, path, regime)
            if edge is not None:
                active_paths.append(edge)
        
        # ìœ„í—˜ ì ìˆ˜ ê³„ì‚°
        risk_score = self.calculate_risk_score(active_paths, regime)
        
        # ì£¼ìš” ìœ„í—˜ ì§„ì›ì§€ ì‹ë³„
        primary_source = self.identify_risk_source(active_paths)
        
        # ì˜ˆìƒ ì˜í–¥ ê³„ì‚°
        expected_impacts = self.get_expected_impacts(active_paths)
        
        # í•´ì„ í…ìŠ¤íŠ¸ ìƒì„±
        interpretation = self.generate_interpretation(
            active_paths,
            risk_score,
            primary_source,
            expected_impacts
        )
        
        return SpilloverResult(
            timestamp=datetime.now().isoformat(),
            active_paths=active_paths,
            risk_score=risk_score,
            primary_risk_source=primary_source,
            expected_impacts=expected_impacts,
            interpretation=interpretation
        )

# critical_path_analyzer.py íŒŒì¼ ëì— ì¶”ê°€

@dataclass
class CryptoSentimentResult:
    """
    ì•”í˜¸í™”í ì‹¬ë¦¬ ë¶„ì„ ê²°ê³¼
    
    ê²½ì œí•™ì  ì˜ë¯¸:
    - sentiment_score: ì•”í˜¸í™”í ì‹œì¥ ì‹¬ë¦¬ ì ìˆ˜ (0-100)
    - btc_spy_correlation: BTC-ì£¼ì‹ ìƒê´€ê´€ê³„ (ë ˆì§ì— ë”°ë¼ ë‹¤ë¦„)
    - correlation_regime: ìƒê´€ê´€ê³„ ê¸°ë°˜ ë ˆì§ (DECOUPLED/COUPLED/CRISIS_COUPLED)
    - is_leading_indicator: ì„ í–‰ì§€í‘œë¡œ ì‘ë™ ì¤‘ì¸ì§€ ì—¬ë¶€
    - risk_contribution: ì „ì²´ ìœ„í—˜ë„ì— ê¸°ì—¬í•˜ëŠ” ë¹„ì¤‘ (ìœ„ê¸° ì‹œ ì¦ê°€)
    - causality_analysis: Granger Causality ê²€ì • ê²°ê³¼ (ì¸ê³¼ê´€ê³„ ë°©í–¥ì„±)
    """
    timestamp: str
    sentiment_score: float             # 0-100
    sentiment_level: str               # "EXTREME_FEAR", "FEAR", "NEUTRAL", "GREED", "EXTREME_GREED"
    btc_spy_correlation: float         # 20ì¼ ë¡¤ë§ ìƒê´€ê´€ê³„
    correlation_regime: str            # "DECOUPLED", "COUPLED", "CRISIS_COUPLED"
    is_leading_indicator: bool         # ì„ í–‰ì§€í‘œë¡œ ì‘ë™ ì¤‘ì¸ì§€
    leading_signal: Optional[str]      # "RISK_OFF_WARNING", "RISK_ON_SIGNAL", None
    risk_contribution: float           # ì „ì²´ ìœ„í—˜ë„ì— ê¸°ì—¬í•˜ëŠ” ë¹„ì¤‘ (0-20%)
    components: Dict                   # ê°œë³„ ì§€í‘œ ê°’ë“¤
    interpretation: str                # í•´ì„ í…ìŠ¤íŠ¸
    causality_analysis: Dict = field(default_factory=lambda: {})  # Granger Causality ê²€ì • ê²°ê³¼
    
    def to_dict(self) -> Dict:
        """ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜ (JSON ì§ë ¬í™”ìš©)"""
        return asdict(self)


class CryptoSentimentBlock:
    """
    ì•”í˜¸í™”í ì‹¬ë¦¬ ì§€í‘œ ë¸”ë¡
    
    IMF ì—°êµ¬ ë° State-Dependent ì—°êµ¬ì— ê¸°ë°˜í•˜ì—¬ ì•”í˜¸í™”íë¥¼ ë³„ë„ ë¸”ë¡ìœ¼ë¡œ ë¶„ë¦¬í•˜ê³ ,
    ë ˆì§ì— ë”°ë¼ ë‹¤ë¥´ê²Œ í•´ì„í•©ë‹ˆë‹¤.
    
    ê²½ì œí•™ì  ë°°ê²½:
    - BTC-ì£¼ì‹ ìƒê´€ê´€ê³„ê°€ ë ˆì§ì— ë”°ë¼ ê·¹ì ìœ¼ë¡œ ë‹¤ë¦„
    - í‰ì‹œ: ë‚®ì€ ìƒê´€ê´€ê³„ (0.1-0.3), ë…ìì  ì›€ì§ì„
    - ìœ„ê¸°ì‹œ: ë†’ì€ ìƒê´€ê´€ê³„ (0.6-0.8), ë™ë°˜ í•˜ë½
    - ì•”í˜¸í™”íê°€ "ìœ ë™ì„±ì˜ ì¹´ë‚˜ë¦¬ì•„" ì—­í•  ê°€ëŠ¥ì„± (IMF)
    - BTCâ†’ì£¼ì‹ ë°©í–¥ spilloverê°€ ë°˜ëŒ€ë³´ë‹¤ ê°•í•¨ (íŠ¹íˆ ìœ„ê¸° ì‹œ)
    
    í•µì‹¬ íŠ¹ì§•:
    - í‰ì‹œ/ìœ„ê¸°ì‹œ í•´ì„ì´ ë‹¤ë¦„ (state-dependent)
    - ì„ í–‰ì„± í…ŒìŠ¤íŠ¸ ë‚´ì¥
    - ë ˆì§ì— ë”°ë¼ ì „ì²´ ìœ„í—˜ë„ ê¸°ì—¬ë„ ë³€ë™
    """
    
    # ìƒê´€ê´€ê³„ ê¸°ë°˜ ë ˆì§ ì •ì˜
    CORRELATION_REGIMES = {
        'DECOUPLED': {'min': -0.2, 'max': 0.3},      # ë…ìì  ì›€ì§ì„
        'COUPLED': {'min': 0.3, 'max': 0.6},         # ì—°ë™
        'CRISIS_COUPLED': {'min': 0.6, 'max': 1.0},  # ìœ„ê¸° ë™ì¡°í™”
    }
    
    # ë ˆì§ë³„ ìœ„í—˜ ê¸°ì—¬ë„
    RISK_CONTRIBUTION = {
        'DECOUPLED': 0.05,       # 5% - ë…ì ì‹ í˜¸ë¡œë§Œ í•´ì„
        'COUPLED': 0.10,         # 10%
        'CRISIS_COUPLED': 0.20,  # 20% - ì„ í–‰ì§€í‘œë¡œ ì¤‘ìš”
    }
    
    def __init__(self, lookback: int = 20, correlation_window: int = 20):
        """
        Args:
            lookback: ë¡¤ë§ ìœˆë„ìš° ê¸°ê°„ (ê¸°ë³¸ê°’ 20ì¼)
            correlation_window: ìƒê´€ê´€ê³„ ê³„ì‚° ìœˆë„ìš° (ê¸°ë³¸ê°’ 20ì¼)
        """
        self.lookback = lookback
        self.correlation_window = correlation_window
    
    def calculate_btc_momentum(self, btc_data: pd.DataFrame) -> Dict:
        """
        BTC ëª¨ë©˜í…€ ê³„ì‚°
        
        ì§€í‘œ:
        - 5ì¼ ìˆ˜ìµë¥ 
        - 20ì¼ ìˆ˜ìµë¥ 
        - 5ì¼ MA vs 20ì¼ MA ìœ„ì¹˜
        - ê±°ë˜ëŸ‰ ë³€í™”
        
        Returns:
            Dict with momentum indicators
        """
        if btc_data is None or (hasattr(btc_data, 'empty') and btc_data.empty) or (btc_data is not None and 'Close' not in btc_data.columns):
            return {
                'return_5d': 0.0,
                'return_20d': 0.0,
                'ma5_above_ma20': False,
                'volume_trend': 0.0,
            }
        
        close = btc_data['Close']
        
        # ìˆ˜ìµë¥  ê³„ì‚°
        if len(close) >= 5:
            return_5d = (float(close.iloc[-1]) / float(close.iloc[-5]) - 1) * 100
        else:
            return_5d = 0.0
        
        if len(close) >= 20:
            return_20d = (float(close.iloc[-1]) / float(close.iloc[-20]) - 1) * 100
        else:
            return_20d = 0.0
        
        # ì´ë™í‰ê·  ê³„ì‚°
        ma_5 = close.rolling(window=5, min_periods=1).mean()
        ma_20 = close.rolling(window=20, min_periods=1).mean()
        
        ma5_above_ma20 = False
        if len(ma_5) > 0 and len(ma_20) > 0:
            current_ma5 = float(ma_5.iloc[-1])
            current_ma20 = float(ma_20.iloc[-1])
            ma5_above_ma20 = current_ma5 > current_ma20 if not pd.isna(current_ma5) and not pd.isna(current_ma20) else False
        
        # ê±°ë˜ëŸ‰ ì¶”ì„¸
        volume_trend = 0.0
        if 'Volume' in btc_data.columns and len(btc_data) >= 20:
            volume = btc_data['Volume']
            if len(volume) >= 20:
                recent_volume = float(volume.tail(5).mean()) if len(volume) >= 5 else 0
                avg_volume = float(volume.tail(20).mean())
                if avg_volume > 0:
                    volume_trend = (recent_volume / avg_volume - 1) * 100
        
        return {
            'return_5d': return_5d,
            'return_20d': return_20d,
            'ma5_above_ma20': ma5_above_ma20,
            'volume_trend': volume_trend,
        }
    
    def calculate_btc_spy_correlation(
        self, 
        btc_data: pd.DataFrame,
        spy_data: pd.DataFrame
    ) -> float:
        """
        BTC-SPY ë¡¤ë§ ìƒê´€ê´€ê³„ ê³„ì‚°
        
        Returns:
            float: ìƒê´€ê³„ìˆ˜ (-1 to 1)
        """
        btc_empty = hasattr(btc_data, 'empty') and btc_data.empty if btc_data is not None else True
        spy_empty = hasattr(spy_data, 'empty') and spy_data.empty if spy_data is not None else True
        if btc_empty or spy_empty:
            return 0.0
        
        if 'Close' not in btc_data.columns or 'Close' not in spy_data.columns:
            return 0.0
        
        btc_close = btc_data['Close']
        spy_close = spy_data['Close']
        
        # ì¸ë±ìŠ¤ ì •ë ¬
        common_index = btc_close.index.intersection(spy_close.index)
        if len(common_index) < self.correlation_window:
            return 0.0
        
        # ìµœê·¼ Nì¼ê°„ ìˆ˜ìµë¥  ê³„ì‚°
        btc_returns = btc_close.loc[common_index].pct_change().dropna()
        spy_returns = spy_close.loc[common_index].pct_change().dropna()
        
        # ê³µí†µ ì¸ë±ìŠ¤ë¡œ ì •ë ¬
        common_returns_index = btc_returns.index.intersection(spy_returns.index)
        if len(common_returns_index) < self.correlation_window:
            return 0.0
        
        btc_recent = btc_returns.loc[common_returns_index].tail(self.correlation_window)
        spy_recent = spy_returns.loc[common_returns_index].tail(self.correlation_window)
        
        if len(btc_recent) < self.correlation_window or len(spy_recent) < self.correlation_window:
            return 0.0
        
        # ìƒê´€ê´€ê³„ ê³„ì‚°
        correlation = float(btc_recent.corr(spy_recent))
        
        return correlation if not pd.isna(correlation) else 0.0
    
    def calculate_granger_causality(
        self,
        series_x: pd.Series,  # ì›ì¸ í›„ë³´ ì‹œê³„ì—´ (ì˜ˆ: BTC)
        series_y: pd.Series,  # ê²°ê³¼ í›„ë³´ ì‹œê³„ì—´ (ì˜ˆ: SPY)
        max_lag: int = 5      # ìµœëŒ€ ì‹œì°¨
    ) -> Dict:
        """
        Granger Causality ê²€ì • ìˆ˜í–‰
        
        ê²½ì œí•™ì  ë°°ê²½:
        - Granger(1969): "Xì˜ ê³¼ê±°ê°’ì´ Y ì˜ˆì¸¡ì— ë„ì›€ì´ ë˜ëŠ”ê°€?"
        - ìƒê´€ê´€ê³„ëŠ” ì¸ê³¼ê´€ê³„ê°€ ì•„ë‹˜ (Correlation â‰  Causation)
        - Granger CausalityëŠ” í†µê³„ì  ì¸ê³¼ê´€ê³„ë¥¼ ê²€ì •
        
        Args:
            series_x: ì›ì¸ í›„ë³´ ì‹œê³„ì—´ (ì˜ˆ: BTC ìˆ˜ìµë¥ )
            series_y: ê²°ê³¼ í›„ë³´ ì‹œê³„ì—´ (ì˜ˆ: SPY ìˆ˜ìµë¥ )
            max_lag: ìµœëŒ€ ì‹œì°¨ (ê¸°ë³¸ê°’ 5ì¼)
        
        Returns:
            {
                'x_causes_y': bool,        # Xê°€ Yë¥¼ Granger-causeí•˜ëŠ”ì§€
                'y_causes_x': bool,        # Yê°€ Xë¥¼ Granger-causeí•˜ëŠ”ì§€
                'x_to_y_pvalue': float,    # X->Y ê²€ì •ì˜ p-value
                'y_to_x_pvalue': float,    # Y->X ê²€ì •ì˜ p-value
                'optimal_lag': int,        # ìµœì  ì‹œì°¨
                'relationship': str        # "X_LEADS", "Y_LEADS", "BIDIRECTIONAL", "NO_CAUSALITY"
            }
        """
        if not STATSMODELS_AVAILABLE:
            return {
                'x_causes_y': False,
                'y_causes_x': False,
                'x_to_y_pvalue': 1.0,
                'y_to_x_pvalue': 1.0,
                'optimal_lag': 0,
                'relationship': 'NO_CAUSALITY'
            }
        
        # ë°ì´í„° ì •ë ¬ ë° ì •ê·œí™”
        common_index = series_x.index.intersection(series_y.index)
        if len(common_index) < max_lag + 10:  # ìµœì†Œ ë°ì´í„° ìš”êµ¬ì‚¬í•­
            return {
                'x_causes_y': False,
                'y_causes_x': False,
                'x_to_y_pvalue': 1.0,
                'y_to_x_pvalue': 1.0,
                'optimal_lag': 0,
                'relationship': 'NO_CAUSALITY'
            }
        
        x_aligned = series_x.loc[common_index].dropna()
        y_aligned = series_y.loc[common_index].dropna()
        
        # ê³µí†µ ì¸ë±ìŠ¤ë¡œ ì •ë ¬
        common_idx = x_aligned.index.intersection(y_aligned.index)
        if len(common_idx) < max_lag + 10:
            return {
                'x_causes_y': False,
                'y_causes_x': False,
                'x_to_y_pvalue': 1.0,
                'y_to_x_pvalue': 1.0,
                'optimal_lag': 0,
                'relationship': 'NO_CAUSALITY'
            }
        
        x_data = x_aligned.loc[common_idx].values
        y_data = y_aligned.loc[common_idx].values
        
        # ê²°ì¸¡ê°’ ì œê±°
        valid_mask = ~(np.isnan(x_data) | np.isnan(y_data))
        x_data = x_data[valid_mask]
        y_data = y_data[valid_mask]
        
        if len(x_data) < max_lag + 10 or len(y_data) < max_lag + 10:
            return {
                'x_causes_y': False,
                'y_causes_x': False,
                'x_to_y_pvalue': 1.0,
                'y_to_x_pvalue': 1.0,
                'optimal_lag': 0,
                'relationship': 'NO_CAUSALITY'
            }
        
        # DataFrame ìƒì„± (grangercausalitytestsëŠ” 2ì—´ DataFrame í•„ìš”)
        data_xy = pd.DataFrame({'y': y_data, 'x': x_data})
        data_yx = pd.DataFrame({'x': x_data, 'y': y_data})
        
        # X->Y ê²€ì •
        x_to_y_pvalue = 1.0
        x_causes_y = False
        optimal_lag_xy = 1
        
        try:
            # ê° ì‹œì°¨ë³„ë¡œ ê²€ì •í•˜ê³  ìµœì†Œ p-value ì°¾ê¸°
            min_pvalue_xy = 1.0
            for lag in range(1, min(max_lag + 1, len(data_xy) // 10)):
                try:
                    gc_result = grangercausalitytests(data_xy, maxlag=lag, verbose=False)
                    # grangercausalitytests ë°˜í™˜ê°’: {lag: (test_results_dict, test_statistics)}
                    # test_results_dictì˜ í‚¤: 'ssr_ftest', 'ssr_chi2test', 'lrtest', 'params_ftest'
                    if lag in gc_result:
                        test_results = gc_result[lag][0]
                        # F-test p-value ì¶”ì¶œ
                        if 'ssr_ftest' in test_results:
                            pvalue = test_results['ssr_ftest'][1]  # (F-stat, p-value)
                            if pvalue < min_pvalue_xy:
                                min_pvalue_xy = pvalue
                                optimal_lag_xy = lag
                except (KeyError, IndexError, TypeError, ValueError) as e:
                    # íŠ¹ì • ì‹œì°¨ì—ì„œ ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰
                    continue
            
            x_to_y_pvalue = min_pvalue_xy
            x_causes_y = x_to_y_pvalue < 0.05  # 5% ìœ ì˜ìˆ˜ì¤€
        except Exception as e:
            # ê²€ì • ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’ ìœ ì§€
            pass
        
        # Y->X ê²€ì •
        y_to_x_pvalue = 1.0
        y_causes_x = False
        optimal_lag_yx = 1
        
        try:
            min_pvalue_yx = 1.0
            for lag in range(1, min(max_lag + 1, len(data_yx) // 10)):
                try:
                    gc_result = grangercausalitytests(data_yx, maxlag=lag, verbose=False)
                    if lag in gc_result:
                        test_results = gc_result[lag][0]
                        if 'ssr_ftest' in test_results:
                            pvalue = test_results['ssr_ftest'][1]
                            if pvalue < min_pvalue_yx:
                                min_pvalue_yx = pvalue
                                optimal_lag_yx = lag
                except (KeyError, IndexError, TypeError, ValueError) as e:
                    continue
            
            y_to_x_pvalue = min_pvalue_yx
            y_causes_x = y_to_x_pvalue < 0.05
        except Exception as e:
            # ê²€ì • ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’ ìœ ì§€
            pass
        
        # ê´€ê³„ ìœ í˜• ê²°ì •
        optimal_lag = max(optimal_lag_xy, optimal_lag_yx)
        
        if x_causes_y and y_causes_x:
            relationship = "BIDIRECTIONAL"
        elif x_causes_y:
            relationship = "X_LEADS"
        elif y_causes_x:
            relationship = "Y_LEADS"
        else:
            relationship = "NO_CAUSALITY"
        
        return {
            'x_causes_y': x_causes_y,
            'y_causes_x': y_causes_x,
            'x_to_y_pvalue': float(x_to_y_pvalue),
            'y_to_x_pvalue': float(y_to_x_pvalue),
            'optimal_lag': optimal_lag,
            'relationship': relationship
        }
    
    def determine_correlation_regime(self, correlation: float) -> str:
        """
        ìƒê´€ê´€ê³„ ê¸°ë°˜ ë ˆì§ íŒë‹¨
        
        Returns:
            str: "DECOUPLED", "COUPLED", "CRISIS_COUPLED"
        """
        for regime, bounds in self.CORRELATION_REGIMES.items():
            if bounds['min'] <= correlation <= bounds['max']:
                return regime
        
        # ê¸°ë³¸ê°’
        if correlation < 0.3:
            return "DECOUPLED"
        elif correlation < 0.6:
            return "COUPLED"
        else:
            return "CRISIS_COUPLED"
    
    def calculate_sentiment_score(self, btc_data: pd.DataFrame) -> Tuple[float, str]:
        """
        ì‹¬ë¦¬ ì ìˆ˜ ê³„ì‚° (Crypto Fear & Greed ìœ ì‚¬ ê°œë…)
        
        êµ¬ì„±ìš”ì†Œ:
        1. ëª¨ë©˜í…€ (40%): 5ì¼/20ì¼ ìˆ˜ìµë¥ 
        2. ë³€ë™ì„± (20%): ìµœê·¼ ë³€ë™ì„± vs í‰ê· 
        3. ê±°ë˜ëŸ‰ (20%): ê±°ë˜ëŸ‰ ì¶”ì„¸
        4. MA ìœ„ì¹˜ (20%): 5MA vs 20MA
        
        Returns:
            Tuple[score (0-100), level]
        
        Level ì •ì˜:
        - 0-20: EXTREME_FEAR
        - 20-40: FEAR
        - 40-60: NEUTRAL
        - 60-80: GREED
        - 80-100: EXTREME_GREED
        """
        if btc_data is None or (hasattr(btc_data, 'empty') and btc_data.empty) or (btc_data is not None and 'Close' not in btc_data.columns):
            return 50.0, "NEUTRAL"
        
        close = btc_data['Close']
        
        # 1. ëª¨ë©˜í…€ ì ìˆ˜ (40%)
        momentum = self.calculate_btc_momentum(btc_data)
        return_5d = momentum['return_5d']
        return_20d = momentum['return_20d']
        
        # 5ì¼ ìˆ˜ìµë¥  ì •ê·œí™” (-10% ~ +10% â†’ 0-100)
        momentum_5d_score = normalize_to_score(return_5d, min_val=-10.0, max_val=10.0)
        # 20ì¼ ìˆ˜ìµë¥  ì •ê·œí™” (-20% ~ +20% â†’ 0-100)
        momentum_20d_score = normalize_to_score(return_20d, min_val=-20.0, max_val=20.0)
        momentum_score = (momentum_5d_score * 0.6 + momentum_20d_score * 0.4) * 0.4
        
        # 2. ë³€ë™ì„± ì ìˆ˜ (20%) - ë‚®ì€ ë³€ë™ì„± = ë†’ì€ ì ìˆ˜ (ì•ˆì •ì )
        if len(close) >= 20:
            returns = close.pct_change().dropna()
            recent_vol = returns.tail(5).std() * np.sqrt(252) * 100 if len(returns) >= 5 else 0
            avg_vol = returns.tail(20).std() * np.sqrt(252) * 100 if len(returns) >= 20 else recent_vol
            
            if avg_vol > 0:
                vol_ratio = recent_vol / avg_vol
                # ë³€ë™ì„±ì´ ë‚®ì„ìˆ˜ë¡ ë†’ì€ ì ìˆ˜ (ì—­ë³€í™˜)
                volatility_score = (2.0 - vol_ratio) / 2.0 * 100 if vol_ratio <= 2.0 else 0
                volatility_score = max(0, min(100, volatility_score)) * 0.2
            else:
                volatility_score = 50.0 * 0.2
        else:
            volatility_score = 50.0 * 0.2
        
        # 3. ê±°ë˜ëŸ‰ ì ìˆ˜ (20%) - ê±°ë˜ëŸ‰ ì¦ê°€ = ë†’ì€ ì ìˆ˜ (ê´€ì‹¬ ì¦ê°€)
        volume_trend = momentum['volume_trend']
        # ê±°ë˜ëŸ‰ ì¶”ì„¸ ì •ê·œí™” (-50% ~ +100% â†’ 0-100)
        volume_score = normalize_to_score(volume_trend, min_val=-50.0, max_val=100.0) * 0.2
        
        # 4. MA ìœ„ì¹˜ ì ìˆ˜ (20%) - 5MA > 20MA = ë†’ì€ ì ìˆ˜
        ma_5 = close.rolling(window=5, min_periods=1).mean()
        ma_20 = close.rolling(window=20, min_periods=1).mean()
        
        ma_score = 50.0
        if len(ma_5) > 0 and len(ma_20) > 0:
            current_ma5 = float(ma_5.iloc[-1])
            current_ma20 = float(ma_20.iloc[-1])
            if not pd.isna(current_ma5) and not pd.isna(current_ma20) and current_ma20 > 0:
                ma_ratio = (current_ma5 / current_ma20 - 1) * 100
                # MA ë¹„ìœ¨ ì •ê·œí™” (-5% ~ +5% â†’ 0-100)
                ma_score = normalize_to_score(ma_ratio, min_val=-5.0, max_val=5.0)
        
        ma_score = ma_score * 0.2
        
        # ì¢…í•© ì ìˆ˜
        total_score = momentum_score + volatility_score + volume_score + ma_score
        total_score = max(0.0, min(100.0, total_score))
        
        # ë ˆë²¨ ê²°ì •
        if total_score < 20:
            level = "EXTREME_FEAR"
        elif total_score < 40:
            level = "FEAR"
        elif total_score < 60:
            level = "NEUTRAL"
        elif total_score < 80:
            level = "GREED"
        else:
            level = "EXTREME_GREED"
        
        return total_score, level
    
    def check_leading_indicator(
        self, 
        btc_data: pd.DataFrame,
        spy_data: pd.DataFrame
    ) -> Tuple[bool, Optional[str]]:
        """
        ì„ í–‰ì§€í‘œ ì—­í•  í…ŒìŠ¤íŠ¸
        
        ì¡°ê±´:
        1. BTCê°€ 5ì¼ê°„ -10% ì´ìƒ í•˜ë½ AND
        2. SPYëŠ” ì•„ì§ -3% ë¯¸ë§Œ í•˜ë½ AND
        3. ìƒê´€ê´€ê³„ê°€ ìƒìŠ¹ ì¶”ì„¸
        â†’ RISK_OFF_WARNING
        
        ë°˜ëŒ€ ì¡°ê±´:
        1. BTCê°€ 5ì¼ê°„ +10% ì´ìƒ ìƒìŠ¹ AND
        2. SPYëŠ” ì•„ì§ +3% ë¯¸ë§Œ ìƒìŠ¹
        â†’ RISK_ON_SIGNAL
        
        Returns:
            Tuple[is_leading, signal_type]
        """
        btc_empty = hasattr(btc_data, 'empty') and btc_data.empty if btc_data is not None else True
        spy_empty = hasattr(spy_data, 'empty') and spy_data.empty if spy_data is not None else True
        if btc_empty or spy_empty:
            return False, None
        
        if 'Close' not in btc_data.columns or 'Close' not in spy_data.columns:
            return False, None
        
        btc_close = btc_data['Close']
        spy_close = spy_data['Close']
        
        # 5ì¼ ìˆ˜ìµë¥  ê³„ì‚°
        if len(btc_close) < 5 or len(spy_close) < 5:
            return False, None
        
        btc_return_5d = (float(btc_close.iloc[-1]) / float(btc_close.iloc[-5]) - 1) * 100
        spy_return_5d = (float(spy_close.iloc[-1]) / float(spy_close.iloc[-5]) - 1) * 100
        
        # RISK_OFF_WARNING ì²´í¬
        if btc_return_5d <= -10.0 and spy_return_5d > -3.0:
            # ìƒê´€ê´€ê³„ ìƒìŠ¹ ì¶”ì„¸ í™•ì¸ (ì„ íƒì )
            correlation = self.calculate_btc_spy_correlation(btc_data, spy_data)
            if correlation > 0.3:  # ìƒê´€ê´€ê³„ê°€ ì–´ëŠ ì •ë„ ìˆìœ¼ë©´
                return True, "RISK_OFF_WARNING"
        
        # RISK_ON_SIGNAL ì²´í¬
        if btc_return_5d >= 10.0 and spy_return_5d < 3.0:
            return True, "RISK_ON_SIGNAL"
        
        return False, None
    
    def calculate_btc_gld_ratio(
        self, 
        btc_data: pd.DataFrame,
        gld_data: pd.DataFrame
    ) -> Dict:
        """
        BTC/GLD ë¹„ìœ¨ ë¶„ì„
        
        í•´ì„:
        - ë¹„ìœ¨ ìƒìŠ¹: íˆ¬ê¸°ì  ì„ í˜¸ ì¦ê°€
        - ë¹„ìœ¨ í•˜ë½: ì•ˆì „ìì‚° ì„ í˜¸
        
        Returns:
            Dict with ratio and trend
        """
        btc_empty = hasattr(btc_data, 'empty') and btc_data.empty if btc_data is not None else True
        gld_empty = hasattr(gld_data, 'empty') and gld_data.empty if gld_data is not None else True
        if btc_empty or gld_empty:
            return {
                'ratio': 0.0,
                'ratio_change_5d': 0.0,
                'ratio_change_20d': 0.0,
                'trend': 'NEUTRAL'
            }
        
        if 'Close' not in btc_data.columns or 'Close' not in gld_data.columns:
            return {
                'ratio': 0.0,
                'ratio_change_5d': 0.0,
                'ratio_change_20d': 0.0,
                'trend': 'NEUTRAL'
            }
        
        btc_close = btc_data['Close']
        gld_close = gld_data['Close']
        
        # ê³µí†µ ì¸ë±ìŠ¤
        common_index = btc_close.index.intersection(gld_close.index)
        if len(common_index) == 0:
            return {
                'ratio': 0.0,
                'ratio_change_5d': 0.0,
                'ratio_change_20d': 0.0,
                'trend': 'NEUTRAL'
            }
        
        # ë¹„ìœ¨ ê³„ì‚°
        ratio_series = btc_close.loc[common_index] / gld_close.loc[common_index]
        
        if len(ratio_series) == 0:
            return {
                'ratio': 0.0,
                'ratio_change_5d': 0.0,
                'ratio_change_20d': 0.0,
                'trend': 'NEUTRAL'
            }
        
        current_ratio = float(ratio_series.iloc[-1])
        
        # 5ì¼/20ì¼ ë³€í™”ìœ¨
        if len(ratio_series) >= 5:
            ratio_change_5d = (current_ratio / float(ratio_series.iloc[-5]) - 1) * 100
        else:
            ratio_change_5d = 0.0
        
        if len(ratio_series) >= 20:
            ratio_change_20d = (current_ratio / float(ratio_series.iloc[-20]) - 1) * 100
        else:
            ratio_change_20d = 0.0
        
        # ì¶”ì„¸ íŒë‹¨
        if ratio_change_5d > 5.0:
            trend = 'RISING'  # íˆ¬ê¸°ì  ì„ í˜¸ ì¦ê°€
        elif ratio_change_5d < -5.0:
            trend = 'FALLING'  # ì•ˆì „ìì‚° ì„ í˜¸
        else:
            trend = 'NEUTRAL'
        
        return {
            'ratio': current_ratio,
            'ratio_change_5d': ratio_change_5d,
            'ratio_change_20d': ratio_change_20d,
            'trend': trend
        }
    
    def calculate_risk_contribution(self, correlation_regime: str) -> float:
        """
        ì „ì²´ ìœ„í—˜ë„ì— ê¸°ì—¬í•˜ëŠ” ë¹„ì¤‘ ê²°ì •
        
        ìƒê´€ê´€ê³„ê°€ ë†’ì„ìˆ˜ë¡ (ìœ„ê¸° ì‹œ) ê¸°ì—¬ë„ ì¦ê°€
        
        Returns:
            float: ìœ„í—˜ ê¸°ì—¬ë„ (0-0.2, ì¦‰ 0-20%)
        """
        return self.RISK_CONTRIBUTION.get(correlation_regime, 0.05)
    
    def generate_interpretation(
        self,
        sentiment_score: float,
        sentiment_level: str,
        correlation: float,
        correlation_regime: str,
        is_leading: bool,
        leading_signal: Optional[str],
        risk_contribution: float,
        causality_analysis: Dict = None
    ) -> str:
        """
        ë¶„ì„ ê²°ê³¼ í•´ì„ í…ìŠ¤íŠ¸ ìƒì„±
        
        Returns:
            str: í•´ì„ í…ìŠ¤íŠ¸
        """
        if causality_analysis is None:
            causality_analysis = {}
        
        base_text = f"ì•”í˜¸í™”í ì‹œì¥ ì‹¬ë¦¬ëŠ” {sentiment_level} ìƒíƒœì…ë‹ˆë‹¤ (ì ìˆ˜: {sentiment_score:.1f}). "
        
        # ìƒê´€ê´€ê³„ í•´ì„
        if correlation_regime == "DECOUPLED":
            base_text += f"BTC-ì£¼ì‹ ìƒê´€ê´€ê³„ê°€ ë‚®ìŠµë‹ˆë‹¤ ({correlation:.2f}). ë…ìì  ì›€ì§ì„ì´ ê´€ì°°ë©ë‹ˆë‹¤. "
        elif correlation_regime == "COUPLED":
            base_text += f"BTC-ì£¼ì‹ ìƒê´€ê´€ê³„ê°€ ë³´í†µì…ë‹ˆë‹¤ ({correlation:.2f}). ì¼ë¶€ ì—°ë™ì´ ê´€ì°°ë©ë‹ˆë‹¤. "
        else:  # CRISIS_COUPLED
            base_text += f"âš ï¸ BTC-ì£¼ì‹ ìƒê´€ê´€ê³„ê°€ ë†’ìŠµë‹ˆë‹¤ ({correlation:.2f}). ìœ„ê¸° ë™ì¡°í™”ê°€ ì§„í–‰ ì¤‘ì…ë‹ˆë‹¤. "
        
        # Granger Causality ì¸ê³¼ê´€ê³„ í•´ì„
        if causality_analysis and causality_analysis.get('relationship') != 'NO_CAUSALITY':
            relationship = causality_analysis.get('relationship', 'NO_CAUSALITY')
            x_to_y_pvalue = causality_analysis.get('x_to_y_pvalue', 1.0)
            y_to_x_pvalue = causality_analysis.get('y_to_x_pvalue', 1.0)
            optimal_lag = causality_analysis.get('optimal_lag', 0)
            
            if relationship == "X_LEADS":
                base_text += f"ğŸ“Š Granger Causality ê²€ì • ê²°ê³¼: BTCê°€ SPYë¥¼ ì„ í–‰í•©ë‹ˆë‹¤ (p={x_to_y_pvalue:.3f}, ì‹œì°¨ {optimal_lag}ì¼). "
            elif relationship == "Y_LEADS":
                base_text += f"ğŸ“Š Granger Causality ê²€ì • ê²°ê³¼: SPYê°€ BTCë¥¼ ì„ í–‰í•©ë‹ˆë‹¤ (p={y_to_x_pvalue:.3f}, ì‹œì°¨ {optimal_lag}ì¼). "
            elif relationship == "BIDIRECTIONAL":
                base_text += f"ğŸ“Š Granger Causality ê²€ì • ê²°ê³¼: BTCì™€ SPYê°€ ì–‘ë°©í–¥ ì¸ê³¼ê´€ê³„ë¥¼ ë³´ì…ë‹ˆë‹¤ (BTCâ†’SPY: p={x_to_y_pvalue:.3f}, SPYâ†’BTC: p={y_to_x_pvalue:.3f}). "
        
        # ì„ í–‰ì§€í‘œ í•´ì„
        if is_leading and leading_signal:
            if leading_signal == "RISK_OFF_WARNING":
                base_text += "ğŸš¨ BTCê°€ ì£¼ì‹ë³´ë‹¤ ë¨¼ì € í•˜ë½í•˜ê³  ìˆì–´ ìœ„í—˜ íšŒí”¼ ì‹ í˜¸ë¡œ ì‘ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. "
            elif leading_signal == "RISK_ON_SIGNAL":
                base_text += "BTCê°€ ì£¼ì‹ë³´ë‹¤ ë¨¼ì € ìƒìŠ¹í•˜ê³  ìˆì–´ ìœ„í—˜ ì„ í˜¸ ì‹ í˜¸ë¡œ ì‘ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. "
        
        # ìœ„í—˜ ê¸°ì—¬ë„ í•´ì„
        if risk_contribution >= 0.15:
            base_text += f"ì „ì²´ ìœ„í—˜ë„ì— {risk_contribution*100:.0f}% ê¸°ì—¬í•˜ê³  ìˆì–´ ì£¼ì˜ê°€ í•„ìš”í•©ë‹ˆë‹¤."
        elif risk_contribution >= 0.10:
            base_text += f"ì „ì²´ ìœ„í—˜ë„ì— {risk_contribution*100:.0f}% ê¸°ì—¬í•˜ê³  ìˆìŠµë‹ˆë‹¤."
        else:
            base_text += f"ë…ìì  ì‹ í˜¸ë¡œ í•´ì„ë©ë‹ˆë‹¤ (ìœ„í—˜ ê¸°ì—¬ë„ {risk_contribution*100:.0f}%)."
        
        return base_text
    
    def analyze(
        self,
        btc_data: pd.DataFrame,
        spy_data: pd.DataFrame,
        gld_data: pd.DataFrame
    ) -> CryptoSentimentResult:
        """
        ì „ì²´ ë¶„ì„ ì‹¤í–‰
        
        Args:
            btc_data: BTC-USD ê°€ê²© ë°ì´í„°
            spy_data: SPY ê°€ê²© ë°ì´í„° (ìƒê´€ê´€ê³„ ê³„ì‚°ìš©)
            gld_data: GLD ê°€ê²© ë°ì´í„° (BTC/GLD ë¹„ìœ¨ìš©)
        
        Returns:
            CryptoSentimentResult ê°ì²´
        """
        # ì‹¬ë¦¬ ì ìˆ˜ ê³„ì‚°
        sentiment_score, sentiment_level = self.calculate_sentiment_score(btc_data)
        
        # BTC-SPY ìƒê´€ê´€ê³„ ê³„ì‚°
        correlation = self.calculate_btc_spy_correlation(btc_data, spy_data)
        
        # ìƒê´€ê´€ê³„ ë ˆì§ íŒë‹¨
        correlation_regime = self.determine_correlation_regime(correlation)
        
        # Granger Causality ê²€ì • (ì¸ê³¼ê´€ê³„ ë°©í–¥ì„± íŒŒì•…)
        causality_analysis = {}
        btc_empty = hasattr(btc_data, 'empty') and btc_data.empty if btc_data is not None else True
        spy_empty = hasattr(spy_data, 'empty') and spy_data.empty if spy_data is not None else True
        if not btc_empty and not spy_empty:
            if 'Close' in btc_data.columns and 'Close' in spy_data.columns:
                # ìˆ˜ìµë¥  ê³„ì‚°
                btc_returns = btc_data['Close'].pct_change().dropna()
                spy_returns = spy_data['Close'].pct_change().dropna()
                
                # ê³µí†µ ì¸ë±ìŠ¤ë¡œ ì •ë ¬
                common_index = btc_returns.index.intersection(spy_returns.index)
                if len(common_index) >= 30:  # ìµœì†Œ 30ì¼ ë°ì´í„° í•„ìš”
                    btc_aligned = btc_returns.loc[common_index]
                    spy_aligned = spy_returns.loc[common_index]
                    
                    # Granger Causality ê²€ì • ìˆ˜í–‰
                    causality_analysis = self.calculate_granger_causality(
                        series_x=btc_aligned,  # BTCê°€ ì›ì¸ í›„ë³´
                        series_y=spy_aligned,  # SPYê°€ ê²°ê³¼ í›„ë³´
                        max_lag=5
                    )
        
        # ì„ í–‰ì§€í‘œ ì²´í¬
        is_leading, leading_signal = self.check_leading_indicator(btc_data, spy_data)
        
        # ìœ„í—˜ ê¸°ì—¬ë„ ê³„ì‚°
        risk_contribution = self.calculate_risk_contribution(correlation_regime)
        
        # BTC/GLD ë¹„ìœ¨ ë¶„ì„
        btc_gld_ratio = self.calculate_btc_gld_ratio(btc_data, gld_data)
        
        # ëª¨ë©˜í…€ ê³„ì‚°
        momentum = self.calculate_btc_momentum(btc_data)
        
        # êµ¬ì„±ìš”ì†Œ í†µí•©
        components = {
            **momentum,
            'btc_gld_ratio': btc_gld_ratio,
            'correlation': correlation,
            'causality_analysis': causality_analysis,
        }
        
        # í•´ì„ í…ìŠ¤íŠ¸ ìƒì„±
        interpretation = self.generate_interpretation(
            sentiment_score,
            sentiment_level,
            correlation,
            correlation_regime,
            is_leading,
            leading_signal,
            risk_contribution,
            causality_analysis
        )
        
        return CryptoSentimentResult(
            timestamp=datetime.now().isoformat(),
            sentiment_score=sentiment_score,
            sentiment_level=sentiment_level,
            btc_spy_correlation=correlation,
            correlation_regime=correlation_regime,
            is_leading_indicator=is_leading,
            leading_signal=leading_signal,
            risk_contribution=risk_contribution,
            components=components,
            interpretation=interpretation,
            causality_analysis=causality_analysis
        )

# critical_path_analyzer.py íŒŒì¼ ëì— ì¶”ê°€

@dataclass
class CriticalPathResult:
    """
    Critical Path ë¶„ì„ ì¢…í•© ê²°ê³¼
    
    ê²½ì œí•™ì  ì˜ë¯¸:
    - total_risk_score: ì „ì²´ ì‹œì¥ ìœ„í—˜ë„ (0-100)
    - path_contributions: ê²½ë¡œë³„ ìœ„í—˜ ê¸°ì—¬ë„ (í•©ê³„ = total_risk_score)
    - primary_risk_path: ê°€ì¥ í° ê¸°ì—¬ë„ë¥¼ ê°€ì§„ ê²½ë¡œ (ìœ„í—˜ ì§„ì›ì§€)
    """
    timestamp: str
    
    # ì „ì²´ ìœ„í—˜ë„
    total_risk_score: float           # 0-100
    risk_level: str                   # "LOW", "MEDIUM", "HIGH", "CRITICAL"
    
    # ë ˆì§ ì •ë³´
    current_regime: str
    regime_confidence: float
    transition_probability: float
    
    # ê²½ë¡œë³„ ê¸°ì—¬ë„ (raw scores, ì ˆëŒ€ê°’)
    path_contributions: Dict[str, float]  
    # ì˜ˆ: {"liquidity": 25, "concentration": 22, "credit": 10, ...}
    
    # ê²½ë¡œë³„ ë¶„í¬ (100% ì •ê·œí™”, ì‹œê°í™”ìš©)
    path_distribution: Dict[str, float]
    # ì˜ˆ: {"liquidity": 35.2%, "concentration": 30.1%, ...}
    
    # í•˜ìœ„ ëª¨ë“ˆ ê²°ê³¼
    risk_appetite_result: RiskAppetiteUncertaintyResult
    regime_result: RegimeResult
    spillover_result: SpilloverResult
    crypto_result: CryptoSentimentResult
    
    # í•´ì„ ë° ê²½ê³ 
    primary_risk_path: str            # ê°€ì¥ í° ê¸°ì—¬ë„ ê²½ë¡œ
    active_warnings: List[str]        # í™œì„±í™”ëœ ê²½ê³  ëª©ë¡
    interpretation: str               # ì¢…í•© í•´ì„
    
    def to_dict(self) -> Dict:
        """ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜ (JSON ì§ë ¬í™”ìš©)"""
        d = asdict(self)
        d['risk_appetite_result'] = self.risk_appetite_result.to_dict()
        d['regime_result'] = self.regime_result.to_dict()
        d['spillover_result'] = self.spillover_result.to_dict()
        d['crypto_result'] = self.crypto_result.to_dict()
        return d


# ============================================================================
# Stress Regime Multiplier (Elicit Report Enhancement)
# ============================================================================

@dataclass
class StressMultiplierResult:
    """ìŠ¤íŠ¸ë ˆìŠ¤ ë ˆì§ ìŠ¹ìˆ˜ ê²°ê³¼"""
    timestamp: str
    base_multiplier: float           # ê¸°ë³¸ ë ˆì§ ìŠ¹ìˆ˜
    correlation_adjustment: float    # ìƒê´€ê´€ê³„ ì¡°ì • (Longin-Solnik)
    volatility_scaling: float        # ë³€ë™ì„± ìŠ¤ì¼€ì¼ë§
    contagion_factor: float          # ì „ì—¼ ê°€ì† ê³„ìˆ˜
    final_multiplier: float          # ìµœì¢… ìŠ¹ìˆ˜
    regime: str                      # í˜„ì¬ ë ˆì§
    methodology_notes: str           # ë°©ë²•ë¡  ì„¤ëª…
    academic_references: List[str]   # í•™ìˆ  ì°¸ê³ ë¬¸í—Œ


class StressRegimeMultiplier:
    """
    ìŠ¤íŠ¸ë ˆìŠ¤ ë ˆì§ ìŠ¹ìˆ˜ ê³„ì‚°ê¸° (Elicit Report Enhancement)

    í•™ìˆ ì  ê·¼ê±°:
    - Longin & Solnik (2001): ê·¹ë‹¨ì  ì‹œì¥ì—ì„œ ìƒê´€ê´€ê³„ ë¹„ëŒ€ì¹­ ë°œê²¬
    - Forbes & Rigobon (2002): ìœ„ê¸° ì‹œ "contagion" vs "interdependence" êµ¬ë¶„
    - Elicit Report: ìœ„ê¸° ì‹œ ìƒê´€ê´€ê³„ 61.4% ì¦ê°€ í™•ì¸

    Perplexity ê²€ì¦ ê²°ê³¼:
    - í•™ìˆ ì  í•©ì˜: ìŠ¤íŠ¸ë ˆìŠ¤ ê¸°ê°„ì— ìƒê´€ê´€ê³„ ì¦ê°€ (confirmatory bias ì£¼ì˜)
    - Forbes-Rigobon ì¡°ì •: ë³€ë™ì„± ì¦ê°€ë¡œ ì¸í•œ spurious correlation ë³´ì • í•„ìš”
    - ì‹¤ë¬´ì  í•¨ì˜: ë¶„ì‚° íš¨ê³¼ ê°ì†Œ â†’ ë¦¬ìŠ¤í¬ ê³¼ì†Œí‰ê°€ ë°©ì§€
    """

    # ë ˆì§ë³„ ê¸°ë³¸ ìŠ¹ìˆ˜ (ê¸°ì¡´ ë¡œì§ ê¸°ë°˜)
    BASE_MULTIPLIERS = {
        'BULL': 0.8,
        'NEUTRAL': 1.0,
        'TRANSITION': 1.0,
        'BEAR': 1.2,
        'CRISIS': 1.5
    }

    # ìƒê´€ê´€ê³„ ì¦ê°€ ê³„ìˆ˜ (Elicit: 61.4% ì¦ê°€)
    CRISIS_CORRELATION_INCREASE = 0.614

    # VIX ì„ê³„ê°’ (ìŠ¤íŠ¸ë ˆìŠ¤ ë ˆë²¨ ê²°ì •)
    VIX_THRESHOLDS = {
        'normal': 20,
        'elevated': 25,
        'stress': 30,
        'crisis': 40
    }

    def __init__(
        self,
        correlation_window: int = 60,
        volatility_window: int = 20
    ):
        self.correlation_window = correlation_window
        self.volatility_window = volatility_window

    def calculate_multiplier(
        self,
        market_data: Dict[str, pd.DataFrame],
        current_regime: str,
        vix_level: Optional[float] = None
    ) -> StressMultiplierResult:
        """
        ìŠ¤íŠ¸ë ˆìŠ¤ ë ˆì§ ìŠ¹ìˆ˜ ê³„ì‚°

        Parameters:
        -----------
        market_data : Dict[str, DataFrame]
            ì‹œì¥ ë°ì´í„° (SPY, QQQ ë“±)
        current_regime : str
            í˜„ì¬ ì‹œì¥ ë ˆì§ (BULL/BEAR/NEUTRAL/CRISIS)
        vix_level : float (optional)
            í˜„ì¬ VIX ë ˆë²¨

        Returns:
        --------
        StressMultiplierResult
        """
        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')

        # 1. ê¸°ë³¸ ë ˆì§ ìŠ¹ìˆ˜
        base_multiplier = self.BASE_MULTIPLIERS.get(current_regime.upper(), 1.0)

        # 2. VIX ê¸°ë°˜ ë³€ë™ì„± ìŠ¤ì¼€ì¼ë§
        if vix_level is None:
            vix_data = market_data.get('^VIX') or market_data.get('VIX')
            if vix_data is not None and not vix_data.empty and 'Close' in vix_data.columns:
                vix_level = float(vix_data['Close'].iloc[-1])
            else:
                vix_level = 20.0  # ê¸°ë³¸ê°’

        volatility_scaling = self._calculate_volatility_scaling(vix_level)

        # 3. ìƒê´€ê´€ê³„ ì¡°ì • (Longin-Solnik / Forbes-Rigobon)
        correlation_adjustment = self._calculate_correlation_adjustment(
            market_data, current_regime, vix_level
        )

        # 4. ì „ì—¼ ê°€ì† ê³„ìˆ˜
        contagion_factor = self._calculate_contagion_factor(
            market_data, current_regime
        )

        # 5. ìµœì¢… ìŠ¹ìˆ˜ ê³„ì‚°
        # ê³µì‹: Final = Base Ã— (1 + VolScaling) Ã— (1 + CorrAdj) Ã— (1 + Contagion)
        final_multiplier = (
            base_multiplier
            * (1 + volatility_scaling)
            * (1 + correlation_adjustment)
            * (1 + contagion_factor)
        )

        # ìƒí•œì„  (ê³¼ë„í•œ ìŠ¹ìˆ˜ ë°©ì§€)
        final_multiplier = min(final_multiplier, 3.0)

        # ë°©ë²•ë¡  ì„¤ëª…
        methodology_notes = self._generate_methodology_notes(
            base_multiplier, volatility_scaling,
            correlation_adjustment, contagion_factor, vix_level
        )

        return StressMultiplierResult(
            timestamp=timestamp,
            base_multiplier=base_multiplier,
            correlation_adjustment=correlation_adjustment,
            volatility_scaling=volatility_scaling,
            contagion_factor=contagion_factor,
            final_multiplier=final_multiplier,
            regime=current_regime,
            methodology_notes=methodology_notes,
            academic_references=[
                "Longin & Solnik (2001): Extreme Correlation of International Equity Markets",
                "Forbes & Rigobon (2002): No Contagion, Only Interdependence",
                "Elicit Report (2026): 61.4% correlation increase during stress"
            ]
        )

    def _calculate_volatility_scaling(self, vix_level: float) -> float:
        """VIX ê¸°ë°˜ ë³€ë™ì„± ìŠ¤ì¼€ì¼ë§ ê³„ì‚°"""
        # ì •ìƒ VIX (20) ëŒ€ë¹„ ì´ˆê³¼ë¶„ì— ë¹„ë¡€í•˜ì—¬ ìŠ¤ì¼€ì¼ë§
        if vix_level <= self.VIX_THRESHOLDS['normal']:
            return 0.0
        elif vix_level <= self.VIX_THRESHOLDS['elevated']:
            return (vix_level - 20) / 100  # 0~5% ì¶”ê°€
        elif vix_level <= self.VIX_THRESHOLDS['stress']:
            return (vix_level - 20) / 50   # 0~20% ì¶”ê°€
        elif vix_level <= self.VIX_THRESHOLDS['crisis']:
            return (vix_level - 20) / 40   # 0~50% ì¶”ê°€
        else:
            return 0.5 + (vix_level - 40) / 100  # 50%+ ì¶”ê°€

    def _calculate_correlation_adjustment(
        self,
        market_data: Dict[str, pd.DataFrame],
        regime: str,
        vix_level: float
    ) -> float:
        """
        ìƒê´€ê´€ê³„ ì¡°ì • ê³„ì‚° (Longin-Solnik / Forbes-Rigobon)

        í•µì‹¬ ì•„ì´ë””ì–´:
        - ìœ„ê¸° ì‹œ ìì‚° ê°„ ìƒê´€ê´€ê³„ê°€ ë¹„ì„ í˜•ì ìœ¼ë¡œ ì¦ê°€
        - Elicit Report: í‰ê·  61.4% ìƒê´€ê´€ê³„ ì¦ê°€ ê´€ì¸¡
        - Forbes-Rigobon: ë³€ë™ì„± ì¦ê°€ë¡œ ì¸í•œ spurious correlation ë³´ì • í•„ìš”
        """
        # ìŠ¤íŠ¸ë ˆìŠ¤ ë ˆë²¨ ê²°ì •
        if regime.upper() in ['CRISIS'] or vix_level > 35:
            stress_level = 'CRISIS'
        elif regime.upper() in ['BEAR'] or vix_level > 25:
            stress_level = 'STRESS'
        else:
            stress_level = 'NORMAL'

        # ìƒê´€ê´€ê³„ ì¡°ì •ê°’
        if stress_level == 'CRISIS':
            # Elicit 61.4% Ã— 0.7 (Forbes-Rigobon ë³´ì •)
            raw_adjustment = self.CRISIS_CORRELATION_INCREASE * 0.7
        elif stress_level == 'STRESS':
            raw_adjustment = self.CRISIS_CORRELATION_INCREASE * 0.4
        else:
            raw_adjustment = 0.0

        # ì‹¤ì œ ìƒê´€ê´€ê³„ ë³€í™” ì¸¡ì • (ë°ì´í„° ìˆìœ¼ë©´)
        try:
            empirical_adj = self._measure_empirical_correlation_change(market_data)
            if empirical_adj is not None:
                # ì´ë¡ ê°’ê³¼ ì‹¤ì¦ê°’ì˜ ê°€ì¤‘ í‰ê· 
                return 0.6 * raw_adjustment + 0.4 * empirical_adj
        except Exception:
            pass

        return raw_adjustment

    def _measure_empirical_correlation_change(
        self,
        market_data: Dict[str, pd.DataFrame]
    ) -> Optional[float]:
        """ì‹¤ì œ ìƒê´€ê´€ê³„ ë³€í™” ì¸¡ì •"""
        # SPY-QQQ, SPY-TLT ë“± ì£¼ìš” ìì‚° ìŒì˜ ë¡¤ë§ ìƒê´€ê´€ê³„ ë³€í™”
        spy_data = market_data.get('SPY')
        qqq_data = market_data.get('QQQ')
        tlt_data = market_data.get('TLT')

        if spy_data is None or qqq_data is None:
            return None

        try:
            # ìˆ˜ìµë¥  ê³„ì‚°
            spy_ret = spy_data['Close'].pct_change().dropna()
            qqq_ret = qqq_data['Close'].pct_change().dropna()

            # ìµœê·¼ ìƒê´€ê´€ê³„ vs ì¥ê¸° ìƒê´€ê´€ê³„
            if len(spy_ret) < self.correlation_window:
                return None

            short_window = min(20, len(spy_ret) // 2)
            long_corr = spy_ret.tail(self.correlation_window).corr(
                qqq_ret.tail(self.correlation_window)
            )
            short_corr = spy_ret.tail(short_window).corr(
                qqq_ret.tail(short_window)
            )

            # ìƒê´€ê´€ê³„ ë³€í™”ìœ¨
            if long_corr != 0:
                return (short_corr - long_corr) / abs(long_corr)
            return 0.0
        except Exception:
            return None

    def _calculate_contagion_factor(
        self,
        market_data: Dict[str, pd.DataFrame],
        regime: str
    ) -> float:
        """
        ì „ì—¼ ê°€ì† ê³„ìˆ˜ ê³„ì‚°

        ìœ„ê¸° ì‹œ ìì‚° ê°„ ì¶©ê²© ì „íŒŒ ì†ë„ê°€ ê°€ì†í™”ë¨ì„ ë°˜ì˜
        """
        if regime.upper() not in ['BEAR', 'CRISIS']:
            return 0.0

        # ì„¹í„° ETF ë™ì¡°í™” ì¸¡ì •
        sector_etfs = ['XLF', 'XLK', 'XLE', 'XLV', 'XLI', 'XLY', 'XLP']
        available_sectors = [s for s in sector_etfs if s in market_data]

        if len(available_sectors) < 3:
            # ë°ì´í„° ë¶€ì¡± ì‹œ ë ˆì§ ê¸°ë°˜ ê¸°ë³¸ê°’
            return 0.1 if regime.upper() == 'BEAR' else 0.2

        try:
            returns = {}
            for sector in available_sectors:
                if 'Close' in market_data[sector].columns:
                    returns[sector] = market_data[sector]['Close'].pct_change().dropna()

            if len(returns) < 3:
                return 0.1

            returns_df = pd.DataFrame(returns).dropna()
            if len(returns_df) < 20:
                return 0.1

            # ìƒê´€ê´€ê³„ í–‰ë ¬
            corr_matrix = returns_df.tail(20).corr()

            # í‰ê·  ìƒê´€ê´€ê³„ (ëŒ€ê°ì„  ì œì™¸)
            n = len(corr_matrix)
            if n < 2:
                return 0.1

            off_diag = corr_matrix.values[~np.eye(n, dtype=bool)]
            avg_corr = np.mean(off_diag)

            # ë†’ì€ ë™ì¡°í™” = ë†’ì€ ì „ì—¼ ê°€ì†
            # í‰ê·  ìƒê´€ê´€ê³„ > 0.7ì´ë©´ ì „ì—¼ ê°€ì†
            if avg_corr > 0.8:
                return 0.3
            elif avg_corr > 0.7:
                return 0.2
            elif avg_corr > 0.5:
                return 0.1
            return 0.0
        except Exception:
            return 0.1

    def _generate_methodology_notes(
        self,
        base: float, vol: float, corr: float, contagion: float, vix: float
    ) -> str:
        """ë°©ë²•ë¡  ì„¤ëª… ìƒì„±"""
        notes = []
        notes.append(f"ê¸°ë³¸ ë ˆì§ ìŠ¹ìˆ˜: {base:.2f}")

        if vol > 0:
            notes.append(f"ë³€ë™ì„± ìŠ¤ì¼€ì¼ë§: +{vol*100:.1f}% (VIX={vix:.1f})")

        if corr > 0:
            notes.append(
                f"ìƒê´€ê´€ê³„ ì¡°ì •: +{corr*100:.1f}% "
                f"(Longin-Solnik/Forbes-Rigobon ê¸°ë°˜)"
            )

        if contagion > 0:
            notes.append(f"ì „ì—¼ ê°€ì†: +{contagion*100:.1f}% (ì„¹í„° ë™ì¡°í™”)")

        return " | ".join(notes)

    def apply_to_risk_score(
        self,
        base_risk_score: float,
        multiplier_result: StressMultiplierResult
    ) -> Tuple[float, str]:
        """
        ë¦¬ìŠ¤í¬ ì ìˆ˜ì— ìŠ¤íŠ¸ë ˆìŠ¤ ìŠ¹ìˆ˜ ì ìš©

        Parameters:
        -----------
        base_risk_score : float
            ê¸°ë³¸ ë¦¬ìŠ¤í¬ ì ìˆ˜ (0-100)
        multiplier_result : StressMultiplierResult
            ìŠ¤íŠ¸ë ˆìŠ¤ ìŠ¹ìˆ˜ ê²°ê³¼

        Returns:
        --------
        Tuple[adjusted_score, explanation]
        """
        adjusted_score = base_risk_score * multiplier_result.final_multiplier
        adjusted_score = min(100.0, adjusted_score)  # ìƒí•œ 100

        explanation = (
            f"Base: {base_risk_score:.1f} Ã— Multiplier: {multiplier_result.final_multiplier:.2f} "
            f"= Adjusted: {adjusted_score:.1f}"
        )

        return adjusted_score, explanation


class CriticalPathAggregator:
    """
    Critical Path ë¶„ì„ í†µí•© ëª¨ë“ˆ
    
    4ê°œ í•˜ìœ„ ëª¨ë“ˆì„ ì¡°ìœ¨í•˜ê³  ìµœì¢… ê²°ê³¼ ì‚°ì¶œ
    
    í•˜ìœ„ ëª¨ë“ˆ:
    1. RiskAppetiteUncertaintyIndex: ë¦¬ìŠ¤í¬ ì„ í˜¸ë„ì™€ ë¶ˆí™•ì‹¤ì„± ë¶„ë¦¬ ì¸¡ì •
    2. EnhancedRegimeDetector: ë ˆì§ íƒì§€ ë° ë ˆì§ë³„ ì„ê³„ê°’ ì œê³µ
    3. SpilloverNetwork: ìì‚°ê°„ ì¶©ê²© ì „ì´ ë„¤íŠ¸ì›Œí¬
    4. CryptoSentimentBlock: ì•”í˜¸í™”í ì‹¬ë¦¬ ì§€í‘œ ë¸”ë¡
    """
    
    # ê²½ë¡œë³„ ê¸°ë³¸ ê°€ì¤‘ì¹˜
    BASE_PATH_WEIGHTS = {
        'liquidity': 0.25,       # ìœ ë™ì„±/ê¸ˆë¦¬ ê²½ë¡œ
        'concentration': 0.25,   # AI/ë¹…í…Œí¬ ì§‘ì¤‘ ê²½ë¡œ
        'credit': 0.20,          # ì‹ ìš© ìŠ¤íŠ¸ë ˆìŠ¤ ê²½ë¡œ
        'volatility': 0.15,      # ë³€ë™ì„±/ê³µí¬ ê²½ë¡œ
        'rotation': 0.10,        # ì„¹í„° ë¡œí…Œì´ì…˜ ê²½ë¡œ
        'crypto': 0.05,          # ì•”í˜¸í™”í (ê¸°ë³¸ê°’, ë™ì  ì¡°ì •)
    }
    
    # Perplexity ì œì•ˆ ê¸°ë°˜ ê²€ì¦ëœ ì„ê³„ê°’ ìƒìˆ˜
    THRESHOLDS = {
        'zscore': {'warning': 1.5, 'alert': 2.0, 'critical': 2.5},
        'ml_prob': {'warning': 0.20, 'alert': 0.40, 'critical': 0.60},
        'rsi': {'overbought': 70, 'oversold': 30, 'extreme': {'ob': 80, 'os': 20}},
        'drawdown': {'days': 10, 'threshold': -0.05},
        'vix': {'normal': (15, 25), 'stress': 30, 'complacency': 12},
        'bb': {'window': 20, 'std': 2.0, 'compression_ratio': 0.5},
    }
    
    # Risk Appetite ê°€ì¤‘í•© (Bekaert ê¸°ë°˜)
    RISK_APPETITE_WEIGHTS = {'HYG_LQD': 0.4, 'XLY_XLP': 0.3, 'IWM_SPY': 0.3}
    
    def __init__(self, config: Optional[Dict] = None):
        """
        Args:
            config: ì„¤ì • ë”•ì…”ë„ˆë¦¬ (ì„ íƒì )
        """
        # í•˜ìœ„ ëª¨ë“ˆ ì´ˆê¸°í™”
        self.ra_uncertainty = RiskAppetiteUncertaintyIndex(lookback=20)
        self.regime_detector = EnhancedRegimeDetector(short_ma=20, long_ma=120)
        self.spillover_network = SpilloverNetwork(lookback=20)
        self.crypto_sentiment = CryptoSentimentBlock(lookback=20, correlation_window=20)
        
        # ì„¤ì • ë¡œë“œ
        self.config = config or {}
        self.path_weights = self.BASE_PATH_WEIGHTS.copy()
    
    def collect_required_data(self, market_data: Dict[str, pd.DataFrame]) -> Dict:
        """
        í•„ìš”í•œ ë°ì´í„°ê°€ ëª¨ë‘ ìˆëŠ”ì§€ í™•ì¸í•˜ê³  ì •ë¦¬
        
        í•„ìˆ˜ í‹°ì»¤:
        - SPY, QQQ, TLT, GLD, VIX (ê¸°ë³¸)
        - HYG, LQD, XLY, XLP, IWM, XLF (RA/Spillover)
        - BTC-USD (Crypto)
        - SMH, NVDA, DXY (ì¶”ê°€ ê²½ë¡œ)
        
        Returns:
            ì •ë¦¬ëœ ë°ì´í„° ë”•ì…”ë„ˆë¦¬ ë° ëˆ„ë½ëœ í‹°ì»¤ ëª©ë¡
        """
        required_tickers = [
            'SPY', 'QQQ', 'TLT', 'GLD', '^VIX', 'VIX',
            'HYG', 'LQD', 'XLY', 'XLP', 'IWM', 'XLF',
            'BTC-USD', 'SMH', 'NVDA', 'DXY', 'DX-Y.NYB', 'EEM'
        ]
        
        collected = {}
        missing = []
        
        for ticker in required_tickers:
            # í‹°ì»¤ëª… ë³€í˜• ì‹œë„
            data = market_data.get(ticker)
            if data is None:
                # ëŒ€ì²´ í‹°ì»¤ ì‹œë„
                alt_tickers = {
                    '^VIX': 'VIX',
                    'DX-Y.NYB': 'DXY',
                }
                alt_ticker = alt_tickers.get(ticker)
                if alt_ticker:
                    data = market_data.get(alt_ticker)
            
            if data is not None and not data.empty:
                collected[ticker] = data
            else:
                missing.append(ticker)
        
        return {
            'data': collected,
            'missing': missing
        }
    
    def run_submodules(self, market_data: Dict[str, pd.DataFrame]) -> Dict:
        """
        4ê°œ í•˜ìœ„ ëª¨ë“ˆ ìˆœì°¨ ì‹¤í–‰
        
        ì‹¤í–‰ ìˆœì„œ:
        1. Regime Detector (ì„ê³„ê°’ ê²°ì •ì— í•„ìš”)
        2. Risk Appetite & Uncertainty
        3. Spillover Network (ë ˆì§ ì •ë³´ í™œìš©)
        4. Crypto Sentiment
        
        Returns:
            Dict with all submodule results
        """
        results = {}
        
        # 1. Regime Detector (ë¨¼ì € ì‹¤í–‰ - ë‹¤ë¥¸ ëª¨ë“ˆì—ì„œ ë ˆì§ ì •ë³´ í•„ìš”)
        spy_data = market_data.get('SPY')
        vix_data = market_data.get('^VIX')
        if vix_data is None or (hasattr(vix_data, 'empty') and vix_data.empty):
            vix_data = market_data.get('VIX')
        
        if spy_data is not None and vix_data is not None:
            if hasattr(spy_data, 'empty') and spy_data.empty:
                spy_data = None
            if hasattr(vix_data, 'empty') and vix_data.empty:
                vix_data = None
        
        if spy_data is not None and vix_data is not None:
            regime_result = self.regime_detector.analyze(spy_data, vix_data)
            results['regime'] = regime_result
            current_regime = regime_result.current_regime
        else:
            current_regime = "TRANSITION"
            # ê¸°ë³¸ RegimeResult ìƒì„± (ì—ëŸ¬ ë°©ì§€)
            results['regime'] = None
        
        # 2. Risk Appetite & Uncertainty
        try:
            ra_result = self.ra_uncertainty.analyze(market_data)
            results['risk_appetite'] = ra_result
        except Exception as e:
            print(f"Warning: Risk Appetite analysis failed: {e}")
            results['risk_appetite'] = None
        
        # 3. Spillover Network (ë ˆì§ ì •ë³´ í™œìš©)
        try:
            spillover_result = self.spillover_network.analyze(market_data, current_regime)
            results['spillover'] = spillover_result
        except Exception as e:
            print(f"Warning: Spillover analysis failed: {e}")
            results['spillover'] = None
        
        # 4. Crypto Sentiment
        btc_data = market_data.get('BTC-USD')
        spy_data = market_data.get('SPY')
        gld_data = market_data.get('GLD')
        
        if btc_data is not None and spy_data is not None and gld_data is not None:
            try:
                crypto_result = self.crypto_sentiment.analyze(btc_data, spy_data, gld_data)
                results['crypto'] = crypto_result
            except Exception as e:
                print(f"Warning: Crypto sentiment analysis failed: {e}")
                results['crypto'] = None
        else:
            results['crypto'] = None
        
        return results
    
    def calculate_path_contributions(
        self, 
        submodule_results: Dict,
        regime: str
    ) -> Tuple[Dict[str, float], Dict[str, float]]:
        """
        ê²½ë¡œë³„ ìœ„í—˜ ê¸°ì—¬ë„ ê³„ì‚°
        
        ë¡œì§:
        1. ê° ê²½ë¡œë³„ ì›ì‹œ ì ìˆ˜(raw score) ê³„ì‚°
        2. ê° ê²½ë¡œë³„ ì ìˆ˜ë¥¼ 0-100 ë²”ìœ„ë¡œ í´ë¦¬í•‘
        3. ì‹œê°í™”ìš© ë¶„í¬(path_distribution) ë³„ë„ ê³„ì‚° (100% ì •ê·œí™”)
        
        ê²½ë¡œë³„ ì ìˆ˜ ì‚°ì¶œ:
        - liquidity: TLT ì‹ í˜¸ + DXY ì‹ í˜¸ + ìˆ˜ìµë¥ ê³¡ì„ 
        - concentration: QQQ/SPY + RSP/SPY + NVDA/SMH
        - credit: HYG/LQD + XLF ì‹ í˜¸
        - volatility: VIX ë ˆë²¨ + ë¶ˆí™•ì‹¤ì„± ì§€ìˆ˜
        - rotation: XLY/XLP + IWM/SPY
        - crypto: CryptoSentimentResultì˜ risk_contribution
        
        Returns:
            Tuple[path_contributions (raw scores), path_distribution (100% ì •ê·œí™”)]
        """
        contributions = {}
        
        # 1. Liquidity ê²½ë¡œ (ìœ ë™ì„±/ê¸ˆë¦¬)
        liquidity_score = 0.0
        if submodule_results.get('spillover'):
            spillover = submodule_results['spillover']
            # TLT ê´€ë ¨ ê²½ë¡œ ì°¾ê¸°
            for edge in spillover.active_paths:
                if edge.source in ['TLT', 'DXY'] and edge.category == 'liquidity':
                    liquidity_score += edge.signal_strength * 0.5
        
        # Risk Appetiteì—ì„œ ë¶ˆí™•ì‹¤ì„± ì ìˆ˜ í™œìš©
        if submodule_results.get('risk_appetite'):
            ra = submodule_results['risk_appetite']
            # ë¶ˆí™•ì‹¤ì„±ì´ ë†’ìœ¼ë©´ ìœ ë™ì„± ê²½ë¡œ ìœ„í—˜ ì¦ê°€
            liquidity_score += ra.uncertainty_score * 0.3
        
        # 0-100 ë²”ìœ„ë¡œ í´ë¦¬í•‘
        contributions['liquidity'] = max(0.0, min(100.0, liquidity_score))
        
        # 2. Concentration ê²½ë¡œ (AI/ë¹…í…Œí¬ ì§‘ì¤‘) - ê°œì„ 
        concentration_score = 0.0
        
        # Spillover ê²½ë¡œì—ì„œ ì§‘ì¤‘ë„ ì‹ í˜¸ í™•ì¸
        if submodule_results.get('spillover'):
            spillover = submodule_results['spillover']
            for edge in spillover.active_paths:
                if edge.category == 'concentration':
                    concentration_score += edge.signal_strength * 0.6
        
        # ì§ì ‘ ê³„ì‚° ì¶”ê°€: Risk Appetiteì˜ components í™œìš©
        if submodule_results.get('risk_appetite'):
            ra = submodule_results['risk_appetite']
            components = ra.components
            
            # HYG/LQD zscoreê°€ ë†’ìœ¼ë©´ ì‹ ìš© ì„ í˜¸ (ì§‘ì¤‘ ìœ„í—˜ ì¦ê°€)
            hyg_lqd_z = abs(components.get('hyg_lqd_zscore', 0))
            if hyg_lqd_z > 1.0:  # ì„ê³„ê°’ ì™„í™”: 1.5 -> 1.0
                concentration_score += hyg_lqd_z * 6  # ê°€ì¤‘ì¹˜ ì¡°ì •
            
            # XLY/XLP zscoreë¡œ ê²½ê¸°ë¯¼ê°ì£¼ ì„ í˜¸ë„ í™•ì¸
            xly_xlp_z = components.get('xly_xlp_zscore', 0)
            if xly_xlp_z > 1.0:  # ì–‘ìˆ˜ì¼ ë•Œë§Œ ì„±ì¥ì£¼ ì§‘ì¤‘ (ì„ê³„ê°’ ì™„í™”)
                concentration_score += xly_xlp_z * 5
            
            # ìƒê´€ê´€ê³„ ë¶„ì‚°ì´ ë‚®ìœ¼ë©´ ë™ì¡°í™” (ì§‘ì¤‘ ìœ„í—˜ ì¦ê°€)
            corr_var = components.get('corr_variance_score', 50)
            if corr_var < 30:  # ë‚®ì€ ë¶„ì‚° = ë†’ì€ ìƒê´€ = ë™ì¡°í™”
                concentration_score += (30 - corr_var) * 0.5
        
        # 0-100 ë²”ìœ„ë¡œ í´ë¦¬í•‘
        contributions['concentration'] = max(0.0, min(100.0, concentration_score))
        
        # 3. Credit ê²½ë¡œ (ì‹ ìš© ìŠ¤íŠ¸ë ˆìŠ¤)
        credit_score = 0.0
        if submodule_results.get('spillover'):
            spillover = submodule_results['spillover']
            for edge in spillover.active_paths:
                if edge.category == 'credit':
                    credit_score += edge.signal_strength * 0.5
        
        # Risk Appetiteì—ì„œ ë¦¬ìŠ¤í¬ ì„ í˜¸ë„ í™œìš©
        if submodule_results.get('risk_appetite'):
            ra = submodule_results['risk_appetite']
            # ë¦¬ìŠ¤í¬ ì„ í˜¸ë„ê°€ ë‚®ìœ¼ë©´ ì‹ ìš© ê²½ë¡œ ìœ„í—˜ ì¦ê°€
            credit_score += (100 - ra.risk_appetite_score) * 0.3
        
        # 0-100 ë²”ìœ„ë¡œ í´ë¦¬í•‘
        contributions['credit'] = max(0.0, min(100.0, credit_score))
        
        # 4. Volatility ê²½ë¡œ (ë³€ë™ì„±/ê³µí¬)
        volatility_score = 0.0
        if submodule_results.get('risk_appetite'):
            ra = submodule_results['risk_appetite']
            # ë¶ˆí™•ì‹¤ì„± ì ìˆ˜ í™œìš©
            volatility_score = ra.uncertainty_score * 0.5
        
        # Spilloverì—ì„œ VIX ê´€ë ¨ ê²½ë¡œ
        if submodule_results.get('spillover'):
            spillover = submodule_results['spillover']
            for edge in spillover.active_paths:
                if edge.category == 'volatility':
                    volatility_score += edge.signal_strength * 0.5
        
        # 0-100 ë²”ìœ„ë¡œ í´ë¦¬í•‘
        contributions['volatility'] = max(0.0, min(100.0, volatility_score))
        
        # 5. Rotation ê²½ë¡œ (ì„¹í„° ë¡œí…Œì´ì…˜) - ê°œì„ 
        rotation_score = 0.0
        
        # Spillover ê²½ë¡œì—ì„œ ë¡œí…Œì´ì…˜ ì‹ í˜¸ í™•ì¸
        if submodule_results.get('spillover'):
            spillover = submodule_results['spillover']
            for edge in spillover.active_paths:
                if edge.category == 'rotation':
                    rotation_score += edge.signal_strength * 0.5
        
        # ì§ì ‘ ê³„ì‚° ì¶”ê°€: Risk Appetiteì˜ components í™œìš©
        if submodule_results.get('risk_appetite'):
            ra = submodule_results['risk_appetite']
            components = ra.components
            
            # IWM/SPY zscoreë¡œ ì†Œí˜•ì£¼ ë¡œí…Œì´ì…˜ í™•ì¸ (ì„ê³„ê°’ ì™„í™”)
            iwm_spy_z = abs(components.get('iwm_spy_zscore', 0))
            if iwm_spy_z > 0.8:  # ì„ê³„ê°’ ì™„í™”: 1.0 -> 0.8
                rotation_score += iwm_spy_z * 10
            
            # XLY/XLP zscoreë¡œ ì„¹í„° ë¡œí…Œì´ì…˜ í™•ì¸ (ì„ê³„ê°’ ì™„í™”)
            xly_xlp_z = abs(components.get('xly_xlp_zscore', 0))
            if xly_xlp_z > 0.8:  # ì„ê³„ê°’ ì™„í™”: 1.0 -> 0.8
                rotation_score += xly_xlp_z * 8
            
            # VRP(Variance Risk Premium)ê°€ ë†’ìœ¼ë©´ ë¡œí…Œì´ì…˜ ì‹ í˜¸
            vrp_score = components.get('vrp_score', 50)
            if vrp_score > 60:
                rotation_score += (vrp_score - 60) * 0.3
        
        # 0-100 ë²”ìœ„ë¡œ í´ë¦¬í•‘
        contributions['rotation'] = max(0.0, min(100.0, rotation_score))
        
        # 6. Crypto ê²½ë¡œ
        crypto_score = 0.0
        if submodule_results.get('crypto'):
            crypto = submodule_results['crypto']
            # ìœ„í—˜ ê¸°ì—¬ë„ í™œìš© (0-0.2 ë²”ìœ„ë¥¼ 0-40ìœ¼ë¡œ ë³€í™˜, ìƒí•œì„  ì„¤ì •)
            crypto_score = min(crypto.risk_contribution * 200, 40)  # ìµœëŒ€ 40ì 
            # ì‹¬ë¦¬ ì ìˆ˜ë„ ë°˜ì˜ (ê·¹ë‹¨ì  ìƒíƒœì¼ ë•Œ)
            if crypto.sentiment_level in ['EXTREME_FEAR', 'EXTREME_GREED']:
                crypto_score += 20
        
        # 0-100 ë²”ìœ„ë¡œ í´ë¦¬í•‘
        contributions['crypto'] = max(0.0, min(100.0, crypto_score))
        
        # ì‹œê°í™”ìš© ë¶„í¬ ê³„ì‚° (100% ì •ê·œí™”)
        path_distribution = {}
        total_raw = sum(contributions.values())
        if total_raw > 0:
            for path_name, score in contributions.items():
                path_distribution[path_name] = (score / total_raw) * 100.0
        else:
            # ëª¨ë“  ê²½ë¡œê°€ 0ì¸ ê²½ìš° ê· ë“± ë¶„ë°°
            num_paths = len(contributions)
            if num_paths > 0:
                equal_share = 100.0 / num_paths
                path_distribution = {path_name: equal_share for path_name in contributions.keys()}
            else:
                path_distribution = {}
        
        return contributions, path_distribution
    
    def adjust_weights_for_regime(self, regime: str) -> Dict[str, float]:
        """
        ë ˆì§ì— ë”°ë¼ ê²½ë¡œ ê°€ì¤‘ì¹˜ ì¡°ì •
        
        BULL: ì§‘ì¤‘ë„ ê²½ë¡œ ê°€ì¤‘ì¹˜ ì¦ê°€ (ê· ì—´ ê°ì§€ ì¤‘ìš”)
        BEAR: ì‹ ìš©, ìœ ë™ì„± ê²½ë¡œ ê°€ì¤‘ì¹˜ ì¦ê°€
        CRISIS: ë³€ë™ì„± ê²½ë¡œ ê°€ì¤‘ì¹˜ ì¦ê°€
        
        Returns:
            ì¡°ì •ëœ ê°€ì¤‘ì¹˜ ë”•ì…”ë„ˆë¦¬
        """
        weights = self.BASE_PATH_WEIGHTS.copy()
        
        if regime == "BULL":
            # ì§‘ì¤‘ë„ ê²½ë¡œ ê°€ì¤‘ì¹˜ ì¦ê°€
            weights['concentration'] *= 1.3
            weights['liquidity'] *= 0.9
            weights['credit'] *= 0.8
        elif regime == "BEAR":
            # ì‹ ìš©, ìœ ë™ì„± ê²½ë¡œ ê°€ì¤‘ì¹˜ ì¦ê°€
            weights['credit'] *= 1.4
            weights['liquidity'] *= 1.2
            weights['volatility'] *= 1.1
        elif regime == "CRISIS":
            # ë³€ë™ì„± ê²½ë¡œ ê°€ì¤‘ì¹˜ ì¦ê°€
            weights['volatility'] *= 1.5
            weights['credit'] *= 1.3
            weights['liquidity'] *= 1.2
        # TRANSITIONì€ ê¸°ë³¸ ê°€ì¤‘ì¹˜ ìœ ì§€
        
        # í•©ê³„ë¥¼ 1.0ìœ¼ë¡œ ì •ê·œí™”
        total = sum(weights.values())
        if total > 0:
            weights = {k: v / total for k, v in weights.items()}
        
        return weights
    
    def calculate_total_risk(
        self, 
        path_contributions: Dict[str, float]
    ) -> Tuple[float, str]:
        """
        ì „ì²´ ìœ„í—˜ë„ ê³„ì‚° (ê°€ì¤‘í‰ê·  ë°©ì‹)
        
        ê° ê²½ë¡œì˜ raw scoreì— BASE_PATH_WEIGHTS ê°€ì¤‘ì¹˜ë¥¼ ê³±í•œ ê°€ì¤‘í‰ê· ì„ ê³„ì‚°í•©ë‹ˆë‹¤.
        ì´ëŠ” í¬íŠ¸í´ë¦¬ì˜¤ ìœ„í—˜ ì¸¡ì •ì˜ í‘œì¤€ ë°©ë²•ë¡ ì…ë‹ˆë‹¤.
        
        Returns:
            Tuple[score (0-100), level]
        
        Level ì •ì˜:
        - 0-25: LOW
        - 25-50: MEDIUM
        - 50-75: HIGH
        - 75-100: CRITICAL
        """
        # ê°€ì¤‘í‰ê·  ê³„ì‚°: ê° ê²½ë¡œì˜ raw score Ã— ê¸°ë³¸ ê°€ì¤‘ì¹˜
        weighted_sum = 0.0
        total_weight = 0.0
        
        for path_name, raw_score in path_contributions.items():
            weight = self.BASE_PATH_WEIGHTS.get(path_name, 0.0)
            weighted_sum += raw_score * weight
            total_weight += weight
        
        # ê°€ì¤‘í‰ê·  ê³„ì‚°
        if total_weight > 0:
            total_score = weighted_sum / total_weight
        else:
            total_score = 0.0
        
        # 0-100 ë²”ìœ„ë¡œ í´ë¦¬í•‘
        total_score = max(0.0, min(100.0, total_score))
        
        if total_score < 25:
            level = "LOW"
        elif total_score < 50:
            level = "MEDIUM"
        elif total_score < 75:
            level = "HIGH"
        else:
            level = "CRITICAL"
        
        return total_score, level
    
    def generate_warnings(self, submodule_results: Dict) -> List[str]:
        """
        í™œì„±í™”ëœ ê²½ê³  ëª©ë¡ ìƒì„±
        
        ê²½ê³  ì˜ˆì‹œ:
        - "TLT ê¸‰ë½ ì¤‘: ê¸ˆë¦¬ ìƒìŠ¹ ì••ë ¥, QQQ ì˜í–¥ ì˜ˆìƒ (3ì¼ ì‹œì°¨)"
        - "BTC ì„ í–‰ í•˜ë½ ê°ì§€: RISK_OFF_WARNING"
        - "ë ˆì§ ì „í™˜ ì§•í›„: BULL â†’ TRANSITION (í™•ë¥  65%)"
        - "VIX-ì‹¤í˜„ë³€ë™ì„± ê´´ë¦¬ í™•ëŒ€: ë¶ˆí™•ì‹¤ì„± í”„ë¦¬ë¯¸ì—„ ì¦ê°€"
        
        Returns:
            List of warning strings
        """
        warnings = []
        
        # 1. Spillover ê²½ë¡œ ê²½ê³ 
        if submodule_results.get('spillover'):
            spillover = submodule_results['spillover']
            for edge in spillover.active_paths:
                if edge.signal_strength >= 70:
                    direction = "í•˜ë½" if edge.expected_target_move == "DOWN" else "ìƒìŠ¹"
                    warnings.append(
                        f"{edge.source} ê¸‰ë³€ ì¤‘: {edge.theory_note}, "
                        f"{edge.target} {direction} ì••ë ¥ ì˜ˆìƒ ({edge.adjusted_lag}ì¼ ì‹œì°¨)"
                    )
        
        # 2. ë ˆì§ ì „í™˜ ê²½ê³ 
        if submodule_results.get('regime'):
            regime = submodule_results['regime']
            if regime.transition_probability >= 50:
                warnings.append(
                    f"ë ˆì§ ì „í™˜ ì§•í›„: {regime.current_regime} â†’ "
                    f"{regime.transition_direction} (í™•ë¥  {regime.transition_probability:.0f}%)"
                )
        
        # 3. Crypto ì„ í–‰ì§€í‘œ ê²½ê³ 
        if submodule_results.get('crypto'):
            crypto = submodule_results['crypto']
            if crypto.is_leading_indicator and crypto.leading_signal:
                if crypto.leading_signal == "RISK_OFF_WARNING":
                    warnings.append("BTC ì„ í–‰ í•˜ë½ ê°ì§€: RISK_OFF_WARNING - ì£¼ì‹ ì‹œì¥ í•˜ë½ ì„ í–‰ ê°€ëŠ¥")
                elif crypto.leading_signal == "RISK_ON_SIGNAL":
                    warnings.append("BTC ì„ í–‰ ìƒìŠ¹ ê°ì§€: RISK_ON_SIGNAL - ì£¼ì‹ ì‹œì¥ ìƒìŠ¹ ì„ í–‰ ê°€ëŠ¥")
        
        # 4. ë¶ˆí™•ì‹¤ì„± í”„ë¦¬ë¯¸ì—„ ê²½ê³ 
        if submodule_results.get('risk_appetite'):
            ra = submodule_results['risk_appetite']
            if 'vix_realized_gap' in ra.components:
                gap = ra.components['vix_realized_gap']
                if gap > 10:
                    warnings.append(
                        f"VIX-ì‹¤í˜„ë³€ë™ì„± ê´´ë¦¬ í™•ëŒ€ ({gap:.1f}%p): ë¶ˆí™•ì‹¤ì„± í”„ë¦¬ë¯¸ì—„ ì¦ê°€"
                    )
        
        # 5. ìœ„ê¸° ìƒíƒœ ê²½ê³ 
        if submodule_results.get('regime'):
            regime = submodule_results['regime']
            if regime.current_regime == "CRISIS":
                warnings.append("ğŸš¨ ìœ„ê¸° ë ˆì§ ê°ì§€: ìœ ë™ì„± í™•ë³´ ë° ë°©ì–´ì  í¬ì§€ì…˜ ê¶Œì¥")
        
        return warnings
    
    def generate_interpretation(
        self, 
        total_risk: float,
        risk_level: str,
        path_contributions: Dict,
        submodule_results: Dict
    ) -> str:
        """
        ì¢…í•© í•´ì„ í…ìŠ¤íŠ¸ ìƒì„±
        
        Returns:
            str: í•´ì„ í…ìŠ¤íŠ¸
        """
        # ê¸°ë³¸ ìœ„í—˜ë„ ì„¤ëª…
        interpretation = f"í˜„ì¬ ì‹œì¥ ìœ„í—˜ë„ëŠ” {total_risk:.1f}% ({risk_level}) ìˆ˜ì¤€ì…ë‹ˆë‹¤. "
        
        # ì£¼ìš” ìœ„í—˜ ê²½ë¡œ
        sorted_paths = sorted(
            path_contributions.items(), 
            key=lambda x: x[1], 
            reverse=True
        )
        
        if sorted_paths:
            top_path = sorted_paths[0]
            path_names = {
                'liquidity': 'ìœ ë™ì„±/ê¸ˆë¦¬',
                'concentration': 'AI/ë¹…í…Œí¬ ì§‘ì¤‘',
                'credit': 'ì‹ ìš© ìŠ¤íŠ¸ë ˆìŠ¤',
                'volatility': 'ë³€ë™ì„±/ê³µí¬',
                'rotation': 'ì„¹í„° ë¡œí…Œì´ì…˜',
                'crypto': 'ì•”í˜¸í™”í'
            }
            top_path_name = path_names.get(top_path[0], top_path[0])
            interpretation += f"ì£¼ìš” ìœ„í—˜ ìš”ì¸ì€ {top_path_name} ê²½ë¡œ({top_path[1]:.1f}%)ì…ë‹ˆë‹¤. "
        
        # ë ˆì§ ì •ë³´
        if submodule_results.get('regime'):
            regime = submodule_results['regime']
            interpretation += f"í˜„ì¬ {regime.current_regime} ë ˆì§ì´ë©°, "
            if regime.transition_probability >= 50:
                interpretation += f"ë ˆì§ ì „í™˜ í™•ë¥ ì´ {regime.transition_probability:.0f}%ë¡œ ìƒìŠ¹ ì¤‘ì…ë‹ˆë‹¤. "
            else:
                interpretation += f"ë ˆì§ ì•ˆì •ë„ëŠ” {regime.regime_confidence:.0f}%ì…ë‹ˆë‹¤. "
        
        # Risk Appetite ì •ë³´
        if submodule_results.get('risk_appetite'):
            ra = submodule_results['risk_appetite']
            interpretation += (
                f"ë¦¬ìŠ¤í¬ ì„ í˜¸ë„ëŠ” {ra.risk_appetite_score:.0f}ì ({ra.risk_appetite_level}), "
                f"ë¶ˆí™•ì‹¤ì„±ì€ {ra.uncertainty_score:.0f}ì ({ra.uncertainty_level})ì…ë‹ˆë‹¤. "
            )
        
        # Spillover ì •ë³´
        if submodule_results.get('spillover'):
            spillover = submodule_results['spillover']
            if spillover.active_paths:
                interpretation += (
                    f"í™œì„±í™”ëœ ì¶©ê²© ì „ì´ ê²½ë¡œëŠ” {len(spillover.active_paths)}ê°œì´ë©°, "
                    f"ì£¼ìš” ìœ„í—˜ ì§„ì›ì§€ëŠ” {spillover.primary_risk_source}ì…ë‹ˆë‹¤. "
                )
        
        return interpretation
    
    def analyze(
        self, 
        market_data: Dict[str, pd.DataFrame]
    ) -> CriticalPathResult:
        """
        ì „ì²´ ë¶„ì„ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
        
        1. ë°ì´í„° ê²€ì¦ ë° ì •ë¦¬
        2. í•˜ìœ„ ëª¨ë“ˆ ì‹¤í–‰
        3. ê²½ë¡œë³„ ê¸°ì—¬ë„ ê³„ì‚°
        4. ì „ì²´ ìœ„í—˜ë„ ì‚°ì¶œ
        5. ê²½ê³  ë° í•´ì„ ìƒì„±
        6. ê²°ê³¼ ê°ì²´ ë°˜í™˜
        
        Args:
            market_data: í‹°ì»¤ë³„ ê°€ê²© ë°ì´í„° ë”•ì…”ë„ˆë¦¬
        
        Returns:
            CriticalPathResult ê°ì²´
        """
        # 1. ë°ì´í„° ê²€ì¦
        data_check = self.collect_required_data(market_data)
        if len(data_check['missing']) > 5:
            print(f"Warning: {len(data_check['missing'])} required tickers missing")
        
        # 2. í•˜ìœ„ ëª¨ë“ˆ ì‹¤í–‰
        submodule_results = self.run_submodules(market_data)
        
        # ë ˆì§ ì •ë³´ ì¶”ì¶œ
        if submodule_results.get('regime'):
            current_regime = submodule_results['regime'].current_regime
            regime_confidence = submodule_results['regime'].regime_confidence
            transition_prob = submodule_results['regime'].transition_probability
        else:
            current_regime = "TRANSITION"
            regime_confidence = 50.0
            transition_prob = 0.0
        
        # 3. ê²½ë¡œë³„ ê¸°ì—¬ë„ ê³„ì‚°
        path_contributions, path_distribution = self.calculate_path_contributions(
            submodule_results,
            current_regime
        )
        
        # 4. ì „ì²´ ìœ„í—˜ë„ ì‚°ì¶œ
        total_risk, risk_level = self.calculate_total_risk(path_contributions)
        
        # 5. ì£¼ìš” ìœ„í—˜ ê²½ë¡œ ì‹ë³„
        if path_contributions:
            primary_risk_path = max(path_contributions.items(), key=lambda x: x[1])[0]
        else:
            primary_risk_path = "NONE"
        
        # 6. ê²½ê³  ìƒì„±
        active_warnings = self.generate_warnings(submodule_results)
        
        # 7. í•´ì„ ìƒì„±
        interpretation = self.generate_interpretation(
            total_risk,
            risk_level,
            path_contributions,
            submodule_results
        )
        
        # 8. ê²°ê³¼ ê°ì²´ ìƒì„± (None ê°’ ì²˜ë¦¬)
        risk_appetite_result = submodule_results.get('risk_appetite')
        regime_result = submodule_results.get('regime')
        spillover_result = submodule_results.get('spillover')
        crypto_result = submodule_results.get('crypto')
        
        # ê¸°ë³¸ê°’ ìƒì„± (Noneì¸ ê²½ìš°)
        if risk_appetite_result is None:
            risk_appetite_result = RiskAppetiteUncertaintyResult(
                timestamp=datetime.now().isoformat(),
                risk_appetite_score=50.0,
                uncertainty_score=50.0,
                risk_appetite_level="MEDIUM",
                uncertainty_level="MEDIUM",
                market_state="MIXED",
                components={},
                interpretation="ë°ì´í„° ë¶€ì¡±ìœ¼ë¡œ ë¶„ì„ ë¶ˆê°€"
            )
        
        if regime_result is None:
            regime_result = RegimeResult(
                timestamp=datetime.now().isoformat(),
                current_regime="TRANSITION",
                regime_confidence=50.0,
                transition_probability=0.0,
                transition_direction="STABLE",
                thresholds={},
                ma_status={},
                interpretation="ë°ì´í„° ë¶€ì¡±ìœ¼ë¡œ ë¶„ì„ ë¶ˆê°€"
            )
        
        if spillover_result is None:
            spillover_result = SpilloverResult(
                timestamp=datetime.now().isoformat(),
                active_paths=[],
                risk_score=0.0,
                primary_risk_source="NONE",
                expected_impacts={},
                interpretation="ë°ì´í„° ë¶€ì¡±ìœ¼ë¡œ ë¶„ì„ ë¶ˆê°€"
            )
        
        if crypto_result is None:
            crypto_result = CryptoSentimentResult(
                timestamp=datetime.now().isoformat(),
                sentiment_score=50.0,
                sentiment_level="NEUTRAL",
                btc_spy_correlation=0.0,
                correlation_regime="DECOUPLED",
                is_leading_indicator=False,
                leading_signal=None,
                risk_contribution=0.05,
                components={},
                interpretation="ë°ì´í„° ë¶€ì¡±ìœ¼ë¡œ ë¶„ì„ ë¶ˆê°€"
            )
        
        return CriticalPathResult(
            timestamp=datetime.now().isoformat(),
            total_risk_score=total_risk,
            risk_level=risk_level,
            current_regime=current_regime,
            regime_confidence=regime_confidence,
            transition_probability=transition_prob,
            path_contributions=path_contributions,
            path_distribution=path_distribution,
            risk_appetite_result=risk_appetite_result,
            regime_result=regime_result,
            spillover_result=spillover_result,
            crypto_result=crypto_result,
            primary_risk_path=primary_risk_path,
            active_warnings=active_warnings,
            interpretation=interpretation
        )


# ============================================================
# í†µí•© í•¨ìˆ˜ (main.pyì—ì„œ í˜¸ì¶œ)
# ============================================================

def run_critical_path_analysis(
    market_data: Dict[str, pd.DataFrame]
) -> CriticalPathResult:
    """
    Critical Path ë¶„ì„ ì‹¤í–‰ (í¸ì˜ í•¨ìˆ˜)
    
    Usage:
        from critical_path_analyzer import run_critical_path_analysis
        result = run_critical_path_analysis(market_data)
        print(f"Total Risk: {result.total_risk_score}%")
    
    Args:
        market_data: í‹°ì»¤ë³„ ê°€ê²© ë°ì´í„° ë”•ì…”ë„ˆë¦¬
    
    Returns:
        CriticalPathResult ê°ì²´
    """
    aggregator = CriticalPathAggregator()
    return aggregator.analyze(market_data)