"""
Multi-LLM Insight Discussion System
====================================
ì—¬ëŸ¬ LLM ëª¨ë¸ì„ í™œìš©í•œ ì¸ì‚¬ì´íŠ¸ ì¶”ì¶œ ë° í† ë¡  ì‹œìŠ¤í…œ

ì§€ì› ëª¨ë¸:
- Perplexity (sonar-pro): ì‹¤ì‹œê°„ ê²€ìƒ‰ ê¸°ë°˜ ì¸ì‚¬ì´íŠ¸
- Claude (claude-opus-4-5-20251101): ì‹¬ì¸µ ë¶„ì„
- Gemini (gemini-2.0-flash-exp): ë¹ ë¥¸ íŒ¨í„´ ì¸ì‹
- OpenAI (o1-mini / gpt-4o): êµ¬ì¡°ì  ì¶”ë¡ 
"""

import asyncio
import json
import os
from dataclasses import dataclass, field
from datetime import datetime
from enum import Enum
from typing import Any, Dict, List, Optional

from dotenv import load_dotenv

load_dotenv()


class LLMProvider(Enum):
    """LLM ì œê³µìž"""
    PERPLEXITY = "perplexity"
    CLAUDE = "claude"
    GEMINI = "gemini"
    OPENAI = "openai"


@dataclass
class LLMInsight:
    """ê°œë³„ LLMì˜ ì¸ì‚¬ì´íŠ¸"""
    provider: LLMProvider
    model: str
    topic: str
    insight: str
    confidence: float  # 0.0 ~ 1.0
    key_points: List[str] = field(default_factory=list)
    risks_identified: List[str] = field(default_factory=list)
    opportunities: List[str] = field(default_factory=list)
    disagreements: List[str] = field(default_factory=list)  # ë‹¤ë¥¸ ëª¨ë¸ê³¼ì˜ ì˜ê²¬ ì°¨ì´
    timestamp: str = ""

    def __post_init__(self):
        if not self.timestamp:
            self.timestamp = datetime.now().isoformat()


@dataclass
class DiscussionResult:
    """í† ë¡  ê²°ê³¼"""
    topic: str
    insights: List[LLMInsight]
    consensus_points: List[str]  # ëª¨ë“  ëª¨ë¸ì´ ë™ì˜í•˜ëŠ” í¬ì¸íŠ¸
    divergence_points: List[str]  # ì˜ê²¬ ì°¨ì´ê°€ ìžˆëŠ” í¬ì¸íŠ¸
    final_synthesis: str  # ìµœì¢… ì¢…í•©
    actionable_items: List[str]  # ì‹¤í–‰ ê°€ëŠ¥í•œ í•­ëª©
    confidence_score: float
    timestamp: str = ""

    def __post_init__(self):
        if not self.timestamp:
            self.timestamp = datetime.now().isoformat()


class MultiLLMDiscussion:
    """ë©€í‹° LLM í† ë¡  ì‹œìŠ¤í…œ"""

    # ìµœì‹  ëª¨ë¸ ì„¤ì •
    MODELS = {
        LLMProvider.PERPLEXITY: "sonar-pro",
        LLMProvider.CLAUDE: "claude-opus-4-5-20251101",
        LLMProvider.GEMINI: "gemini-2.0-flash-exp",
        LLMProvider.OPENAI: "gpt-4o"
    }

    def __init__(self):
        self._clients = {}
        self._validate_api_keys()

    def _validate_api_keys(self) -> Dict[str, bool]:
        """API í‚¤ ê²€ì¦"""
        keys = {
            LLMProvider.PERPLEXITY: os.getenv('PERPLEXITY_API_KEY'),
            LLMProvider.CLAUDE: os.getenv('ANTHROPIC_API_KEY'),
            LLMProvider.GEMINI: os.getenv('GEMINI_API_KEY'),
            LLMProvider.OPENAI: os.getenv('OPENAI_API_KEY')
        }
        self.available_providers = {k: bool(v) for k, v in keys.items()}
        return self.available_providers

    def _get_client(self, provider: LLMProvider):
        """API í´ë¼ì´ì–¸íŠ¸ ë°˜í™˜"""
        if provider in self._clients:
            return self._clients[provider]

        if provider == LLMProvider.PERPLEXITY:
            from openai import OpenAI
            self._clients[provider] = OpenAI(
                api_key=os.getenv('PERPLEXITY_API_KEY'),
                base_url="https://api.perplexity.ai"
            )

        elif provider == LLMProvider.CLAUDE:
            import anthropic
            self._clients[provider] = anthropic.Anthropic(
                api_key=os.getenv('ANTHROPIC_API_KEY')
            )

        elif provider == LLMProvider.GEMINI:
            import google.generativeai as genai
            genai.configure(api_key=os.getenv('GEMINI_API_KEY'))
            self._clients[provider] = genai.GenerativeModel(self.MODELS[provider])

        elif provider == LLMProvider.OPENAI:
            from openai import OpenAI
            self._clients[provider] = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))

        return self._clients[provider]

    async def get_insight(
        self,
        provider: LLMProvider,
        topic: str,
        context: Dict[str, Any],
        other_insights: Optional[List[LLMInsight]] = None
    ) -> Optional[LLMInsight]:
        """ê°œë³„ LLMì—ì„œ ì¸ì‚¬ì´íŠ¸ ì¶”ì¶œ"""

        if not self.available_providers.get(provider):
            print(f"[WARN] {provider.value} API key not available")
            return None

        # ì»¨í…ìŠ¤íŠ¸ ìš”ì•½
        context_summary = self._summarize_context(context)

        # ë‹¤ë¥¸ ì¸ì‚¬ì´íŠ¸ê°€ ìžˆìœ¼ë©´ ì°¸ì¡°
        other_views = ""
        if other_insights:
            other_views = "\n\n### ë‹¤ë¥¸ ëª¨ë¸ë“¤ì˜ ì˜ê²¬:\n"
            for insight in other_insights:
                other_views += f"- {insight.provider.value}: {insight.insight[:200]}...\n"

        prompt = f"""ë‹¹ì‹ ì€ ê¸ˆìœµ ì‹œìž¥ ë¶„ì„ ì „ë¬¸ê°€ìž…ë‹ˆë‹¤.
ë‹¤ìŒ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ê³  ì¸ì‚¬ì´íŠ¸ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”.

### í† í”½: {topic}

### í˜„ìž¬ ì‹œìž¥ ë°ì´í„°:
{context_summary}
{other_views}

### ìš”ì²­ì‚¬í•­:
1. í•µì‹¬ ì¸ì‚¬ì´íŠ¸ (3-5ë¬¸ìž¥)
2. ì£¼ìš” í¬ì¸íŠ¸ (3-5ê°œ bullet points)
3. ì‹ë³„ëœ ë¦¬ìŠ¤í¬ (ìžˆë‹¤ë©´)
4. ê¸°íšŒ ìš”ì¸ (ìžˆë‹¤ë©´)
5. ì‹ ë¢°ë„ (0.0-1.0)
6. ë‹¤ë¥¸ ì˜ê²¬ê³¼ ë‹¤ë¥¸ ì  (ìžˆë‹¤ë©´)

JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
{{
    "insight": "í•µì‹¬ ì¸ì‚¬ì´íŠ¸...",
    "key_points": ["í¬ì¸íŠ¸1", "í¬ì¸íŠ¸2", ...],
    "risks": ["ë¦¬ìŠ¤í¬1", ...],
    "opportunities": ["ê¸°íšŒ1", ...],
    "confidence": 0.8,
    "disagreements": ["ë‹¤ë¥¸ ì 1", ...]
}}"""

        try:
            response = await self._call_llm(provider, prompt)
            return self._parse_insight(provider, topic, response)
        except Exception as e:
            print(f"[ERROR] {provider.value}: {e}")
            return None

    async def _call_llm(self, provider: LLMProvider, prompt: str) -> str:
        """LLM API í˜¸ì¶œ"""
        client = self._get_client(provider)

        if provider == LLMProvider.PERPLEXITY:
            response = client.chat.completions.create(
                model=self.MODELS[provider],
                messages=[{"role": "user", "content": prompt}],
                max_tokens=2000
            )
            return response.choices[0].message.content

        elif provider == LLMProvider.CLAUDE:
            response = client.messages.create(
                model=self.MODELS[provider],
                max_tokens=2000,
                messages=[{"role": "user", "content": prompt}]
            )
            return response.content[0].text

        elif provider == LLMProvider.GEMINI:
            response = await asyncio.get_event_loop().run_in_executor(
                None, lambda: client.generate_content(prompt)
            )
            return response.text

        elif provider == LLMProvider.OPENAI:
            response = client.chat.completions.create(
                model=self.MODELS[provider],
                messages=[{"role": "user", "content": prompt}],
                max_tokens=2000
            )
            return response.choices[0].message.content

        return ""

    def _summarize_context(self, context: Dict[str, Any]) -> str:
        """ì»¨í…ìŠ¤íŠ¸ ìš”ì•½"""
        summary_parts = []

        if 'market_summary' in context:
            summary_parts.append(f"ì‹œìž¥ ìš”ì•½: {context['market_summary']}")

        if 'regime_analysis' in context:
            summary_parts.append(f"ë ˆì§ ë¶„ì„: {context['regime_analysis']}")

        if 'technical_indicators' in context:
            ti = context['technical_indicators']
            summary_parts.append(f"""ê¸°ìˆ  ì§€í‘œ:
- VIX: {ti.get('vix', 'N/A')} (ë³€ë™: {ti.get('vix_change', 'N/A')})
- RSI: {ti.get('rsi_14', 'N/A')}
- MACD: {ti.get('macd', 'N/A')}
- í˜„ìž¬ê°€: {ti.get('current_price', 'N/A')}
- ì§€ì§€ì„ : {ti.get('support_level', 'N/A')}
- ì €í•­ì„ : {ti.get('resistance_level', 'N/A')}""")

        if 'global_market' in context:
            gm = context['global_market']
            summary_parts.append(f"""ê¸€ë¡œë²Œ ì‹œìž¥:
- DXY: {gm.get('dxy', 'N/A')}
- Nikkei: {gm.get('nikkei', 'N/A')} ({gm.get('nikkei_change', 'N/A'):.1f}%)
- DAX: {gm.get('dax', 'N/A')} ({gm.get('dax_change', 'N/A'):.1f}%)
- Gold: {gm.get('gold', 'N/A')}
- WTI: {gm.get('wti', 'N/A')}
- ê¸€ë¡œë²Œ ì„¼í‹°ë¨¼íŠ¸: {gm.get('global_sentiment', 'N/A')}""")

        if 'scenarios' in context:
            scenarios = context['scenarios']
            scenario_str = "ì‹œë‚˜ë¦¬ì˜¤:\n"
            for s in scenarios:
                scenario_str += f"- {s.get('name', 'N/A')}: {s.get('probability', 'N/A')}% ({s.get('expected_return', 'N/A')})\n"
            summary_parts.append(scenario_str)

        if 'sector_recommendations' in context:
            sr = context['sector_recommendations']
            bullish = [s['name'] for s in sr.get('bullish_sectors', [])]
            bearish = [s['name'] for s in sr.get('bearish_sectors', [])]
            summary_parts.append(f"""ì„¹í„° ê¶Œê³ :
- ìœ ë§: {', '.join(bullish)}
- ì£¼ì˜: {', '.join(bearish)}""")

        return "\n\n".join(summary_parts)

    def _parse_insight(self, provider: LLMProvider, topic: str, response: str) -> Optional[LLMInsight]:
        """ì‘ë‹µ íŒŒì‹±"""
        try:
            # JSON ì¶”ì¶œ ì‹œë„
            import re
            json_match = re.search(r'\{[\s\S]*\}', response)
            if json_match:
                data = json.loads(json_match.group())
            else:
                # JSON ì—†ìœ¼ë©´ í…ìŠ¤íŠ¸ ê·¸ëŒ€ë¡œ ì‚¬ìš©
                return LLMInsight(
                    provider=provider,
                    model=self.MODELS[provider],
                    topic=topic,
                    insight=response[:500],
                    confidence=0.5,
                    key_points=[],
                    risks_identified=[],
                    opportunities=[],
                    disagreements=[]
                )

            return LLMInsight(
                provider=provider,
                model=self.MODELS[provider],
                topic=topic,
                insight=data.get('insight', ''),
                confidence=data.get('confidence', 0.7),
                key_points=data.get('key_points', []),
                risks_identified=data.get('risks', []),
                opportunities=data.get('opportunities', []),
                disagreements=data.get('disagreements', [])
            )
        except Exception as e:
            print(f"[WARN] Parse error for {provider.value}: {e}")
            return LLMInsight(
                provider=provider,
                model=self.MODELS[provider],
                topic=topic,
                insight=response[:500] if response else "Failed to parse",
                confidence=0.3,
                key_points=[],
                risks_identified=[],
                opportunities=[],
                disagreements=[]
            )

    async def run_discussion(
        self,
        topic: str,
        context: Dict[str, Any],
        rounds: int = 2
    ) -> DiscussionResult:
        """ë©€í‹° ë¼ìš´ë“œ í† ë¡  ì‹¤í–‰"""

        print(f"\n{'='*60}")
        print(f"ðŸ¤– Multi-LLM Discussion: {topic}")
        print(f"{'='*60}")

        all_insights: List[LLMInsight] = []

        # Round 1: ê°œë³„ ì¸ì‚¬ì´íŠ¸ ìˆ˜ì§‘ (ë³‘ë ¬)
        print(f"\n[Round 1] ê°œë³„ ì¸ì‚¬ì´íŠ¸ ìˆ˜ì§‘...")
        providers = [p for p in LLMProvider if self.available_providers.get(p)]

        tasks = [
            self.get_insight(provider, topic, context)
            for provider in providers
        ]

        results = await asyncio.gather(*tasks, return_exceptions=True)

        for result in results:
            if isinstance(result, LLMInsight):
                all_insights.append(result)
                print(f"  âœ… {result.provider.value}: ì‹ ë¢°ë„ {result.confidence:.2f}")
            elif isinstance(result, Exception):
                print(f"  âŒ Error: {result}")

        # Round 2+: ë‹¤ë¥¸ ì˜ê²¬ ì°¸ì¡°í•˜ì—¬ ìž¬ë¶„ì„
        if rounds > 1 and len(all_insights) > 1:
            print(f"\n[Round 2] ìƒí˜¸ ì°¸ì¡° ë¶„ì„...")

            updated_insights = []
            for insight in all_insights:
                other_insights = [i for i in all_insights if i.provider != insight.provider]
                updated = await self.get_insight(
                    insight.provider,
                    topic,
                    context,
                    other_insights
                )
                if updated:
                    updated_insights.append(updated)
                    print(f"  ðŸ”„ {updated.provider.value}: ì—…ë°ì´íŠ¸ë¨")

            if updated_insights:
                all_insights = updated_insights

        # í•©ì˜ì  & ì°¨ì´ì  ë¶„ì„
        consensus, divergence = self._analyze_consensus(all_insights)

        # ìµœì¢… ì¢…í•©
        synthesis = self._synthesize(topic, all_insights, consensus, divergence)

        # ì‹¤í–‰ ê°€ëŠ¥ í•­ëª© ì¶”ì¶œ
        actionables = self._extract_actionables(all_insights)

        # í‰ê·  ì‹ ë¢°ë„
        avg_confidence = sum(i.confidence for i in all_insights) / len(all_insights) if all_insights else 0.0

        return DiscussionResult(
            topic=topic,
            insights=all_insights,
            consensus_points=consensus,
            divergence_points=divergence,
            final_synthesis=synthesis,
            actionable_items=actionables,
            confidence_score=avg_confidence
        )

    def _analyze_consensus(self, insights: List[LLMInsight]) -> tuple:
        """í•©ì˜ì ê³¼ ì°¨ì´ì  ë¶„ì„"""
        if not insights:
            return [], []

        # ëª¨ë“  key_points ìˆ˜ì§‘
        all_points = []
        for insight in insights:
            all_points.extend(insight.key_points)

        # ì¤‘ë³µë„ ê¸°ë°˜ í•©ì˜ì  ì¶”ì¶œ (ë‹¨ìˆœí™”ëœ ë²„ì „)
        consensus = []
        divergence = []

        # ë¦¬ìŠ¤í¬ ê´€ë ¨ í•©ì˜
        all_risks = []
        for insight in insights:
            all_risks.extend(insight.risks_identified)
        if all_risks:
            unique_risks = list(set(all_risks))[:3]
            consensus.append(f"ì‹ë³„ëœ ì£¼ìš” ë¦¬ìŠ¤í¬: {', '.join(unique_risks)}")

        # ê¸°íšŒ ê´€ë ¨ í•©ì˜
        all_opps = []
        for insight in insights:
            all_opps.extend(insight.opportunities)
        if all_opps:
            unique_opps = list(set(all_opps))[:3]
            consensus.append(f"ì‹ë³„ëœ ê¸°íšŒ: {', '.join(unique_opps)}")

        # ì˜ê²¬ ì°¨ì´
        for insight in insights:
            if insight.disagreements:
                divergence.extend([
                    f"[{insight.provider.value}] {d}" for d in insight.disagreements
                ])

        return consensus, divergence

    def _synthesize(
        self,
        topic: str,
        insights: List[LLMInsight],
        consensus: List[str],
        divergence: List[str]
    ) -> str:
        """ìµœì¢… ì¢…í•©"""

        parts = [f"## {topic} - ë©€í‹° LLM ì¢…í•© ë¶„ì„\n"]

        # ì°¸ì—¬ ëª¨ë¸
        models = [f"{i.provider.value} ({i.model})" for i in insights]
        parts.append(f"**ì°¸ì—¬ ëª¨ë¸**: {', '.join(models)}\n")

        # ê°œë³„ ì¸ì‚¬ì´íŠ¸ ìš”ì•½
        parts.append("\n### ê°œë³„ ë¶„ì„ ìš”ì•½\n")
        for insight in insights:
            parts.append(f"**{insight.provider.value.upper()}** (ì‹ ë¢°ë„: {insight.confidence:.0%})")
            parts.append(f"> {insight.insight}\n")

        # í•©ì˜ì 
        if consensus:
            parts.append("\n### âœ… í•©ì˜ëœ í¬ì¸íŠ¸\n")
            for point in consensus:
                parts.append(f"- {point}")

        # ì°¨ì´ì 
        if divergence:
            parts.append("\n### âš ï¸ ì˜ê²¬ ì°¨ì´\n")
            for point in divergence:
                parts.append(f"- {point}")

        return "\n".join(parts)

    def _extract_actionables(self, insights: List[LLMInsight]) -> List[str]:
        """ì‹¤í–‰ ê°€ëŠ¥ í•­ëª© ì¶”ì¶œ"""
        actionables = []

        # ë†’ì€ ì‹ ë¢°ë„ ì¸ì‚¬ì´íŠ¸ì—ì„œ ê¸°íšŒ ì¶”ì¶œ
        high_conf = [i for i in insights if i.confidence >= 0.7]
        for insight in high_conf:
            for opp in insight.opportunities[:2]:
                actionables.append(f"[{insight.provider.value}] {opp}")

        return actionables[:5]  # ìµœëŒ€ 5ê°œ

    def to_markdown(self, result: DiscussionResult) -> str:
        """ë§ˆí¬ë‹¤ìš´ ì¶œë ¥"""
        lines = [
            f"# Multi-LLM Insight Discussion",
            f"**í† í”½**: {result.topic}",
            f"**ì‹œê°„**: {result.timestamp}",
            f"**ì¢…í•© ì‹ ë¢°ë„**: {result.confidence_score:.0%}",
            "",
            result.final_synthesis,
            "",
            "### ðŸ“‹ ì‹¤í–‰ ê°€ëŠ¥ í•­ëª©",
        ]

        for item in result.actionable_items:
            lines.append(f"- {item}")

        return "\n".join(lines)


async def discuss_report_insights(report_path: str) -> DiscussionResult:
    """ë¦¬í¬íŠ¸ íŒŒì¼ ê¸°ë°˜ ì¸ì‚¬ì´íŠ¸ í† ë¡ """

    with open(report_path, 'r', encoding='utf-8') as f:
        report = json.load(f)

    discussion = MultiLLMDiscussion()

    # ì‚¬ìš© ê°€ëŠ¥í•œ API ì¶œë ¥
    print("\nðŸ”‘ Available APIs:")
    for provider, available in discussion.available_providers.items():
        status = "âœ…" if available else "âŒ"
        print(f"  {status} {provider.value}")

    # ì£¼ìš” í† í”½ë³„ í† ë¡ 
    topics = [
        "í˜„ìž¬ ë ˆì§ì—ì„œì˜ ìµœì  íˆ¬ìž ì „ëžµ",
        "ë¦¬ìŠ¤í¬ ëŒ€ë¹„ ê¸°íšŒ ìš”ì¸ ë¶„ì„",
        "í–¥í›„ 1ê°œì›” ì‹œìž¥ ë°©í–¥ì„±"
    ]

    results = []
    for topic in topics:
        result = await discussion.run_discussion(topic, report, rounds=2)
        results.append(result)
        print(f"\n{discussion.to_markdown(result)}")

    return results[0] if results else None


# CLI í…ŒìŠ¤íŠ¸
if __name__ == "__main__":
    import sys

    if len(sys.argv) > 1:
        report_path = sys.argv[1]
    else:
        # ê¸°ë³¸ ê²½ë¡œ
        report_path = "/home/tj/projects/autoai/eimas/outputs/ai_report_20260107_015128.json"

    if os.path.exists(report_path):
        asyncio.run(discuss_report_insights(report_path))
    else:
        print(f"Report not found: {report_path}")
