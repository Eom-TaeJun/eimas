"""
Shock Propagation Graph (SPG)
==============================
ê±°ì‹œì§€í‘œì™€ ìì‚° ê°„ì˜ ì¸ê³¼ê´€ê³„ë¥¼ ê·œëª…í•˜ëŠ” ì¶©ê²© ì „íŒŒ ë„¤íŠ¸ì›Œí¬

ê²½ì œí•™ì  ë°°ê²½:
- ë‹¨ìˆœ ìƒê´€ê´€ê³„(Correlation)ê°€ ì•„ë‹Œ ì¸ê³¼ê´€ê³„(Causality) ê·œëª…
- Granger Causality: "Xê°€ Yë¥¼ ì˜ˆì¸¡í•˜ëŠ” ë° ë„ì›€ì´ ë˜ëŠ”ê°€?"
- Impulse Response: ì¶©ê²©ì´ ì–´ë–»ê²Œ ì „íŒŒë˜ëŠ”ê°€?

í•µì‹¬ ê¸°ëŠ¥:
1. Lead-Lag Analysis: ìµœì  ì‹œì°¨ íƒìƒ‰
2. Granger Causality: í†µê³„ì  ì¸ê³¼ê´€ê³„ ê²€ì •
3. DAG Construction: ë°©í–¥ì„± ê·¸ë˜í”„ êµ¬ì¶•
4. Critical Path: ìµœì¥ ì „íŒŒ ê²½ë¡œ íƒìƒ‰
5. Node Centrality: ì„ í–‰/í›„í–‰ ì§€í‘œ êµ¬ë¶„

References:
- Granger (1969): "Investigating Causal Relations by Econometric Models"
- Palantir Ontology: Event-based relationship modeling
"""

import numpy as np
import pandas as pd
import networkx as nx
from typing import Dict, List, Optional, Tuple, Any, Set
from dataclasses import dataclass, field, asdict
from datetime import datetime
from enum import Enum
import warnings

warnings.filterwarnings('ignore')

# Optional imports
try:
    from statsmodels.tsa.stattools import grangercausalitytests, adfuller
    from statsmodels.tsa.api import VAR
    STATSMODELS_AVAILABLE = True
except ImportError:
    STATSMODELS_AVAILABLE = False
    print("[WARN] statsmodels not available. Granger tests will use simplified method.")

try:
    from scipy import stats
    from scipy.signal import correlate
    SCIPY_AVAILABLE = True
except ImportError:
    SCIPY_AVAILABLE = False


class NodeLayer(Enum):
    """
    ë…¸ë“œ ë ˆì´ì–´ (ê²½ì œí•™ì  ê³„ì¸µ)

    ì¶©ê²© ì „íŒŒ ìˆœì„œ:
    POLICY â†’ LIQUIDITY â†’ RISK_PREMIUM â†’ ASSET_PRICE
    """
    POLICY = 1        # Fed Funds, ECB Rate
    LIQUIDITY = 2     # RRP, TGA, M2, Stablecoin Supply
    RISK_PREMIUM = 3  # VIX, Credit Spread, HY Spread
    ASSET_PRICE = 4   # SPY, QQQ, BTC, Gold
    UNKNOWN = 5


class CausalityStrength(Enum):
    """ì¸ê³¼ê´€ê³„ ê°•ë„"""
    STRONG = "strong"      # p < 0.01
    MODERATE = "moderate"  # p < 0.05
    WEAK = "weak"          # p < 0.10
    NONE = "none"          # p >= 0.10


@dataclass
class LeadLagResult:
    """Lead-Lag ë¶„ì„ ê²°ê³¼"""
    source: str
    target: str
    optimal_lag: int           # ì–‘ìˆ˜: sourceê°€ lead, ìŒìˆ˜: targetì´ lead
    max_correlation: float     # ìµœì  lagì—ì„œì˜ ìƒê´€ê´€ê³„
    correlation_at_zero: float # lag=0ì—ì„œì˜ ìƒê´€ê´€ê³„
    is_leading: bool           # sourceê°€ targetì„ ì„ í–‰í•˜ëŠ”ê°€?
    confidence: float          # ì‹ ë¢°ë„ (ìƒê´€ê´€ê³„ ê¸°ë°˜)


@dataclass
class GrangerResult:
    """Granger Causality ê²€ì • ê²°ê³¼"""
    source: str
    target: str
    optimal_lag: int
    f_statistic: float
    p_value: float
    strength: CausalityStrength
    is_significant: bool
    bidirectional: bool = False  # ì–‘ë°©í–¥ ì¸ê³¼ê´€ê³„


@dataclass
class EconomicEdge:
    """
    ê²½ì œí•™ì  ì¸ê³¼ ì—£ì§€ (Palantir Ontology Style)
    
    ëª¨ë“  ì—£ì§€ëŠ” Impulse Response Pathë¡œ ì·¨ê¸‰:
    - ì¶©ê²©ì´ Node Aì—ì„œ ë°œìƒí•˜ë©´ ì–´ë–»ê²Œ Node B, Cë¡œ ì „íŒŒë˜ëŠ”ì§€ ê²½ì œ ì´ë¡ ì— ê¸°ë°˜í•˜ì—¬ ì„¤ëª…
    
    Attributes:
        source: ì›ì¸ ë…¸ë“œ
        target: ê²°ê³¼ ë…¸ë“œ
        sign: ì¸ê³¼ ë°©í–¥ (+: ì–‘ì˜ ê´€ê³„, -: ìŒì˜ ê´€ê³„)
        lag: ì „íŒŒ ì‹œì°¨ (ê±°ë˜ì¼)
        time_horizon: íš¨ê³¼ ì§€ì† ê¸°ê°„ (short/medium/long)
        mechanism: ì „ë‹¬ ë©”ì»¤ë‹ˆì¦˜ (monetary_transmission, risk_premium, etc.)
        theory_reference: ê²½ì œ ì´ë¡  ì¶œì²˜ (IS-LM, QTM, Taylor Rule, etc.)
        narrative: ìì—°ì–´ ì„¤ëª…
    """
    source: str
    target: str
    sign: str  # "+" or "-"
    lag: int = 0
    time_horizon: str = "short"  # "short" (1-5d), "medium" (1-4w), "long" (1m+)
    mechanism: str = ""
    theory_reference: str = ""
    narrative: str = ""
    p_value: float = 1.0
    strength: CausalityStrength = CausalityStrength.NONE
    
    def to_dict(self) -> Dict:
        return {
            'source': self.source,
            'target': self.target,
            'sign': self.sign,
            'lag': self.lag,
            'time_horizon': self.time_horizon,
            'mechanism': self.mechanism,
            'theory_reference': self.theory_reference,
            'narrative': self.narrative,
            'p_value': self.p_value,
            'strength': self.strength.value
        }
    
    def to_arrow(self) -> str:
        """í™”ì‚´í‘œ í˜•íƒœë¡œ í‘œí˜„"""
        sign_symbol = "â†‘" if self.sign == "+" else "â†“"
        return f"{self.source} --[{self.sign}, lag={self.lag}d]--> {self.target}{sign_symbol}"


@dataclass
class ShockPath:
    """ì¶©ê²© ì „íŒŒ ê²½ë¡œ"""
    source: str
    path: List[str]
    total_lag: int             # ì „ì²´ ì „íŒŒ ì‹œê°„ (ì¼)
    cumulative_impact: float   # ëˆ„ì  ì¶©ê²© ê°•ë„
    bottleneck: Optional[str]  # ë³‘ëª© ë…¸ë“œ


@dataclass
class NodeAnalysis:
    """ë…¸ë“œ ë¶„ì„ ê²°ê³¼"""
    node: str
    layer: NodeLayer
    in_degree: int             # ì˜í–¥ ë°›ëŠ” ê´€ê³„ ìˆ˜
    out_degree: int            # ì˜í–¥ ì£¼ëŠ” ê´€ê³„ ìˆ˜
    leading_score: float       # ì„ í–‰ ì ìˆ˜ (out - in)
    betweenness: float         # ì „íŒŒ ì¤‘ê°œ ì ìˆ˜
    avg_lead_time: float       # í‰ê·  ì„ í–‰ ì‹œê°„
    role: str                  # "LEADING", "LAGGING", "BRIDGE", "ISOLATED"


@dataclass
class PropagationAnalysis:
    """ì „ì²´ ì „íŒŒ ë¶„ì„ ê²°ê³¼"""
    timestamp: str
    nodes: List[NodeAnalysis]
    edges: List[Dict]
    critical_paths: List[ShockPath]
    leading_indicators: List[str]
    lagging_indicators: List[str]
    bridge_nodes: List[str]

    def to_dict(self) -> Dict:
        return {
            'timestamp': self.timestamp,
            'nodes': [asdict(n) for n in self.nodes],
            'edges': self.edges,
            'critical_paths': [asdict(p) for p in self.critical_paths],
            'leading_indicators': self.leading_indicators,
            'lagging_indicators': self.lagging_indicators,
            'bridge_nodes': self.bridge_nodes
        }


# ============================================================================
# Layer Classification (ê²½ì œí•™ì  ë„ë©”ì¸ ì§€ì‹)
# ============================================================================

LAYER_MAPPING = {
    # Policy Layer
    'DFF': NodeLayer.POLICY,         # Fed Funds Rate
    'FEDFUNDS': NodeLayer.POLICY,
    'ECB_RATE': NodeLayer.POLICY,

    # Liquidity Layer
    'RRP': NodeLayer.LIQUIDITY,      # Reverse Repo
    'TGA': NodeLayer.LIQUIDITY,      # Treasury General Account
    'M2': NodeLayer.LIQUIDITY,
    'USDT_SUPPLY': NodeLayer.LIQUIDITY,
    'USDC_SUPPLY': NodeLayer.LIQUIDITY,
    'NET_LIQUIDITY': NodeLayer.LIQUIDITY,

    # Risk Premium Layer
    'VIX': NodeLayer.RISK_PREMIUM,
    '^VIX': NodeLayer.RISK_PREMIUM,
    'VIXCLS': NodeLayer.RISK_PREMIUM,
    'HY_SPREAD': NodeLayer.RISK_PREMIUM,
    'BAMLH0A0HYM2': NodeLayer.RISK_PREMIUM,
    'CREDIT_SPREAD': NodeLayer.RISK_PREMIUM,
    'T10Y2Y': NodeLayer.RISK_PREMIUM,

    # Asset Price Layer
    'SPY': NodeLayer.ASSET_PRICE,
    'QQQ': NodeLayer.ASSET_PRICE,
    'TLT': NodeLayer.ASSET_PRICE,
    'GLD': NodeLayer.ASSET_PRICE,
    'BTC': NodeLayer.ASSET_PRICE,
    'BTC-USD': NodeLayer.ASSET_PRICE,
    'ETH-USD': NodeLayer.ASSET_PRICE,
    'DXY': NodeLayer.ASSET_PRICE,
    'DX-Y.NYB': NodeLayer.ASSET_PRICE,
}


# ============================================================================
# Economic Transmission Templates (Domain Knowledge)
# ============================================================================

TRANSMISSION_TEMPLATES: Dict[Tuple[str, str], Dict] = {
    # -------------------------------------------------------------------------
    # IS-LM Model: Mâ†‘ â†’ râ†“ â†’ Iâ†‘ â†’ Yâ†‘
    # "í†µí™” ê³µê¸‰ ì¦ê°€ â†’ ì´ììœ¨ í•˜ë½ â†’ íˆ¬ì ì¦ê°€ â†’ ì†Œë“ ì¦ê°€"
    # -------------------------------------------------------------------------
    ("M2", "DFF"): {
        "sign": "-",
        "mechanism": "liquidity_preference",
        "theory_reference": "IS-LM",
        "time_horizon": "short",
        "narrative": "í†µí™” ê³µê¸‰ ì¦ê°€ëŠ” ìœ ë™ì„± ì„ í˜¸ ì´ë¡ ì— ë”°ë¼ ì´ììœ¨ í•˜ë½ ì••ë ¥ì„ ê°€í•¨"
    },
    ("DFF", "SPY"): {
        "sign": "-",
        "mechanism": "discount_rate",
        "theory_reference": "IS-LM",
        "time_horizon": "medium",
        "narrative": "ê¸ˆë¦¬ í•˜ë½ ì‹œ í• ì¸ìœ¨ ê°ì†Œë¡œ ì£¼ì‹ í˜„ì¬ê°€ì¹˜ ìƒìŠ¹"
    },
    ("DFF", "TLT"): {
        "sign": "-",
        "mechanism": "bond_pricing",
        "theory_reference": "Fixed Income",
        "time_horizon": "short",
        "narrative": "ê¸ˆë¦¬ í•˜ë½ ì‹œ ì±„ê¶Œ ê°€ê²© ìƒìŠ¹ (ì—­ì˜ ê´€ê³„)"
    },
    
    # -------------------------------------------------------------------------
    # Quantity Theory of Money (QTM): MV = PY
    # ì¥ê¸°ì ìœ¼ë¡œ Mâ†‘ â†’ Pâ†‘ (í†µí™” ì¤‘ë¦½ì„±)
    # -------------------------------------------------------------------------
    ("M2", "GLD"): {
        "sign": "+",
        "mechanism": "inflation_hedge",
        "theory_reference": "QTM",
        "time_horizon": "long",
        "narrative": "ì¥ê¸°ì  í†µí™” ì¦ê°€ëŠ” ì¸í”Œë ˆì´ì…˜ ê¸°ëŒ€ë¥¼ í†µí•´ ê¸ˆ ê°€ê²© ìƒìŠ¹"
    },
    ("M2", "BTC-USD"): {
        "sign": "+",
        "mechanism": "inflation_hedge",
        "theory_reference": "QTM",
        "time_horizon": "long",
        "narrative": "ë¹„íŠ¸ì½”ì¸ì€ ë””ì§€í„¸ ê¸ˆìœ¼ë¡œì„œ í†µí™” íŒ½ì°½ì— ëŒ€í•œ ì¸í”Œë ˆì´ì…˜ í—¤ì§€"
    },
    
    # -------------------------------------------------------------------------
    # Risk Premium Channel
    # VIXâ†‘ â†’ ìœ„í—˜ìì‚°â†“, ì•ˆì „ìì‚°â†‘
    # -------------------------------------------------------------------------
    ("VIX", "SPY"): {
        "sign": "-",
        "mechanism": "risk_premium",
        "theory_reference": "CAPM",
        "time_horizon": "short",
        "narrative": "ë³€ë™ì„± ìƒìŠ¹ ì‹œ ì£¼ì‹ ìœ„í—˜ í”„ë¦¬ë¯¸ì—„ ì¦ê°€ë¡œ ê°€ê²© í•˜ë½"
    },
    ("VIX", "TLT"): {
        "sign": "+",
        "mechanism": "flight_to_safety",
        "theory_reference": "Risk Parity",
        "time_horizon": "short",
        "narrative": "ë³€ë™ì„± ê¸‰ë“± ì‹œ ì•ˆì „ìì‚° ì„ í˜¸ë¡œ êµ­ì±„ ê°€ê²© ìƒìŠ¹"
    },
    ("VIX", "GLD"): {
        "sign": "+",
        "mechanism": "safe_haven",
        "theory_reference": "Portfolio Theory",
        "time_horizon": "short",
        "narrative": "ê³µí¬ ì‹¬ë¦¬ ìƒìŠ¹ ì‹œ ê¸ˆìœ¼ë¡œ ìê¸ˆ ì´ë™"
    },
    
    # -------------------------------------------------------------------------
    # Crypto-Treasury Feedback Loop (Stablecoin â†” Treasury)
    # USDT ë°œí–‰ â†’ Treasury ë§¤ì… â†’ ìˆ˜ìµë¥  í•˜ë½ â†’ Risk-On â†’ Crypto ìƒìŠ¹ â†’ USDT ì¶”ê°€ ë°œí–‰
    # -------------------------------------------------------------------------
    ("USDT_SUPPLY", "TLT"): {
        "sign": "+",
        "mechanism": "stablecoin_treasury_demand",
        "theory_reference": "Crypto Liquidity",
        "time_horizon": "short",
        "narrative": "ìŠ¤í…Œì´ë¸”ì½”ì¸ ë°œí–‰ ì¦ê°€ â†’ ë‹´ë³´ë¡œ êµ­ì±„ ë§¤ì… â†’ ì±„ê¶Œ ê°€ê²© ìƒìŠ¹"
    },
    ("TLT", "BTC-USD"): {
        "sign": "+",
        "mechanism": "liquidity_spillover",
        "theory_reference": "Crypto Liquidity",
        "time_horizon": "short",
        "narrative": "ì±„ê¶Œ ìˆ˜ìµë¥  í•˜ë½ â†’ ìˆ˜ìµ ì¶”êµ¬ ìê¸ˆì´ í¬ë¦½í† ë¡œ ì´ë™"
    },
    ("BTC-USD", "USDT_SUPPLY"): {
        "sign": "+",
        "mechanism": "crypto_demand",
        "theory_reference": "Crypto Liquidity",
        "time_horizon": "short",
        "narrative": "í¬ë¦½í†  ìƒìŠ¹ â†’ ê±°ë˜ ìˆ˜ìš” ì¦ê°€ â†’ ìŠ¤í…Œì´ë¸”ì½”ì¸ ì¶”ê°€ ë°œí–‰ (í”¼ë“œë°± ë£¨í”„)"
    },
    
    # -------------------------------------------------------------------------
    # Taylor Rule: Ï€â†‘ â†’ FFRâ†‘ â†’ Assetâ†“
    # -------------------------------------------------------------------------
    ("DFF", "QQQ"): {
        "sign": "-",
        "mechanism": "growth_stock_sensitivity",
        "theory_reference": "Taylor Rule",
        "time_horizon": "short",
        "narrative": "ì„±ì¥ì£¼ëŠ” ê¸ˆë¦¬ ë¯¼ê°ë„ ë†’ìŒ - ê¸ˆë¦¬ ìƒìŠ¹ ì‹œ QQQ í•˜ë½"
    },
    
    # -------------------------------------------------------------------------
    # Credit Spread Channel
    # -------------------------------------------------------------------------
    ("HY_SPREAD", "SPY"): {
        "sign": "-",
        "mechanism": "credit_risk_premium",
        "theory_reference": "Credit Cycle",
        "time_horizon": "medium",
        "narrative": "í•˜ì´ì¼ë“œ ìŠ¤í”„ë ˆë“œ í™•ëŒ€ëŠ” ì‹ ìš© í™˜ê²½ ì•…í™” â†’ ì£¼ì‹ í•˜ë½"
    },
    ("HY_SPREAD", "BTC-USD"): {
        "sign": "-",
        "mechanism": "risk_off_flow",
        "theory_reference": "Credit Cycle",
        "time_horizon": "short",
        "narrative": "ì‹ ìš© ìŠ¤í”„ë ˆë“œ í™•ëŒ€ ì‹œ ìœ„í—˜ìì‚° íšŒí”¼ë¡œ í¬ë¦½í†  í•˜ë½"
    },
    
    # -------------------------------------------------------------------------
    # Net Liquidity (Fed Balance Sheet - TGA - RRP)
    # -------------------------------------------------------------------------
    ("NET_LIQUIDITY", "SPY"): {
        "sign": "+",
        "mechanism": "liquidity_injection",
        "theory_reference": "Fed Put",
        "time_horizon": "short",
        "narrative": "ìˆœìœ ë™ì„± ì¦ê°€ëŠ” ìœ„í—˜ìì‚° ê°€ê²© ìƒìŠ¹ ì§€ì§€"
    },
    ("NET_LIQUIDITY", "BTC-USD"): {
        "sign": "+",
        "mechanism": "liquidity_injection",
        "theory_reference": "Fed Put",
        "time_horizon": "short",
        "narrative": "ìœ ë™ì„± í™•ì¥ ì‹œ í¬ë¦½í†  ìì‚°ë„ ë™ë°˜ ìƒìŠ¹"
    },
    ("RRP", "NET_LIQUIDITY"): {
        "sign": "-",
        "mechanism": "liquidity_drain",
        "theory_reference": "Fed Operations",
        "time_horizon": "short",
        "narrative": "ì—­ë ˆí¬ ì¦ê°€ëŠ” ì‹œìŠ¤í…œì—ì„œ ìœ ë™ì„± í¡ìˆ˜"
    },
    ("TGA", "NET_LIQUIDITY"): {
        "sign": "-",
        "mechanism": "treasury_cash_buildup",
        "theory_reference": "Treasury Operations",
        "time_horizon": "short",
        "narrative": "ì¬ë¬´ë¶€ í˜„ê¸ˆ ì¶•ì ì€ ì‹œìŠ¤í…œ ìœ ë™ì„± ê°ì†Œ"
    },
}


def get_node_layer(node_name: str) -> NodeLayer:
    """ë…¸ë“œì˜ ê²½ì œí•™ì  ë ˆì´ì–´ ê²°ì •"""
    # ì§ì ‘ ë§¤í•‘ í™•ì¸
    if node_name in LAYER_MAPPING:
        return LAYER_MAPPING[node_name]

    # íŒ¨í„´ ë§¤ì¹­
    name_upper = node_name.upper()
    if any(x in name_upper for x in ['FED', 'RATE', 'POLICY']):
        return NodeLayer.POLICY
    if any(x in name_upper for x in ['LIQUID', 'M2', 'RRP', 'TGA', 'STABLE']):
        return NodeLayer.LIQUIDITY
    if any(x in name_upper for x in ['VIX', 'SPREAD', 'CREDIT', 'YIELD']):
        return NodeLayer.RISK_PREMIUM

    return NodeLayer.ASSET_PRICE  # ê¸°ë³¸ê°’


def get_economic_edge(
    source: str, 
    target: str, 
    lag: int = 0,
    p_value: float = 0.05,
    correlation: float = 0.0
) -> EconomicEdge:
    """
    ì†ŒìŠ¤-íƒ€ê²Ÿ ìŒì— ëŒ€í•œ ê²½ì œí•™ì  ì—£ì§€ ìƒì„±
    
    TRANSMISSION_TEMPLATESì—ì„œ ë„ë©”ì¸ ì§€ì‹ì„ ê°€ì ¸ì˜¤ê³ ,
    ì—†ìœ¼ë©´ ìƒê´€ê´€ê³„ ë¶€í˜¸ë¡œ ê¸°ë³¸ ì—£ì§€ ìƒì„±
    """
    template = TRANSMISSION_TEMPLATES.get((source, target), None)
    
    if template:
        # í…œí”Œë¦¿ì—ì„œ ê²½ì œ ì´ë¡  ë©”íƒ€ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        strength = CausalityStrength.STRONG if p_value < 0.01 else \
                   CausalityStrength.MODERATE if p_value < 0.05 else \
                   CausalityStrength.WEAK if p_value < 0.10 else \
                   CausalityStrength.NONE
        
        return EconomicEdge(
            source=source,
            target=target,
            sign=template["sign"],
            lag=lag,
            time_horizon=template.get("time_horizon", "short"),
            mechanism=template.get("mechanism", ""),
            theory_reference=template.get("theory_reference", ""),
            narrative=template.get("narrative", ""),
            p_value=p_value,
            strength=strength
        )
    else:
        # í…œí”Œë¦¿ ì—†ìœ¼ë©´ ìƒê´€ê´€ê³„ ë¶€í˜¸ë¡œ ê¸°ë³¸ ìƒì„±
        sign = "+" if correlation >= 0 else "-"
        source_layer = get_node_layer(source)
        target_layer = get_node_layer(target)
        
        # ë ˆì´ì–´ ê¸°ë°˜ ë©”ì»¤ë‹ˆì¦˜ ì¶”ë¡ 
        if source_layer == NodeLayer.POLICY:
            mechanism = "monetary_policy"
        elif source_layer == NodeLayer.LIQUIDITY:
            mechanism = "liquidity_channel"
        elif source_layer == NodeLayer.RISK_PREMIUM:
            mechanism = "risk_premium_channel"
        else:
            mechanism = "market_correlation"
        
        strength = CausalityStrength.STRONG if p_value < 0.01 else \
                   CausalityStrength.MODERATE if p_value < 0.05 else \
                   CausalityStrength.WEAK if p_value < 0.10 else \
                   CausalityStrength.NONE
        
        return EconomicEdge(
            source=source,
            target=target,
            sign=sign,
            lag=lag,
            time_horizon="short",
            mechanism=mechanism,
            theory_reference="Statistical",
            narrative=f"{source} {'positively' if sign=='+' else 'negatively'} affects {target}",
            p_value=p_value,
            strength=strength
        )


def generate_shock_narrative(path: List[str], edges: List[EconomicEdge]) -> str:
    """
    ì¶©ê²© ì „íŒŒ ê²½ë¡œì— ëŒ€í•œ ê²½ì œí•™ì  ì„œì‚¬ ìƒì„±
    
    Example output:
    "Fed ê¸ˆë¦¬ ì¸í•˜ (DFFâ†“) â†’ ìœ ë™ì„± ì¦ê°€ (M2â†‘) â†’ ìœ„í—˜ í”„ë¦¬ë¯¸ì—„ ê°ì†Œ (VIXâ†“) â†’ ì£¼ì‹ ìƒìŠ¹ (SPYâ†‘)"
    
    Args:
        path: ë…¸ë“œ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸ ['DFF', 'M2', 'VIX', 'SPY']
        edges: ê²½ë¡œì˜ EconomicEdge ë¦¬ìŠ¤íŠ¸
    
    Returns:
        ìì—°ì–´ ì„œì‚¬ ë¬¸ìì—´
    """
    if len(path) < 2:
        return f"ë‹¨ì¼ ë…¸ë“œ: {path[0]}"
    
    narratives = []
    
    for i, (source, target) in enumerate(zip(path[:-1], path[1:])):
        if i < len(edges):
            edge = edges[i]
            sign_symbol = "â†‘" if edge.sign == "+" else "â†“"
            
            if edge.narrative:
                narratives.append(f"[{edge.theory_reference}] {edge.narrative}")
            else:
                narratives.append(f"{source} â†’ {target}{sign_symbol}")
    
    # ìš”ì•½ ìƒì„±
    first_node = path[0]
    last_node = path[-1]
    total_lag = sum(e.lag for e in edges)
    
    summary = f"\nğŸ“Š ì¶©ê²© ì „íŒŒ ìš”ì•½: {first_node} â†’ ... â†’ {last_node}\n"
    summary += f"   ì „íŒŒ ì‹œê°„: {total_lag}ì¼\n"
    summary += f"   ê²½ë¡œ ê¸¸ì´: {len(path)}ê°œ ë…¸ë“œ\n"
    
    return "\nâ†’ ".join(narratives) + summary


def generate_impulse_response_text(
    shock_source: str,
    shock_magnitude: float,
    affected_nodes: Dict[str, float]
) -> str:
    """
    ì„í„ìŠ¤ ë°˜ì‘ ë¶„ì„ í…ìŠ¤íŠ¸ ìƒì„±
    
    Args:
        shock_source: ì¶©ê²© ë°œìƒ ë…¸ë“œ
        shock_magnitude: ì¶©ê²© í¬ê¸° (ì˜ˆ: -0.10 = -10%)
        affected_nodes: {ë…¸ë“œëª…: ì˜í–¥ë„}
    
    Returns:
        ìì—°ì–´ ë¶„ì„ í…ìŠ¤íŠ¸
    """
    direction = "í•˜ë½" if shock_magnitude < 0 else "ìƒìŠ¹"
    pct = abs(shock_magnitude) * 100
    
    lines = [
        f"# ì„í„ìŠ¤ ë°˜ì‘ ë¶„ì„ (Impulse Response)",
        f"",
        f"## ì¶©ê²© ì •ì˜",
        f"- ì¶©ê²© ë…¸ë“œ: **{shock_source}**",
        f"- ì¶©ê²© í¬ê¸°: **{pct:.1f}% {direction}**",
        f"",
        f"## ì „íŒŒ íš¨ê³¼",
    ]
    
    # ì˜í–¥ë„ ìˆœ ì •ë ¬
    sorted_effects = sorted(affected_nodes.items(), key=lambda x: abs(x[1]), reverse=True)
    
    for node, impact in sorted_effects[:10]:
        impact_pct = impact * 100
        impact_dir = "+" if impact > 0 else ""
        
        # í…œí”Œë¦¿ì—ì„œ ë©”ì»¤ë‹ˆì¦˜ ê°€ì ¸ì˜¤ê¸°
        template = TRANSMISSION_TEMPLATES.get((shock_source, node), {})
        mechanism = template.get("mechanism", "indirect_effect")
        theory = template.get("theory_reference", "")
        
        theory_str = f" [{theory}]" if theory else ""
        lines.append(f"| {node} | {impact_dir}{impact_pct:.2f}% | {mechanism}{theory_str} |")
    
    lines.append("")
    lines.append("## ê²½ì œí•™ì  í•´ì„")
    
    # ì£¼ìš” í•´ì„ ì¶”ê°€
    template = TRANSMISSION_TEMPLATES.get((shock_source, list(affected_nodes.keys())[0] if affected_nodes else ""), {})
    if template.get("narrative"):
        lines.append(f"> {template['narrative']}")
    
    return "\n".join(lines)


# ============================================================================
# Lead-Lag Analysis
# ============================================================================

class LeadLagAnalyzer:
    """
    Lead-Lag ê´€ê³„ ë¶„ì„

    Cross-correlation at multiple lagsë¥¼ í†µí•´
    ì–´ë–¤ ì‹œê³„ì—´ì´ ë‹¤ë¥¸ ì‹œê³„ì—´ì„ ì„ í–‰í•˜ëŠ”ì§€ íƒìƒ‰
    """

    def __init__(self, max_lag: int = 20):
        self.max_lag = max_lag

    def analyze(
        self,
        source: pd.Series,
        target: pd.Series,
        source_name: str = "source",
        target_name: str = "target"
    ) -> LeadLagResult:
        """
        Lead-Lag ë¶„ì„ ìˆ˜í–‰

        Args:
            source: ì†ŒìŠ¤ ì‹œê³„ì—´
            target: íƒ€ê²Ÿ ì‹œê³„ì—´

        Returns:
            LeadLagResult
        """
        # ë°ì´í„° ì •ë ¬ ë° ê²°ì¸¡ì¹˜ ì²˜ë¦¬
        df = pd.DataFrame({'source': source, 'target': target}).dropna()

        if len(df) < self.max_lag * 2:
            return LeadLagResult(
                source=source_name,
                target=target_name,
                optimal_lag=0,
                max_correlation=0.0,
                correlation_at_zero=df['source'].corr(df['target']),
                is_leading=False,
                confidence=0.0
            )

        correlations = {}

        for lag in range(-self.max_lag, self.max_lag + 1):
            if lag > 0:
                # sourceê°€ targetì„ lead (source[t] â†’ target[t+lag])
                corr = df['source'].iloc[:-lag].corr(df['target'].iloc[lag:])
            elif lag < 0:
                # targetì´ sourceë¥¼ lead
                corr = df['source'].iloc[-lag:].corr(df['target'].iloc[:lag])
            else:
                corr = df['source'].corr(df['target'])

            correlations[lag] = corr if not np.isnan(corr) else 0.0

        # ìµœëŒ€ ìƒê´€ê´€ê³„ lag ì°¾ê¸°
        optimal_lag = max(correlations, key=lambda k: abs(correlations[k]))
        max_corr = correlations[optimal_lag]
        zero_corr = correlations[0]

        # ì‹ ë¢°ë„: ìµœì  lagì˜ ìƒê´€ê´€ê³„ê°€ 0ë³´ë‹¤ ì–¼ë§ˆë‚˜ ë†’ì€ì§€
        confidence = abs(max_corr) - abs(zero_corr)
        confidence = max(0, min(1, confidence * 5))  # 0-1 ì •ê·œí™”

        return LeadLagResult(
            source=source_name,
            target=target_name,
            optimal_lag=optimal_lag,
            max_correlation=max_corr,
            correlation_at_zero=zero_corr,
            is_leading=(optimal_lag > 0 and max_corr > 0) or (optimal_lag < 0 and max_corr < 0),
            confidence=confidence
        )

    def analyze_matrix(self, data: pd.DataFrame) -> pd.DataFrame:
        """
        ëª¨ë“  ë³€ìˆ˜ ìŒì— ëŒ€í•œ Lead-Lag ë§¤íŠ¸ë¦­ìŠ¤ ìƒì„±

        Returns:
            DataFrame with optimal lags (ì–‘ìˆ˜ = rowê°€ columnì„ lead)
        """
        columns = data.columns.tolist()
        n = len(columns)

        lag_matrix = pd.DataFrame(
            np.zeros((n, n)),
            index=columns,
            columns=columns
        )

        for i, source in enumerate(columns):
            for j, target in enumerate(columns):
                if i != j:
                    result = self.analyze(
                        data[source], data[target],
                        source, target
                    )
                    lag_matrix.loc[source, target] = result.optimal_lag

        return lag_matrix


# ============================================================================
# Granger Causality
# ============================================================================

class GrangerCausalityAnalyzer:
    """
    Granger Causality ê²€ì •

    "Xê°€ Yë¥¼ Granger-cause í•œë‹¤" =
    "Xì˜ ê³¼ê±° ì •ë³´ê°€ Yì˜ ì˜ˆì¸¡ì— í†µê³„ì ìœ¼ë¡œ ìœ ì˜í•œ ê¸°ì—¬ë¥¼ í•œë‹¤"

    ì£¼ì˜: Granger Causality â‰  True Causality
    í•˜ì§€ë§Œ ì˜ˆì¸¡ì  ê´€ê³„ì˜ ì¢‹ì€ í”„ë¡ì‹œ
    """

    def __init__(self, max_lag: int = 10, significance_level: float = 0.05):
        self.max_lag = max_lag
        self.significance_level = significance_level

    def test(
        self,
        source: pd.Series,
        target: pd.Series,
        source_name: str = "source",
        target_name: str = "target"
    ) -> GrangerResult:
        """
        Granger Causality ê²€ì •

        H0: source does not Granger-cause target
        H1: source Granger-causes target
        """
        # ë°ì´í„° ì¤€ë¹„
        df = pd.DataFrame({'target': target, 'source': source}).dropna()

        if len(df) < self.max_lag * 3:
            return GrangerResult(
                source=source_name,
                target=target_name,
                optimal_lag=0,
                f_statistic=0.0,
                p_value=1.0,
                strength=CausalityStrength.NONE,
                is_significant=False
            )

        if not STATSMODELS_AVAILABLE:
            # Simplified fallback using correlation
            return self._simplified_test(df, source_name, target_name)

        try:
            # Granger Causality ê²€ì •
            result = grangercausalitytests(
                df[['target', 'source']],
                maxlag=self.max_lag,
                verbose=False
            )

            # ìµœì  lag ë° p-value ì¶”ì¶œ
            best_lag = 1
            best_pvalue = 1.0
            best_fstat = 0.0

            for lag in range(1, self.max_lag + 1):
                if lag in result:
                    # F-test ê²°ê³¼ ì‚¬ìš©
                    ftest = result[lag][0]['ssr_ftest']
                    pvalue = ftest[1]
                    fstat = ftest[0]

                    if pvalue < best_pvalue:
                        best_pvalue = pvalue
                        best_lag = lag
                        best_fstat = fstat

            # ê°•ë„ íŒì •
            if best_pvalue < 0.01:
                strength = CausalityStrength.STRONG
            elif best_pvalue < 0.05:
                strength = CausalityStrength.MODERATE
            elif best_pvalue < 0.10:
                strength = CausalityStrength.WEAK
            else:
                strength = CausalityStrength.NONE

            return GrangerResult(
                source=source_name,
                target=target_name,
                optimal_lag=best_lag,
                f_statistic=best_fstat,
                p_value=best_pvalue,
                strength=strength,
                is_significant=(best_pvalue < self.significance_level)
            )

        except Exception as e:
            # ì—ëŸ¬ ì‹œ fallback
            return self._simplified_test(df, source_name, target_name)

    def _simplified_test(
        self,
        df: pd.DataFrame,
        source_name: str,
        target_name: str
    ) -> GrangerResult:
        """Simplified causality test using lagged correlation"""
        lead_lag = LeadLagAnalyzer(max_lag=self.max_lag)
        result = lead_lag.analyze(
            df['source'], df['target'],
            source_name, target_name
        )

        # ìƒê´€ê´€ê³„ ê¸°ë°˜ ìœ ì‚¬ p-value ê³„ì‚°
        n = len(df)
        r = result.max_correlation
        if abs(r) > 0:
            t_stat = r * np.sqrt((n - 2) / (1 - r**2 + 1e-10))
            # ê·¼ì‚¬ p-value
            p_value = 2 * (1 - stats.t.cdf(abs(t_stat), n - 2)) if SCIPY_AVAILABLE else 0.05
        else:
            t_stat = 0
            p_value = 1.0

        if p_value < 0.01:
            strength = CausalityStrength.STRONG
        elif p_value < 0.05:
            strength = CausalityStrength.MODERATE
        elif p_value < 0.10:
            strength = CausalityStrength.WEAK
        else:
            strength = CausalityStrength.NONE

        return GrangerResult(
            source=source_name,
            target=target_name,
            optimal_lag=abs(result.optimal_lag),
            f_statistic=t_stat**2,
            p_value=p_value,
            strength=strength,
            is_significant=(p_value < self.significance_level)
        )

    def test_bidirectional(
        self,
        series1: pd.Series,
        series2: pd.Series,
        name1: str,
        name2: str
    ) -> Tuple[GrangerResult, GrangerResult]:
        """ì–‘ë°©í–¥ Granger Causality ê²€ì •"""
        result_1to2 = self.test(series1, series2, name1, name2)
        result_2to1 = self.test(series2, series1, name2, name1)

        # ì–‘ë°©í–¥ ì—¬ë¶€ ì—…ë°ì´íŠ¸
        if result_1to2.is_significant and result_2to1.is_significant:
            result_1to2.bidirectional = True
            result_2to1.bidirectional = True

        return result_1to2, result_2to1


# ============================================================================
# Shock Propagation Graph
# ============================================================================

class ShockPropagationGraph:
    """
    ì¶©ê²© ì „íŒŒ ê·¸ë˜í”„

    ê²½ì œ ì‹œìŠ¤í…œì—ì„œ ì¶©ê²©ì´ ì–´ë–»ê²Œ ì „íŒŒë˜ëŠ”ì§€ë¥¼
    ë°©í–¥ì„± ê·¸ë˜í”„(DAG)ë¡œ ëª¨ë¸ë§
    """

    def __init__(
        self,
        significance_level: float = 0.05,
        max_lag: int = 20,
        enforce_layer_order: bool = True
    ):
        self.significance_level = significance_level
        self.max_lag = max_lag
        self.enforce_layer_order = enforce_layer_order

        self.graph = nx.DiGraph()
        self.lead_lag_analyzer = LeadLagAnalyzer(max_lag)
        self.granger_analyzer = GrangerCausalityAnalyzer(max_lag, significance_level)

        # ê²°ê³¼ ì €ì¥
        self.lead_lag_results: Dict[Tuple[str, str], LeadLagResult] = {}
        self.granger_results: Dict[Tuple[str, str], GrangerResult] = {}

    def build_from_data(
        self,
        data: pd.DataFrame,
        min_observations: int = 60
    ) -> nx.DiGraph:
        """
        ì‹œê³„ì—´ ë°ì´í„°ì—ì„œ ì¶©ê²© ì „íŒŒ ê·¸ë˜í”„ êµ¬ì¶•

        Args:
            data: ì‹œê³„ì—´ DataFrame (columns = variables)
            min_observations: ìµœì†Œ ê´€ì¸¡ì¹˜ ìˆ˜

        Returns:
            NetworkX DiGraph
        """
        # ì¶©ë¶„í•œ ë°ì´í„°ê°€ ìˆëŠ” ë³€ìˆ˜ë§Œ ì„ íƒ
        valid_columns = [
            col for col in data.columns
            if data[col].dropna().shape[0] >= min_observations
        ]

        if len(valid_columns) < 2:
            print("[SPG] Not enough valid columns")
            return self.graph

        print(f"[SPG] Building graph with {len(valid_columns)} variables")

        # ë…¸ë“œ ì¶”ê°€
        for col in valid_columns:
            layer = get_node_layer(col)
            self.graph.add_node(
                col,
                layer=layer.value,
                layer_name=layer.name
            )

        # ì—£ì§€ ì¶”ê°€ (Granger Causality ê¸°ë°˜)
        n_edges = 0
        for i, source in enumerate(valid_columns):
            for j, target in enumerate(valid_columns):
                if i == j:
                    continue

                # Lead-Lag ë¶„ì„
                ll_result = self.lead_lag_analyzer.analyze(
                    data[source], data[target],
                    source, target
                )
                self.lead_lag_results[(source, target)] = ll_result

                # Granger Causality ê²€ì •
                gc_result = self.granger_analyzer.test(
                    data[source], data[target],
                    source, target
                )
                self.granger_results[(source, target)] = gc_result

                # ìœ ì˜ë¯¸í•œ ì¸ê³¼ê´€ê³„ë§Œ ì—£ì§€ë¡œ ì¶”ê°€
                if gc_result.is_significant:
                    # Layer ìˆœì„œ ê°•ì œ (ì„ íƒì )
                    source_layer = get_node_layer(source)
                    target_layer = get_node_layer(target)

                    if self.enforce_layer_order and source_layer.value > target_layer.value:
                        # ì—­ë°©í–¥ (í•˜ìœ„ ë ˆì´ì–´ â†’ ìƒìœ„ ë ˆì´ì–´)
                        # í”¼ë“œë°± ë£¨í”„ë¡œ í‘œì‹œ
                        edge_type = "feedback"
                    else:
                        edge_type = "propagation"

                    self.graph.add_edge(
                        source, target,
                        lag=gc_result.optimal_lag,
                        strength=gc_result.strength.value,
                        p_value=gc_result.p_value,
                        correlation=ll_result.max_correlation,
                        edge_type=edge_type
                    )
                    n_edges += 1

        print(f"[SPG] Added {n_edges} significant edges")
        return self.graph

    def find_critical_path(
        self,
        source: str,
        max_depth: int = 10
    ) -> Optional[ShockPath]:
        """
        íŠ¹ì • ì†ŒìŠ¤ì—ì„œ ì‹œì‘í•˜ëŠ” ìµœì¥ ì¶©ê²© ì „íŒŒ ê²½ë¡œ íƒìƒ‰

        Critical Path = ê°€ì¥ ê¸´ ì „íŒŒ ì²´ì¸
        """
        if source not in self.graph:
            return None

        def dfs(node: str, path: List[str], total_lag: int, visited: Set[str]) -> Tuple[List[str], int]:
            """DFSë¡œ ìµœì¥ ê²½ë¡œ íƒìƒ‰"""
            best_path = path
            best_lag = total_lag

            if len(path) >= max_depth:
                return best_path, best_lag

            for neighbor in self.graph.successors(node):
                if neighbor not in visited:
                    edge_data = self.graph.edges[node, neighbor]
                    new_lag = total_lag + edge_data.get('lag', 1)
                    new_path = path + [neighbor]

                    result_path, result_lag = dfs(
                        neighbor,
                        new_path,
                        new_lag,
                        visited | {neighbor}
                    )

                    if len(result_path) > len(best_path):
                        best_path = result_path
                        best_lag = result_lag

            return best_path, best_lag

        path, total_lag = dfs(source, [source], 0, {source})

        if len(path) <= 1:
            return None

        # ëˆ„ì  ì¶©ê²© ê°•ë„ ê³„ì‚°
        cumulative_impact = 1.0
        for i in range(len(path) - 1):
            edge_data = self.graph.edges[path[i], path[i+1]]
            corr = abs(edge_data.get('correlation', 0.5))
            cumulative_impact *= corr

        # ë³‘ëª© ë…¸ë“œ íƒìƒ‰ (betweenness ê°€ì¥ ë†’ì€ ë…¸ë“œ)
        betweenness = nx.betweenness_centrality(self.graph)
        path_nodes = path[1:-1]  # ì‹œì‘/ë ì œì™¸
        bottleneck = max(path_nodes, key=lambda n: betweenness.get(n, 0)) if path_nodes else None

        return ShockPath(
            source=source,
            path=path,
            total_lag=total_lag,
            cumulative_impact=cumulative_impact,
            bottleneck=bottleneck
        )

    def find_all_critical_paths(self, top_n: int = 5) -> List[ShockPath]:
        """ëª¨ë“  ì†ŒìŠ¤ì—ì„œ ì‹œì‘í•˜ëŠ” Critical Path íƒìƒ‰"""
        paths = []

        for node in self.graph.nodes:
            layer = get_node_layer(node)
            # Policy/Liquidity ë ˆì´ì–´ë§Œ ì†ŒìŠ¤ë¡œ
            if layer in [NodeLayer.POLICY, NodeLayer.LIQUIDITY]:
                path = self.find_critical_path(node)
                if path and len(path.path) > 2:
                    paths.append(path)

        # ê²½ë¡œ ê¸¸ì´ ê¸°ì¤€ ì •ë ¬
        paths.sort(key=lambda p: len(p.path), reverse=True)
        return paths[:top_n]

    def analyze_nodes(self) -> List[NodeAnalysis]:
        """ëª¨ë“  ë…¸ë“œ ë¶„ì„"""
        analyses = []

        betweenness = nx.betweenness_centrality(self.graph)

        for node in self.graph.nodes:
            in_deg = self.graph.in_degree(node)
            out_deg = self.graph.out_degree(node)

            # í‰ê·  ì„ í–‰ ì‹œê°„ ê³„ì‚°
            out_edges = self.graph.out_edges(node, data=True)
            avg_lead_time = np.mean([
                e[2].get('lag', 0) for e in out_edges
            ]) if out_edges else 0

            # ì—­í•  íŒì •
            leading_score = out_deg - in_deg
            if leading_score > 2:
                role = "LEADING"
            elif leading_score < -2:
                role = "LAGGING"
            elif betweenness.get(node, 0) > 0.1:
                role = "BRIDGE"
            elif in_deg == 0 and out_deg == 0:
                role = "ISOLATED"
            else:
                role = "NEUTRAL"

            analyses.append(NodeAnalysis(
                node=node,
                layer=get_node_layer(node),
                in_degree=in_deg,
                out_degree=out_deg,
                leading_score=leading_score,
                betweenness=betweenness.get(node, 0),
                avg_lead_time=avg_lead_time,
                role=role
            ))

        return analyses

    def get_leading_indicators(self, top_n: int = 5) -> List[str]:
        """ì„ í–‰ ì§€í‘œ ì¶”ì¶œ"""
        analyses = self.analyze_nodes()
        leading = [a for a in analyses if a.role == "LEADING"]
        leading.sort(key=lambda a: a.leading_score, reverse=True)
        return [a.node for a in leading[:top_n]]

    def get_lagging_indicators(self, top_n: int = 5) -> List[str]:
        """í›„í–‰ ì§€í‘œ ì¶”ì¶œ"""
        analyses = self.analyze_nodes()
        lagging = [a for a in analyses if a.role == "LAGGING"]
        lagging.sort(key=lambda a: a.leading_score)
        return [a.node for a in lagging[:top_n]]

    def get_bridge_nodes(self) -> List[str]:
        """ë¸Œë¦¿ì§€ ë…¸ë“œ (ì „íŒŒ ì¤‘ê°œì) ì¶”ì¶œ"""
        analyses = self.analyze_nodes()
        return [a.node for a in analyses if a.role == "BRIDGE"]

    def run_full_analysis(self, data: pd.DataFrame) -> PropagationAnalysis:
        """ì „ì²´ ë¶„ì„ ì‹¤í–‰"""
        print("[SPG] Running full analysis...")

        # ê·¸ë˜í”„ êµ¬ì¶•
        self.build_from_data(data)

        # ë…¸ë“œ ë¶„ì„
        node_analyses = self.analyze_nodes()

        # ì—£ì§€ ì •ë³´
        edges = []
        for u, v, d in self.graph.edges(data=True):
            edges.append({
                'source': u,
                'target': v,
                'lag': d.get('lag', 0),
                'strength': d.get('strength', 'none'),
                'p_value': d.get('p_value', 1.0),
                'correlation': d.get('correlation', 0),
                'edge_type': d.get('edge_type', 'unknown')
            })

        # Critical Paths
        critical_paths = self.find_all_critical_paths()

        return PropagationAnalysis(
            timestamp=datetime.now().isoformat(),
            nodes=node_analyses,
            edges=edges,
            critical_paths=critical_paths,
            leading_indicators=self.get_leading_indicators(),
            lagging_indicators=self.get_lagging_indicators(),
            bridge_nodes=self.get_bridge_nodes()
        )

    def to_adjacency_matrix(self) -> pd.DataFrame:
        """ì¸ì ‘ í–‰ë ¬ ë°˜í™˜ (lag ê°’)"""
        nodes = list(self.graph.nodes)
        n = len(nodes)

        matrix = pd.DataFrame(
            np.zeros((n, n)),
            index=nodes,
            columns=nodes
        )

        for u, v, d in self.graph.edges(data=True):
            matrix.loc[u, v] = d.get('lag', 1)

        return matrix

    def visualize_text(self) -> str:
        """í…ìŠ¤íŠ¸ ê¸°ë°˜ ì‹œê°í™”"""
        lines = ["=" * 60, "Shock Propagation Graph", "=" * 60, ""]

        # ë ˆì´ì–´ë³„ ë…¸ë“œ
        layers = {layer: [] for layer in NodeLayer}
        for node in self.graph.nodes:
            layer = get_node_layer(node)
            layers[layer].append(node)

        for layer in NodeLayer:
            if layers[layer]:
                lines.append(f"[{layer.name}]")
                lines.append(f"  {', '.join(layers[layer])}")
                lines.append("")

        # ì£¼ìš” ì—£ì§€
        lines.append("Key Edges (p < 0.05):")
        for u, v, d in self.graph.edges(data=True):
            if d.get('p_value', 1) < 0.05:
                lag = d.get('lag', 0)
                corr = d.get('correlation', 0)
                lines.append(f"  {u} --[lag={lag}, r={corr:.2f}]--> {v}")

        # Critical Paths
        lines.append("")
        lines.append("Critical Paths:")
        for path in self.find_all_critical_paths(3):
            path_str = " â†’ ".join(path.path)
            lines.append(f"  {path_str} (lag={path.total_lag}d)")

        return "\n".join(lines)


# ============================================================================
# Utility Functions
# ============================================================================

def create_sample_macro_data(n_days: int = 500) -> pd.DataFrame:
    """í…ŒìŠ¤íŠ¸ìš© ê±°ì‹œê²½ì œ ë°ì´í„° ìƒì„±"""
    np.random.seed(42)
    dates = pd.date_range(end=datetime.now(), periods=n_days, freq='D')

    # ê¸°ë³¸ ë…¸ì´ì¦ˆ
    noise = np.random.randn(n_days) * 0.01

    # Fed Funds (ê¸°ë³¸ ë“œë¼ì´ë²„)
    fed_funds = np.cumsum(np.random.randn(n_days) * 0.001) + 4.5

    # DXY (Fed Fundsì— 2ì¼ í›„í–‰)
    dxy = pd.Series(fed_funds).shift(2).fillna(method='bfill').values * 20 + 100 + noise * 2

    # VIX (DXYì— 3ì¼ í›„í–‰)
    vix = pd.Series(dxy).shift(3).fillna(method='bfill').values * 0.2 + 15 + np.abs(noise) * 10

    # SPY (VIXì— 1ì¼ í›„í–‰, ì—­ìƒê´€)
    spy = 500 - pd.Series(vix).shift(1).fillna(method='bfill').values * 2 + noise * 50

    # TLT (Fed Fundsì— 5ì¼ í›„í–‰)
    tlt = 100 - pd.Series(fed_funds).shift(5).fillna(method='bfill').values * 2 + noise * 20

    # GLD (VIXì™€ ì–‘ìƒê´€, 2ì¼ í›„í–‰)
    gld = pd.Series(vix).shift(2).fillna(method='bfill').values * 5 + 1800 + noise * 50

    return pd.DataFrame({
        'FED_FUNDS': fed_funds,
        'DXY': dxy,
        'VIX': vix,
        'SPY': spy,
        'TLT': tlt,
        'GLD': gld
    }, index=dates)


# ============================================================================
# CLI Test
# ============================================================================

if __name__ == "__main__":
    print("=" * 60)
    print("Shock Propagation Graph Test")
    print("=" * 60)

    # ìƒ˜í”Œ ë°ì´í„° ìƒì„±
    print("\n1. Generating sample macro data (500 days)...")
    data = create_sample_macro_data(500)
    print(f"   Variables: {data.columns.tolist()}")

    # SPG êµ¬ì¶•
    print("\n2. Building Shock Propagation Graph...")
    spg = ShockPropagationGraph(
        significance_level=0.05,
        max_lag=10,
        enforce_layer_order=True
    )

    analysis = spg.run_full_analysis(data)

    # ê²°ê³¼ ì¶œë ¥
    print("\n3. Results:")
    print(f"   Nodes: {len(analysis.nodes)}")
    print(f"   Edges: {len(analysis.edges)}")
    print(f"   Leading Indicators: {analysis.leading_indicators}")
    print(f"   Lagging Indicators: {analysis.lagging_indicators}")
    print(f"   Bridge Nodes: {analysis.bridge_nodes}")

    print("\n4. Critical Paths:")
    for path in analysis.critical_paths:
        path_str = " â†’ ".join(path.path)
        print(f"   {path_str}")
        print(f"      Total Lag: {path.total_lag} days")
        print(f"      Cumulative Impact: {path.cumulative_impact:.2%}")

    print("\n5. Node Roles:")
    for node in analysis.nodes:
        print(f"   {node.node}: {node.role} (in={node.in_degree}, out={node.out_degree})")

    print("\n" + spg.visualize_text())

    print("\n" + "=" * 60)
    print("Test completed successfully!")
