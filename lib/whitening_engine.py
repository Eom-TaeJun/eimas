"""
Whitening Engine - Economic Reverse Engineering
================================================
AI ê²°ê³¼ì— ëŒ€í•œ ê²½ì œí•™ì  í•´ì„ ë° ì—­ì¶”ì  ì‹œìŠ¤í…œ

í•µì‹¬ ì² í•™:
- "ì™œ ì´ ê²°ê³¼ê°€ ë‚˜ì™”ëŠ”ê°€?"ì— ë‹µí•  ìˆ˜ ì—†ìœ¼ë©´ ë¸”ë™ë°•ìŠ¤
- ê²°ê³¼ â†’ ì›ì¸ìœ¼ë¡œ ì—­ì¶”ì  (Reverse Engineering)
- ëª¨ë“  ì˜ì‚¬ê²°ì •ì— ê²½ì œí•™ì  ê·¼ê±° ë¶€ì—¬

ê¸°ëŠ¥:
1. Factor Attribution: ì–´ë–¤ íŒ©í„°ê°€ ê²°ê³¼ì— ê¸°ì—¬í–ˆëŠ”ê°€?
2. Causal Validation: ì¸ê³¼ê´€ê³„ ê²½ë¡œê°€ ì‹¤ì œë¡œ ì‘ë™í•˜ëŠ”ê°€?
3. Economic Narrative: ê²°ê³¼ë¥¼ ê²½ì œí•™ì ìœ¼ë¡œ ì„¤ëª…
4. Anomaly Explanation: ì´ìƒì¹˜ê°€ ì™œ ë°œìƒí–ˆëŠ”ê°€?
"""

import numpy as np
import pandas as pd
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass, field
from datetime import datetime
from enum import Enum
import json

# Local imports
try:
    from lib.shock_propagation_graph import (
        ShockPropagationGraph,
        NodeLayer,
        get_node_layer
    )
    SPG_AVAILABLE = True
except ImportError:
    SPG_AVAILABLE = False


class EconomicFactor(Enum):
    """ê²½ì œí•™ì  íŒ©í„°"""
    # Macro Factors
    INTEREST_RATE = "interest_rate"           # ê¸ˆë¦¬ (R)
    INFLATION = "inflation"                   # ì¸í”Œë ˆì´ì…˜ (Ï€)
    LIQUIDITY = "liquidity"                   # ìœ ë™ì„± (M)
    DOLLAR_STRENGTH = "dollar_strength"       # ë‹¬ëŸ¬ ê°•ì„¸ (DXY)

    # Risk Factors
    GEOPOLITICAL = "geopolitical"             # ì§€ì •í•™ì  ë¦¬ìŠ¤í¬
    CREDIT_RISK = "credit_risk"               # ì‹ ìš© ë¦¬ìŠ¤í¬
    VOLATILITY = "volatility"                 # ë³€ë™ì„±

    # Sector Factors
    TECH_MOMENTUM = "tech_momentum"           # ê¸°ìˆ ì£¼ ëª¨ë©˜í…€
    ENERGY_CYCLE = "energy_cycle"             # ì—ë„ˆì§€ ì‚¬ì´í´
    DEFENSIVE_ROTATION = "defensive_rotation" # ë°©ì–´ì£¼ ë¡œí…Œì´ì…˜

    # Flow Factors
    INSTITUTIONAL_FLOW = "institutional_flow" # ê¸°ê´€ ìê¸ˆ íë¦„
    RETAIL_SENTIMENT = "retail_sentiment"     # ê°œì¸ íˆ¬ì ì‹¬ë¦¬
    STABLECOIN_FLOW = "stablecoin_flow"       # ìŠ¤í…Œì´ë¸”ì½”ì¸ ìœ ì…


@dataclass
class FactorAttribution:
    """íŒ©í„° ê¸°ì—¬ë„ ë¶„ì„"""
    factor: EconomicFactor
    contribution: float          # ê¸°ì—¬ë„ (%)
    direction: str              # "POSITIVE", "NEGATIVE"
    confidence: float           # ì‹ ë¢°ë„
    evidence: List[str]         # ê·¼ê±°
    data_points: Dict[str, float] = field(default_factory=dict)


@dataclass
class CausalValidation:
    """ì¸ê³¼ê´€ê³„ ê²€ì¦ ê²°ê³¼"""
    hypothesis: str             # ê°€ì„¤
    path: List[str]             # ì¸ê³¼ ê²½ë¡œ
    is_valid: bool              # ê²€ì¦ ê²°ê³¼
    correlation: float          # ìƒê´€ê´€ê³„
    lag_days: int               # ì‹œì°¨
    p_value: float              # í†µê³„ì  ìœ ì˜ì„±
    counter_evidence: List[str] # ë°˜ì¦


@dataclass
class EconomicNarrative:
    """ê²½ì œí•™ì  ë‚´ëŸ¬í‹°ë¸Œ"""
    timestamp: str
    summary: str                # í•µì‹¬ ìš”ì•½
    key_drivers: List[FactorAttribution]
    causal_paths: List[CausalValidation]
    risk_factors: List[str]
    opportunities: List[str]
    confidence: float
    caveats: List[str]          # ì£¼ì˜ì‚¬í•­


class WhiteningEngine:
    """
    Whitening Engine: ê²°ê³¼ë¥¼ ê²½ì œí•™ì ìœ¼ë¡œ ì„¤ëª…

    "ë°”ì´ì˜¤ ì„¹í„° ë¹„ì¤‘ ì¦ê°€" â†’ "ì™œ?"
    â†’ "ê¸ˆë¦¬ ì¸í•˜ ê¸°ëŒ€ (Râ†“) + ì¥ìˆ˜(Longevity) í…Œë§ˆ ë¶€ìƒ"
    """

    # íŒ©í„°-ìì‚° ë§¤í•‘ (ê²½ì œí•™ì  ë„ë©”ì¸ ì§€ì‹)
    FACTOR_ASSET_MAPPING = {
        EconomicFactor.INTEREST_RATE: {
            'positive': ['TLT', 'XLU', 'XLRE'],  # ê¸ˆë¦¬ í•˜ë½ ìˆ˜í˜œ
            'negative': ['XLF', 'KRE'],           # ê¸ˆë¦¬ í•˜ë½ í”¼í•´
            'proxy': ['DGS10', 'DGS2', 'T10Y2Y']
        },
        EconomicFactor.INFLATION: {
            'positive': ['GLD', 'TIP', 'DBC'],    # ì¸í”Œë ˆ í—¤ì§€
            'negative': ['TLT', 'VCIT'],          # ì¸í”Œë ˆ í”¼í•´
            'proxy': ['CPIAUCSL', 'PCEPILFE']
        },
        EconomicFactor.LIQUIDITY: {
            'positive': ['BTC-USD', 'QQQ', 'ARKK'],  # ìœ ë™ì„± ìˆ˜í˜œ
            'negative': ['SHY'],                      # ìœ ë™ì„± ì¤‘ë¦½
            'proxy': ['RRP', 'TGA', 'M2', 'USDT_SUPPLY']
        },
        EconomicFactor.DOLLAR_STRENGTH: {
            'positive': ['UUP', 'DXY'],           # ë‹¬ëŸ¬ ê°•ì„¸ ìˆ˜í˜œ
            'negative': ['EEM', 'GLD', 'FXE'],    # ë‹¬ëŸ¬ ê°•ì„¸ í”¼í•´
            'proxy': ['DXY', 'DX-Y.NYB']
        },
        EconomicFactor.GEOPOLITICAL: {
            'positive': ['XAR', 'ITA', 'LMT', 'RTX'],  # ë°©ì‚°
            'negative': ['EEM', 'VWO'],                 # ì‹ í¥êµ­
            'proxy': ['VIX', 'GLD']
        },
        EconomicFactor.TECH_MOMENTUM: {
            'positive': ['QQQ', 'XLK', 'SMH', 'SOXX'],
            'negative': ['XLU', 'XLP'],
            'proxy': ['QQQ', 'ARKK']
        },
        EconomicFactor.STABLECOIN_FLOW: {
            'positive': ['BTC-USD', 'ETH-USD'],   # í¬ë¦½í† 
            'negative': [],
            'proxy': ['USDT_SUPPLY', 'USDC_SUPPLY']
        }
    }

    # í´ëŸ¬ìŠ¤í„°-í…Œë§ˆ ë§¤í•‘
    CLUSTER_THEME_MAPPING = {
        'tech': ['AI', 'ë°˜ë„ì²´', 'í´ë¼ìš°ë“œ', 'ì†Œí”„íŠ¸ì›¨ì–´'],
        'healthcare': ['ë°”ì´ì˜¤', 'ì œì•½', 'ì˜ë£Œê¸°ê¸°', 'Longevity'],
        'defense': ['ë°©ì‚°', 'ì‚¬ì´ë²„ë³´ì•ˆ', 'ìš°ì£¼í•­ê³µ'],
        'energy': ['ì„ìœ ', 'ì²œì—°ê°€ìŠ¤', 'ì‹ ì¬ìƒ', 'ì›ìë ¥'],
        'financial': ['ì€í–‰', 'ë³´í—˜', 'í•€í…Œí¬'],
        'consumer': ['ì†Œë¹„ì¬', 'ìœ í†µ', 'eì»¤ë¨¸ìŠ¤']
    }

    def __init__(self, macro_data: Optional[pd.DataFrame] = None):
        self.macro_data = macro_data
        self.spg = ShockPropagationGraph() if SPG_AVAILABLE else None

    def explain_allocation(
        self,
        weights: Dict[str, float],
        previous_weights: Optional[Dict[str, float]] = None,
        returns: Optional[pd.DataFrame] = None,
        macro_data: Optional[pd.DataFrame] = None
    ) -> EconomicNarrative:
        """
        í¬íŠ¸í´ë¦¬ì˜¤ ë°°ë¶„ ê²°ê³¼ë¥¼ ê²½ì œí•™ì ìœ¼ë¡œ ì„¤ëª…

        Args:
            weights: í˜„ì¬ ê°€ì¤‘ì¹˜
            previous_weights: ì´ì „ ê°€ì¤‘ì¹˜ (ë³€í™” ë¶„ì„ìš©)
            returns: ìˆ˜ìµë¥  ë°ì´í„°
            macro_data: ê±°ì‹œ ë°ì´í„°

        Returns:
            EconomicNarrative
        """
        if macro_data is not None:
            self.macro_data = macro_data

        # 1. ê°€ì¤‘ì¹˜ ë³€í™” ë¶„ì„
        weight_changes = self._analyze_weight_changes(weights, previous_weights)

        # 2. íŒ©í„° ê¸°ì—¬ë„ ë¶„ì„
        factor_attributions = self._attribute_factors(weights, weight_changes, returns)

        # 3. ì¸ê³¼ê´€ê³„ ê²€ì¦
        causal_validations = self._validate_causality(factor_attributions)

        # 4. ë¦¬ìŠ¤í¬ ìš”ì¸ ì‹ë³„
        risk_factors = self._identify_risks(weights, factor_attributions)

        # 5. ê¸°íšŒ ìš”ì¸ ì‹ë³„
        opportunities = self._identify_opportunities(weights, factor_attributions)

        # 6. ë‚´ëŸ¬í‹°ë¸Œ ìƒì„±
        summary = self._generate_summary(factor_attributions, weight_changes)

        # 7. ì£¼ì˜ì‚¬í•­
        caveats = self._generate_caveats(factor_attributions, causal_validations)

        # ì‹ ë¢°ë„ ê³„ì‚°
        confidence = self._calculate_confidence(factor_attributions, causal_validations)

        return EconomicNarrative(
            timestamp=datetime.now().isoformat(),
            summary=summary,
            key_drivers=factor_attributions[:5],  # ìƒìœ„ 5ê°œ
            causal_paths=causal_validations,
            risk_factors=risk_factors,
            opportunities=opportunities,
            confidence=confidence,
            caveats=caveats
        )

    def _analyze_weight_changes(
        self,
        weights: Dict[str, float],
        previous_weights: Optional[Dict[str, float]]
    ) -> Dict[str, Dict]:
        """ê°€ì¤‘ì¹˜ ë³€í™” ë¶„ì„"""
        changes = {}

        for asset, weight in weights.items():
            prev = previous_weights.get(asset, 0) if previous_weights else 0
            change = weight - prev

            if abs(change) > 0.005:  # 0.5% ì´ìƒ ë³€í™”ë§Œ
                changes[asset] = {
                    'current': weight,
                    'previous': prev,
                    'change': change,
                    'direction': 'INCREASE' if change > 0 else 'DECREASE',
                    'magnitude': abs(change)
                }

        return changes

    def _attribute_factors(
        self,
        weights: Dict[str, float],
        weight_changes: Dict[str, Dict],
        returns: Optional[pd.DataFrame]
    ) -> List[FactorAttribution]:
        """íŒ©í„° ê¸°ì—¬ë„ ë¶„ì„"""
        attributions = []

        for factor, mapping in self.FACTOR_ASSET_MAPPING.items():
            positive_assets = mapping.get('positive', [])
            negative_assets = mapping.get('negative', [])
            proxy_assets = mapping.get('proxy', [])

            # í•´ë‹¹ íŒ©í„°ì™€ ê´€ë ¨ëœ ìì‚°ì˜ ê°€ì¤‘ì¹˜ í•©ê³„
            positive_weight = sum(weights.get(a, 0) for a in positive_assets)
            negative_weight = sum(weights.get(a, 0) for a in negative_assets)

            # ìˆœ ë…¸ì¶œë„
            net_exposure = positive_weight - negative_weight

            if abs(net_exposure) < 0.01:
                continue

            # ë°©í–¥ ê²°ì •
            direction = "POSITIVE" if net_exposure > 0 else "NEGATIVE"

            # ê·¼ê±° ìˆ˜ì§‘
            evidence = []
            for asset in positive_assets:
                if asset in weights and weights[asset] > 0.01:
                    evidence.append(f"{asset}: {weights[asset]:.1%}")

            # ê±°ì‹œ ë°ì´í„°ì—ì„œ í”„ë¡ì‹œ ê°’ ì¶”ì¶œ
            data_points = {}
            if self.macro_data is not None:
                for proxy in proxy_assets:
                    if proxy in self.macro_data.columns:
                        recent = self.macro_data[proxy].dropna()
                        if len(recent) > 0:
                            data_points[proxy] = recent.iloc[-1]

            # ì‹ ë¢°ë„ ê³„ì‚°
            confidence = min(0.9, abs(net_exposure) * 5 + len(evidence) * 0.1)

            attributions.append(FactorAttribution(
                factor=factor,
                contribution=net_exposure * 100,
                direction=direction,
                confidence=confidence,
                evidence=evidence,
                data_points=data_points
            ))

        # ê¸°ì—¬ë„ ìˆœ ì •ë ¬
        attributions.sort(key=lambda x: abs(x.contribution), reverse=True)
        return attributions

    def _validate_causality(
        self,
        factor_attributions: List[FactorAttribution]
    ) -> List[CausalValidation]:
        """ì¸ê³¼ê´€ê³„ ê²€ì¦"""
        validations = []

        # ì£¼ìš” ì¸ê³¼ê´€ê³„ ê°€ì„¤
        hypotheses = [
            {
                'hypothesis': 'ê¸ˆë¦¬ ì¸í•˜ ê¸°ëŒ€ â†’ ì„±ì¥ì£¼ ì„ í˜¸',
                'path': ['FED_FUNDS', 'DGS10', 'QQQ'],
                'factor': EconomicFactor.INTEREST_RATE
            },
            {
                'hypothesis': 'ìœ ë™ì„± ì¦ê°€ â†’ ìœ„í—˜ìì‚° ì„ í˜¸',
                'path': ['M2', 'NET_LIQUIDITY', 'SPY', 'BTC-USD'],
                'factor': EconomicFactor.LIQUIDITY
            },
            {
                'hypothesis': 'ë‹¬ëŸ¬ ì•½ì„¸ â†’ ì‹ í¥ì‹œì¥/ê¸ˆ ê°•ì„¸',
                'path': ['DXY', 'EEM', 'GLD'],
                'factor': EconomicFactor.DOLLAR_STRENGTH
            },
            {
                'hypothesis': 'ìŠ¤í…Œì´ë¸”ì½”ì¸ ìœ ì… â†’ í¬ë¦½í†  ê°•ì„¸',
                'path': ['USDT_SUPPLY', 'BTC-USD'],
                'factor': EconomicFactor.STABLECOIN_FLOW
            },
            {
                'hypothesis': 'ì§€ì •í•™ì  ê¸´ì¥ â†’ ë°©ì‚°/ê¸ˆ ê°•ì„¸',
                'path': ['VIX', 'GLD', 'XAR'],
                'factor': EconomicFactor.GEOPOLITICAL
            }
        ]

        for hyp in hypotheses:
            # í•´ë‹¹ íŒ©í„°ê°€ í™œì„±í™”ë˜ì—ˆëŠ”ì§€ í™•ì¸
            relevant_attr = next(
                (a for a in factor_attributions if a.factor == hyp['factor']),
                None
            )

            if relevant_attr is None or abs(relevant_attr.contribution) < 1:
                continue

            # ì¸ê³¼ê´€ê³„ ê²€ì¦ (ê±°ì‹œ ë°ì´í„° ìˆì„ ë•Œ)
            is_valid = False
            correlation = 0.0
            lag_days = 0
            p_value = 1.0
            counter_evidence = []

            if self.macro_data is not None and self.spg is not None:
                # SPGë¥¼ í†µí•œ ì¸ê³¼ê´€ê³„ ê²€ì¦
                path = hyp['path']
                available_path = [p for p in path if p in self.macro_data.columns]

                if len(available_path) >= 2:
                    # Granger causality ê²€ì¦
                    source = self.macro_data[available_path[0]].dropna()
                    target = self.macro_data[available_path[-1]].dropna()

                    if len(source) > 30 and len(target) > 30:
                        # ê°„ë‹¨í•œ ìƒê´€ê´€ê³„ ê²€ì¦
                        aligned = pd.concat([source, target], axis=1).dropna()
                        if len(aligned) > 20:
                            correlation = aligned.iloc[:, 0].corr(aligned.iloc[:, 1])
                            is_valid = abs(correlation) > 0.3
                            p_value = 0.05 if is_valid else 0.5

            # ë°˜ì¦ ìˆ˜ì§‘
            if not is_valid:
                counter_evidence.append("ìƒê´€ê´€ê³„ê°€ ì•½í•˜ê±°ë‚˜ ë°ì´í„° ë¶€ì¡±")

            validations.append(CausalValidation(
                hypothesis=hyp['hypothesis'],
                path=hyp['path'],
                is_valid=is_valid,
                correlation=correlation,
                lag_days=lag_days,
                p_value=p_value,
                counter_evidence=counter_evidence
            ))

        return validations

    def _identify_risks(
        self,
        weights: Dict[str, float],
        factor_attributions: List[FactorAttribution]
    ) -> List[str]:
        """ë¦¬ìŠ¤í¬ ìš”ì¸ ì‹ë³„"""
        risks = []

        # ì§‘ì¤‘ë„ ë¦¬ìŠ¤í¬
        top_5_weight = sum(sorted(weights.values(), reverse=True)[:5])
        if top_5_weight > 0.4:
            risks.append(f"ìƒìœ„ 5ê°œ ìì‚° ì§‘ì¤‘ë„ {top_5_weight:.1%} - ë¶„ì‚° ë¶€ì¡±")

        # íŒ©í„° ì ë¦¼ ë¦¬ìŠ¤í¬
        for attr in factor_attributions[:3]:
            if abs(attr.contribution) > 15:
                risks.append(
                    f"{attr.factor.value} íŒ©í„° ë…¸ì¶œ {attr.contribution:.1f}% - "
                    f"{'ê³¼ë„í•œ ë² íŒ…' if attr.direction == 'POSITIVE' else 'ì—­ë°©í–¥ ë¦¬ìŠ¤í¬'}"
                )

        # ìœ ë™ì„± ë¦¬ìŠ¤í¬
        if any(a.factor == EconomicFactor.LIQUIDITY and a.direction == "NEGATIVE"
               for a in factor_attributions):
            risks.append("ìœ ë™ì„± ì¶•ì†Œ í™˜ê²½ì—ì„œ ìœ„í—˜ìì‚° ë…¸ì¶œ")

        # ê¸ˆë¦¬ ë¦¬ìŠ¤í¬
        rate_attr = next(
            (a for a in factor_attributions if a.factor == EconomicFactor.INTEREST_RATE),
            None
        )
        if rate_attr and rate_attr.direction == "POSITIVE" and rate_attr.contribution > 10:
            risks.append("ê¸ˆë¦¬ ì¸ìƒ ì‹œ ë“€ë ˆì´ì…˜ ë¦¬ìŠ¤í¬ ë…¸ì¶œ")

        return risks

    def _identify_opportunities(
        self,
        weights: Dict[str, float],
        factor_attributions: List[FactorAttribution]
    ) -> List[str]:
        """ê¸°íšŒ ìš”ì¸ ì‹ë³„"""
        opportunities = []

        for attr in factor_attributions[:3]:
            if attr.direction == "POSITIVE" and attr.confidence > 0.6:
                if attr.factor == EconomicFactor.LIQUIDITY:
                    opportunities.append(
                        "ìœ ë™ì„± í™•ì¥ í™˜ê²½ - ìœ„í—˜ìì‚° ì¶”ê°€ ë¹„ì¤‘ í™•ëŒ€ ê¸°íšŒ"
                    )
                elif attr.factor == EconomicFactor.TECH_MOMENTUM:
                    opportunities.append(
                        "ê¸°ìˆ ì£¼ ëª¨ë©˜í…€ ì§€ì† - AI/ë°˜ë„ì²´ ìµìŠ¤í¬ì € ìœ íš¨"
                    )
                elif attr.factor == EconomicFactor.STABLECOIN_FLOW:
                    opportunities.append(
                        "ìŠ¤í…Œì´ë¸”ì½”ì¸ ìœ ì… ì¦ê°€ - í¬ë¦½í†  ì¶”ê°€ ë¹„ì¤‘ ê³ ë ¤"
                    )
                elif attr.factor == EconomicFactor.GEOPOLITICAL:
                    opportunities.append(
                        "ì§€ì •í•™ì  ê¸´ì¥ ì§€ì† - ë°©ì‚°/ê¸ˆ í—¤ì§€ íš¨ê³¼ ê¸°ëŒ€"
                    )

        return opportunities

    def _generate_summary(
        self,
        factor_attributions: List[FactorAttribution],
        weight_changes: Dict[str, Dict]
    ) -> str:
        """í•µì‹¬ ìš”ì•½ ìƒì„±"""
        if not factor_attributions:
            return "íŒ©í„° ê¸°ì—¬ë„ ë¶„ì„ ë¶ˆê°€ - ë°ì´í„° ë¶€ì¡±"

        top_factor = factor_attributions[0]

        # ì£¼ìš” ë³€í™” ìì‚°
        major_changes = sorted(
            weight_changes.items(),
            key=lambda x: abs(x[1]['change']),
            reverse=True
        )[:3]

        change_summary = ""
        if major_changes:
            changes = [f"{a}({c['direction'][:1]}{abs(c['change']):.1%})"
                      for a, c in major_changes]
            change_summary = f" ì£¼ìš” ë³€í™”: {', '.join(changes)}."

        return (
            f"í¬íŠ¸í´ë¦¬ì˜¤ëŠ” {top_factor.factor.value} íŒ©í„°ì— "
            f"{abs(top_factor.contribution):.1f}% {top_factor.direction} ë…¸ì¶œ."
            f"{change_summary}"
        )

    def _generate_caveats(
        self,
        factor_attributions: List[FactorAttribution],
        causal_validations: List[CausalValidation]
    ) -> List[str]:
        """ì£¼ì˜ì‚¬í•­ ìƒì„±"""
        caveats = []

        # ë‚®ì€ ì‹ ë¢°ë„ íŒ©í„°
        low_conf = [a for a in factor_attributions if a.confidence < 0.5]
        if low_conf:
            caveats.append(
                f"{len(low_conf)}ê°œ íŒ©í„°ì˜ ì‹ ë¢°ë„ê°€ ë‚®ìŒ - ì¶”ê°€ ê²€ì¦ í•„ìš”"
            )

        # ê²€ì¦ ì‹¤íŒ¨ ì¸ê³¼ê´€ê³„
        invalid_causal = [c for c in causal_validations if not c.is_valid]
        if invalid_causal:
            caveats.append(
                f"{len(invalid_causal)}ê°œ ì¸ê³¼ê´€ê³„ ê°€ì„¤ì´ ë°ì´í„°ë¡œ ê²€ì¦ë˜ì§€ ì•ŠìŒ"
            )

        # ë°ì´í„° ë¶€ì¡±
        if self.macro_data is None or len(self.macro_data) < 60:
            caveats.append("ê±°ì‹œ ë°ì´í„° ë¶€ì¡± - íŒ©í„° ë¶„ì„ì˜ ì‹ ë¢°ë„ ì œí•œ")

        return caveats

    def _calculate_confidence(
        self,
        factor_attributions: List[FactorAttribution],
        causal_validations: List[CausalValidation]
    ) -> float:
        """ì „ì²´ ì‹ ë¢°ë„ ê³„ì‚°"""
        if not factor_attributions:
            return 0.3

        # íŒ©í„° ì‹ ë¢°ë„ í‰ê· 
        factor_conf = np.mean([a.confidence for a in factor_attributions])

        # ì¸ê³¼ê´€ê³„ ê²€ì¦ë¥ 
        if causal_validations:
            valid_rate = sum(1 for c in causal_validations if c.is_valid) / len(causal_validations)
        else:
            valid_rate = 0.5

        return (factor_conf * 0.6 + valid_rate * 0.4)

    def explain_cluster(
        self,
        cluster_id: int,
        cluster_assets: List[str],
        weights: Dict[str, float]
    ) -> Dict[str, Any]:
        """
        í´ëŸ¬ìŠ¤í„°ë¥¼ ê²½ì œí•™ì ìœ¼ë¡œ ì„¤ëª…

        "ì™œ ì´ ìì‚°ë“¤ì´ ë¬¶ì˜€ëŠ”ê°€?"
        """
        # í´ëŸ¬ìŠ¤í„° ë‚´ ê°€ì¤‘ì¹˜ í•©ê³„
        cluster_weight = sum(weights.get(a, 0) for a in cluster_assets)

        # í…Œë§ˆ ì¶”ë¡ 
        themes = []
        for theme, keywords in self.CLUSTER_THEME_MAPPING.items():
            # ìì‚°ëª…ì—ì„œ í…Œë§ˆ í‚¤ì›Œë“œ ë§¤ì¹­ (ê°„ë‹¨í™”ëœ ë²„ì „)
            matches = sum(1 for a in cluster_assets
                         if any(k.lower() in a.lower() for k in keywords))
            if matches > 0:
                themes.append(theme)

        # íŒ©í„° ë…¸ì¶œë„
        factor_exposures = {}
        for factor, mapping in self.FACTOR_ASSET_MAPPING.items():
            positive = [a for a in cluster_assets if a in mapping.get('positive', [])]
            negative = [a for a in cluster_assets if a in mapping.get('negative', [])]
            if positive or negative:
                factor_exposures[factor.value] = {
                    'positive': positive,
                    'negative': negative,
                    'net_count': len(positive) - len(negative)
                }

        return {
            'cluster_id': cluster_id,
            'assets': cluster_assets,
            'total_weight': cluster_weight,
            'inferred_themes': themes if themes else ['General'],
            'factor_exposures': factor_exposures,
            'interpretation': self._interpret_cluster(themes, factor_exposures, cluster_weight)
        }

    def _interpret_cluster(
        self,
        themes: List[str],
        factor_exposures: Dict,
        weight: float
    ) -> str:
        """í´ëŸ¬ìŠ¤í„° í•´ì„"""
        if not themes:
            return f"ë²”ìš© í´ëŸ¬ìŠ¤í„° (ë¹„ì¤‘: {weight:.1%})"

        theme_str = '/'.join(themes)

        # ì£¼ìš” íŒ©í„°
        if factor_exposures:
            top_factor = max(
                factor_exposures.items(),
                key=lambda x: abs(x[1]['net_count'])
            )
            return (
                f"{theme_str} í…Œë§ˆ í´ëŸ¬ìŠ¤í„° (ë¹„ì¤‘: {weight:.1%}). "
                f"ì£¼ìš” íŒ©í„°: {top_factor[0]}"
            )

        return f"{theme_str} í…Œë§ˆ í´ëŸ¬ìŠ¤í„° (ë¹„ì¤‘: {weight:.1%})"

    def reverse_engineer(
        self,
        observation: str,
        weights: Dict[str, float],
        macro_data: Optional[pd.DataFrame] = None
    ) -> Dict[str, Any]:
        """
        ê´€ì°°ëœ í˜„ìƒì„ ì—­ì¶”ì 

        ì˜ˆ: "ë°”ì´ì˜¤ ì„¹í„° ë¹„ì¤‘ì´ ë†’ë‹¤" â†’ "ì™œ?"

        Args:
            observation: ê´€ì°°ëœ í˜„ìƒ (í…ìŠ¤íŠ¸)
            weights: í¬íŠ¸í´ë¦¬ì˜¤ ê°€ì¤‘ì¹˜
            macro_data: ê±°ì‹œ ë°ì´í„°

        Returns:
            ì—­ì¶”ì  ë¶„ì„ ê²°ê³¼
        """
        if macro_data is not None:
            self.macro_data = macro_data

        # ê´€ì°° íŒŒì‹± (í‚¤ì›Œë“œ ê¸°ë°˜)
        keywords = observation.lower().split()

        # ê´€ë ¨ íŒ©í„° ì‹ë³„
        related_factors = []
        for factor, mapping in self.FACTOR_ASSET_MAPPING.items():
            all_assets = mapping.get('positive', []) + mapping.get('negative', [])
            if any(k in ' '.join(all_assets).lower() for k in keywords):
                related_factors.append(factor)

        # ê°€ëŠ¥í•œ ì›ì¸ ëª©ë¡
        possible_causes = []

        for factor in related_factors:
            mapping = self.FACTOR_ASSET_MAPPING[factor]
            proxy_assets = mapping.get('proxy', [])

            # ê±°ì‹œ ë°ì´í„°ì—ì„œ ìµœê·¼ íŠ¸ë Œë“œ í™•ì¸
            trends = {}
            if self.macro_data is not None:
                for proxy in proxy_assets:
                    if proxy in self.macro_data.columns:
                        series = self.macro_data[proxy].dropna()
                        if len(series) >= 20:
                            recent_change = (series.iloc[-1] / series.iloc[-20] - 1) * 100
                            trends[proxy] = recent_change

            possible_causes.append({
                'factor': factor.value,
                'explanation': self._factor_to_explanation(factor),
                'supporting_data': trends,
                'confidence': 0.7 if trends else 0.4
            })

        return {
            'observation': observation,
            'timestamp': datetime.now().isoformat(),
            'possible_causes': possible_causes,
            'recommendation': self._generate_recommendation(possible_causes)
        }

    def _factor_to_explanation(self, factor: EconomicFactor) -> str:
        """íŒ©í„°ë¥¼ ê²½ì œí•™ì  ì„¤ëª…ìœ¼ë¡œ ë³€í™˜"""
        explanations = {
            EconomicFactor.INTEREST_RATE: "ê¸ˆë¦¬ ë³€í™”ë¡œ ì¸í•œ í• ì¸ìœ¨ íš¨ê³¼ (Râ†“ â†’ Growthâ†‘)",
            EconomicFactor.INFLATION: "ì¸í”Œë ˆì´ì…˜ í—¤ì§€ ìˆ˜ìš” ë˜ëŠ” ì‹¤ì§ˆìˆ˜ìµë¥  ë³€í™”",
            EconomicFactor.LIQUIDITY: "ìœ ë™ì„± í™˜ê²½ ë³€í™” (Mâ†‘ â†’ Risk Assetâ†‘)",
            EconomicFactor.DOLLAR_STRENGTH: "ë‹¬ëŸ¬ ê°•/ì•½ì„¸ì— ë”°ë¥¸ ìƒëŒ€ ê°€ì¹˜ ë³€í™”",
            EconomicFactor.GEOPOLITICAL: "ì§€ì •í•™ì  ë¦¬ìŠ¤í¬ì— ë”°ë¥¸ ì•ˆì „ìì‚° ì„ í˜¸",
            EconomicFactor.TECH_MOMENTUM: "ê¸°ìˆ ì£¼ ëª¨ë©˜í…€ ë° AI íˆ¬ì ì‚¬ì´í´",
            EconomicFactor.STABLECOIN_FLOW: "ìŠ¤í…Œì´ë¸”ì½”ì¸ ìœ ì…ì— ë”°ë¥¸ í¬ë¦½í†  ìœ ë™ì„±"
        }
        return explanations.get(factor, "ì•Œ ìˆ˜ ì—†ëŠ” íŒ©í„°")

    def _generate_recommendation(self, possible_causes: List[Dict]) -> str:
        """ê¶Œê³ ì‚¬í•­ ìƒì„±"""
        if not possible_causes:
            return "ì¶”ê°€ ë°ì´í„° í•„ìš”"

        top_cause = max(possible_causes, key=lambda x: x['confidence'])

        if top_cause['confidence'] > 0.6:
            return f"ì£¼ìš” ì›ì¸: {top_cause['factor']}. {top_cause['explanation']}"
        else:
            return "ë³µí•©ì  ìš”ì¸ìœ¼ë¡œ íŒë‹¨ë¨. ì¶”ê°€ ë¶„ì„ í•„ìš”."


# ============================================================================
# CLI Test
# ============================================================================

if __name__ == "__main__":
    print("=" * 60)
    print("Whitening Engine Test")
    print("=" * 60)

    # ìƒ˜í”Œ ë°ì´í„°
    weights = {
        'QQQ': 0.15, 'SMH': 0.10, 'SOXX': 0.08,  # Tech
        'XLV': 0.12, 'IBB': 0.08,                 # Healthcare
        'XAR': 0.05, 'ITA': 0.05,                 # Defense
        'GLD': 0.10, 'TLT': 0.07,                 # Safe haven
        'BTC-USD': 0.05,                          # Crypto
        'SPY': 0.15                               # Broad market
    }

    previous_weights = {
        'QQQ': 0.12, 'SMH': 0.08, 'SOXX': 0.05,
        'XLV': 0.10, 'IBB': 0.05,
        'XAR': 0.03, 'ITA': 0.03,
        'GLD': 0.15, 'TLT': 0.12,
        'BTC-USD': 0.02,
        'SPY': 0.25
    }

    # Whitening Engine ì‹¤í–‰
    engine = WhiteningEngine()
    narrative = engine.explain_allocation(weights, previous_weights)

    print("\n1. Summary:")
    print(f"   {narrative.summary}")

    print("\n2. Key Drivers:")
    for driver in narrative.key_drivers:
        print(f"   [{driver.factor.value}] {driver.contribution:.1f}% {driver.direction}")
        print(f"      Evidence: {', '.join(driver.evidence[:3])}")
        print(f"      Confidence: {driver.confidence:.0%}")

    print("\n3. Causal Validations:")
    for cv in narrative.causal_paths:
        status = "âœ“" if cv.is_valid else "âœ—"
        print(f"   {status} {cv.hypothesis}")
        print(f"      Path: {' â†’ '.join(cv.path)}")

    print("\n4. Risks:")
    for risk in narrative.risk_factors:
        print(f"   âš ï¸ {risk}")

    print("\n5. Opportunities:")
    for opp in narrative.opportunities:
        print(f"   âœ¨ {opp}")

    print("\n6. Caveats:")
    for caveat in narrative.caveats:
        print(f"   ğŸ“ {caveat}")

    print(f"\n7. Overall Confidence: {narrative.confidence:.0%}")

    # Reverse Engineering í…ŒìŠ¤íŠ¸
    print("\n" + "=" * 60)
    print("Reverse Engineering Test")
    print("=" * 60)

    result = engine.reverse_engineer(
        "ê¸°ìˆ ì£¼ì™€ ë°˜ë„ì²´ ë¹„ì¤‘ì´ ë†’ë‹¤",
        weights
    )

    print(f"\nObservation: {result['observation']}")
    print("\nPossible Causes:")
    for cause in result['possible_causes']:
        print(f"   â€¢ {cause['factor']}: {cause['explanation']}")
        print(f"     Confidence: {cause['confidence']:.0%}")

    print(f"\nRecommendation: {result['recommendation']}")

    print("\n" + "=" * 60)
    print("Test completed!")
