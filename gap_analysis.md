# EIMAS GAP Analysis Report

> ê¸°ì¡´ EIMAS ì‹œìŠ¤í…œ vs. DOCX íŒŒì¼ ë¶„ì„(todolist.md) ë¹„êµ ë¶„ì„
>
> ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸: 2026-01-24

---

## ğŸ“Š Executive Summary

**ì „ì²´ êµ¬í˜„ë„: 52%** (100ì  ë§Œì )

| ìƒíƒœ | ë¹„ìœ¨ | ì„¤ëª… |
|------|------|------|
| âœ… ì™„ì „ êµ¬í˜„ | 52% | ê·¸ëŒ€ë¡œ ì‚¬ìš© ê°€ëŠ¥ |
| âš ï¸ ë¶€ë¶„ êµ¬í˜„ | 20% | ë³´ì™„ í•„ìš” |
| âŒ ë¯¸êµ¬í˜„ | 28% | ì‹ ê·œ ìƒì„± í•„ìš” |

**í•µì‹¬ ë°œê²¬ì‚¬í•­:**
- ğŸŸ¢ **ê°•ì **: ê²½ì œí•™ í†µí•© (Whitening, ì¸ê³¼ê´€ê³„, íŒ©íŠ¸ì²´í‚¹) - 95% ì™„ì„±ë„
- ğŸŸ¡ **ë³´ì™„ í•„ìš”**: HFT ë¯¸ì„¸êµ¬ì¡°, HRP ê³ ë„í™” - 40-70% ì™„ì„±ë„
- ğŸ”´ **ì‹ ê·œ í•„ìš”**: ë¸”ë¡ì²´ì¸ PoI, CNN íŒ¨í„´ íƒì§€ - 0% ì™„ì„±ë„

---

## ğŸ¯ ì¹´í…Œê³ ë¦¬ë³„ ìƒì„¸ ë¶„ì„

### 1. í¬íŠ¸í´ë¦¬ì˜¤ ìµœì í™” ë° ìì‚° ë°°ë¶„ (í‰ê·  65% êµ¬í˜„)

#### 1.1 Hierarchical Risk Parity (HRP) ê³ ë„í™”

**í‰ê°€:** âš ï¸ **70% êµ¬í˜„ (Systemic Similarity ëˆ„ë½)**

**í˜„ì¬ êµ¬í˜„ëœ ê¸°ëŠ¥:**
- âœ… Correlation-Distance ë³€í™˜ (`graph_clustered_portfolio.py:344-376`)
- âœ… Hierarchical Clustering (scipy linkage/dendrogram)
- âœ… Recursive Bisection (`_recursive_bisection()`, ë¼ì¸ 1084-1127)
- âœ… ê°€ì¤‘ì¹˜ ê²€ì¦

**ëˆ„ë½ëœ í•µì‹¬ ê¸°ëŠ¥:**
```python
# ğŸ”´ MISSING: Systemic Similarity ê³„ì‚°
# lib/graph_clustered_portfolio.pyì— ì¶”ê°€ í•„ìš”
def compute_systemic_similarity(distance_matrix: np.ndarray) -> np.ndarray:
    """
    D_bar[i,j] = sqrt(sum((D[k,i] - D[k,j])Â²))

    ë‹¨ìˆœ correlation ì´ˆê³¼ â†’ ìì‚° ê°„ ìƒí˜¸ì‘ìš© ê°•ë„ ì •ëŸ‰í™”
    """
    n = distance_matrix.shape[0]
    d_bar = np.zeros((n, n))
    for i in range(n):
        for j in range(n):
            d_bar[i, j] = np.sqrt(np.sum((distance_matrix[:, i] - distance_matrix[:, j])**2))
    return d_bar
```

**ë³´ì™„ ì‘ì—…:**
- Systemic Similarity ë¡œì§ êµ¬í˜„ (3-5ì¼)
- Seriation ìµœì í™” ì•Œê³ ë¦¬ì¦˜ ê°œì„  (ì˜µì…˜)
- HRP vs. CLA ë²¤ì¹˜ë§ˆí‚¹ í…ŒìŠ¤íŠ¸ ì¶”ê°€

**ìš°ì„ ìˆœìœ„:** â­â­â­ (ë‹¨ê¸° - 1ì£¼ ì´ë‚´)

---

#### 1.2 í´ëŸ¬ìŠ¤í„°ë§ ê¸°ë°˜ í¬íŠ¸í´ë¦¬ì˜¤ ìµœì í™”

**í‰ê°€:** âš ï¸ **60% êµ¬í˜„ (DBSCAN, DTW ëˆ„ë½)**

**í˜„ì¬ êµ¬í˜„ëœ ê¸°ëŠ¥:**
- âœ… K-means í´ëŸ¬ìŠ¤í„°ë§ (`_kmeans_clustering()`, ë¼ì¸ 716-769)
- âœ… Hierarchical Clustering (`_hierarchical_clustering()`, ë¼ì¸ 803-829)
- âœ… MST (Minimum Spanning Tree) (`build_mst()`, ë¼ì¸ 315-413)
- âš ï¸ LASSO (ë¶€ë¶„ êµ¬í˜„, ì •ê·œí™” ê°•ë„ ìë™ ì„ íƒ ë¯¸í¡)

**ëˆ„ë½ëœ í•µì‹¬ ê¸°ëŠ¥:**
```python
# ğŸ”´ MISSING: DBSCAN (ì´ìƒì¹˜ íƒì§€)
# lib/graph_clustered_portfolio.pyì— ì¶”ê°€
from sklearn.cluster import DBSCAN

def dbscan_outlier_detection(returns: pd.DataFrame, eps: float = 0.5, min_samples: int = 5) -> List[str]:
    """
    ë°€ë„ ê¸°ë°˜ ì´ìƒ ìì‚° íƒì§€
    Returns: ì´ìƒì¹˜ ticker ë¦¬ìŠ¤íŠ¸
    """
    clustering = DBSCAN(eps=eps, min_samples=min_samples)
    labels = clustering.fit_predict(returns.T)
    outliers = [ticker for ticker, label in zip(returns.columns, labels) if label == -1]
    return outliers

# ğŸ”´ MISSING: Dynamic Time Warping
# lib/time_series_similarity.py (ì‹ ê·œ íŒŒì¼)
def dtw_distance(series1: np.ndarray, series2: np.ndarray) -> float:
    """ì‹œê³„ì—´ ë¦¬ë“œ-ë˜ê·¸ ê´€ê³„ íŒŒì•…"""
    from dtaidistance import dtw
    return dtw.distance(series1, series2)

# ğŸ”´ MISSING: í´ëŸ¬ìŠ¤í„°ë§ í’ˆì§ˆ í‰ê°€
from sklearn.metrics import silhouette_score, davies_bouldin_score

def evaluate_clustering_quality(returns, labels):
    """Silhouette, Davies-Bouldin ì ìˆ˜ ê³„ì‚°"""
    silhouette = silhouette_score(returns.T, labels)
    davies_bouldin = davies_bouldin_score(returns.T, labels)
    return {"silhouette": silhouette, "davies_bouldin": davies_bouldin}
```

**ë³´ì™„ ì‘ì—…:**
- DBSCAN ì´ìƒì¹˜ íƒì§€ êµ¬í˜„ (2-3ì¼)
- DTW ì‹ ê·œ ëª¨ë“ˆ ìƒì„± (`lib/time_series_similarity.py`) (3-5ì¼)
- Silhouette/Davies-Bouldin í‰ê°€ ë©”íŠ¸ë¦­ ì¶”ê°€ (1-2ì¼)

**ìš°ì„ ìˆœìœ„:** â­â­ (ì¤‘ê¸° - 2-3ì£¼)

---

### 2. ì‹œì¥ ë¯¸ì„¸êµ¬ì¡° ë° ê±°ë˜ ë©”ì»¤ë‹ˆì¦˜ (í‰ê·  40% êµ¬í˜„)

#### 2.1 High-Frequency Trading (HFT) í™˜ê²½ ì§€í‘œ

**í‰ê°€:** âš ï¸ **40% êµ¬í˜„ (Tick Rule, Kyle's Lambda ëˆ„ë½)**

**í˜„ì¬ êµ¬í˜„ëœ ê¸°ëŠ¥:**
- âœ… Roll's Measure (`calculate_roll_spread()`, ë¼ì¸ 1223-1290)
- âœ… Amihud's Illiquidity (`calculate_amihud_lambda()`, ë¼ì¸ 1138-1222)
- âš ï¸ VPIN Approximation (ì¼ë³„ ë°ì´í„° ê·¼ì‚¬, ì •í™•í•œ Volume Clock ë¯¸êµ¬í˜„)

**ëˆ„ë½ëœ í•µì‹¬ ê¸°ëŠ¥:**
```python
# ğŸ”´ MISSING: Tick Rule (ê±°ë˜ ë°©í–¥ ë¶„ë¥˜)
# lib/microstructure.pyì— ì¶”ê°€ (1749ì¤„)
def tick_rule_classification(prices: pd.Series) -> pd.Series:
    """
    ê±°ë˜ ë°©í–¥ ë¶„ë¥˜ (Buy/Sell/Neutral)

    Rule:
    - p[t] > p[t-1]: b[t] = 1 (Buy)
    - p[t] < p[t-1]: b[t] = -1 (Sell)
    - p[t] = p[t-1]: b[t] = b[t-1] (ì´ì „ ë°©í–¥ ìœ ì§€)
    """
    b = pd.Series(index=prices.index, dtype=int)
    b.iloc[0] = 1  # ì´ˆê¸°ê°’ = Buy

    for i in range(1, len(prices)):
        if prices.iloc[i] > prices.iloc[i-1]:
            b.iloc[i] = 1
        elif prices.iloc[i] < prices.iloc[i-1]:
            b.iloc[i] = -1
        else:
            b.iloc[i] = b.iloc[i-1]

    return b

# ğŸ”´ MISSING: Kyle's Lambda (Market Impact)
def kyles_lambda(price_changes: pd.Series, signed_volume: pd.Series) -> float:
    """
    Kyle's Lambda = Market Impact ê³„ìˆ˜

    ëª¨ë¸: delta_p[t] = Lambda * (b[t] * V[t]) + error[t]
    OLS íšŒê·€ë¡œ Lambda ì¶”ì •
    """
    from sklearn.linear_model import LinearRegression

    X = signed_volume.values.reshape(-1, 1)
    y = price_changes.values

    model = LinearRegression()
    model.fit(X, y)

    lambda_value = model.coef_[0]
    r_squared = model.score(X, y)

    return {"lambda": lambda_value, "r_squared": r_squared}

# ğŸ”´ MISSING: Volume Clock Sampling
def volume_clock_sampling(df: pd.DataFrame, volume_bucket: float) -> pd.DataFrame:
    """
    Volume ê¸°ì¤€ ë™ê¸°í™” ìƒ˜í”Œë§ (VPIN ì •í™•ë„ í–¥ìƒ)

    Args:
        df: OHLCV ë°ì´í„°í”„ë ˆì„ (columns: ['timestamp', 'open', 'high', 'low', 'close', 'volume'])
        volume_bucket: ê° ë²„í‚·ì˜ ëˆ„ì  ê±°ë˜ëŸ‰ (ì˜ˆ: 1,000,000)

    Returns:
        Volume ê¸°ì¤€ìœ¼ë¡œ ìƒ˜í”Œë§ëœ ë°ì´í„°í”„ë ˆì„
    """
    df = df.copy()
    df['cumulative_volume'] = df['volume'].cumsum()
    df['bucket_id'] = (df['cumulative_volume'] / volume_bucket).astype(int)

    # ê° ë²„í‚·ì˜ ë§ˆì§€ë§‰ ê±°ë˜ ì„ íƒ
    sampled = df.groupby('bucket_id').last().reset_index(drop=True)

    return sampled

# ğŸ”´ MISSING: Quote Stuffing íƒì§€
def detect_quote_stuffing(order_data: pd.DataFrame, cancel_threshold: float = 0.9) -> Dict:
    """
    Quote Stuffing íƒì§€ (ì£¼ë¬¸ ì·¨ì†Œìœ¨ > 90%)

    Args:
        order_data: ì£¼ë¬¸ ë°ì´í„° (columns: ['order_id', 'action', 'timestamp'])
        cancel_threshold: ì£¼ë¬¸ ì·¨ì†Œìœ¨ ì„ê³„ê°’

    Returns:
        {'is_stuffing': bool, 'cancel_rate': float}
    """
    total_orders = len(order_data)
    canceled_orders = len(order_data[order_data['action'] == 'cancel'])
    cancel_rate = canceled_orders / total_orders

    return {
        "is_stuffing": cancel_rate > cancel_threshold,
        "cancel_rate": cancel_rate,
        "total_orders": total_orders,
        "canceled_orders": canceled_orders
    }
```

**ë³´ì™„ ì‘ì—…:**
1. Tick Rule êµ¬í˜„ (1-2ì¼)
2. Kyle's Lambda êµ¬í˜„ (2-3ì¼)
3. Volume Clock Sampling êµ¬í˜„ (2-3ì¼)
4. Quote Stuffing íƒì§€ (ì˜µì…˜, 1-2ì¼)
5. VPIN ì •í™•ë„ ê°œì„  (Volume Clock ê¸°ë°˜)

**ìš°ì„ ìˆœìœ„:** â­â­â­â­ (ìµœìš°ì„  - 1ì£¼ ì´ë‚´)

**íŒŒì¼ ìœ„ì¹˜:** `/home/tj/projects/autoai/eimas/lib/microstructure.py` (1749ì¤„)

---

### 3. ë¸”ë¡ì²´ì¸ ê¸°ë°˜ ì¸ë±ìŠ¤ & ìŠ¤ë§ˆíŠ¸ ê±°ë˜ (í‰ê·  30% êµ¬í˜„)

#### 3.1 Proof-of-Index (PoI) ë° ì˜¨ì²´ì¸ í€€íŠ¸ ì „ëµ

**í‰ê°€:** âŒ **30% êµ¬í˜„ (PoI ëª¨ë“ˆ ì‹ ê·œ í•„ìš”)**

**í˜„ì¬ êµ¬í˜„ëœ ê¸°ëŠ¥:**
- âœ… Stablecoin ë¦¬ìŠ¤í¬ í‰ê°€ (`genius_act_macro.py`, CryptoRiskEvaluator)
- âœ… Multi-dimensional Risk Scoring (ì‹ ìš©, ìœ ë™ì„±, ê·œì œ, ê¸°ìˆ )
- âš ï¸ Mean Reversion Signal (ë¶€ë¶„ êµ¬í˜„, `integrated_strategy.py`)

**ëˆ„ë½ëœ í•µì‹¬ ê¸°ëŠ¥:**
```python
# ğŸ”´ MISSING: Proof-of-Index ì „ì²´ ëª¨ë“ˆ
# lib/proof_of_index.py (ì‹ ê·œ ìƒì„±, ~400ì¤„)

import hashlib
import pandas as pd
import numpy as np
from typing import Dict, List

class ProofOfIndex:
    """
    Proof-of-Index (PoI) ë° ì˜¨ì²´ì¸ í€€íŠ¸ ì „ëµ

    ë°°ê²½:
    - ê¸°ì¡´ ê¸ˆìœµì§€ìˆ˜: ê³„ì‚° ë¸”ë™ë°•ìŠ¤, ì •ì‚° ì§€ì—° (T+2)
    - ë¸”ë¡ì²´ì¸ ê¸°ë°˜ íˆ¬ëª…ì„± ë° ì‹¤ì‹œê°„ ê²€ì¦
    """

    def __init__(self, divisor: float = 1.0):
        self.divisor = divisor
        self.index_history = []

    def calculate_index(self, prices: Dict[str, float], quantities: Dict[str, float]) -> float:
        """
        ì¸ë±ìŠ¤ ê³„ì‚°: I_t = sum(P_i_t * Q_i_t) / D_t

        Args:
            prices: {ticker: price}
            quantities: {ticker: quantity}

        Returns:
            index_value: ê³„ì‚°ëœ ì¸ë±ìŠ¤ ê°’
        """
        total_market_cap = sum(prices[ticker] * quantities[ticker]
                               for ticker in prices.keys())
        index_value = total_market_cap / self.divisor

        self.index_history.append({
            "timestamp": pd.Timestamp.now(),
            "value": index_value,
            "components": prices.copy()
        })

        return index_value

    def hash_index_weights(self, weights: Dict[str, float]) -> str:
        """
        SHA-256 ê¸°ë°˜ ê°€ì¤‘ì¹˜ í•´ì‹œ ìƒì„± (On-chain ê²€ì¦ìš©)

        Args:
            weights: {ticker: weight}

        Returns:
            hash_value: SHA-256 í•´ì‹œ ë¬¸ìì—´
        """
        # ì‚¬ì „ ìˆœì„œë¡œ ì •ë ¬í•˜ì—¬ ì¬í˜„ ê°€ëŠ¥ì„± ë³´ì¥
        sorted_weights = {k: weights[k] for k in sorted(weights.keys())}
        weights_str = str(sorted_weights).encode('utf-8')

        hash_object = hashlib.sha256(weights_str)
        return hash_object.hexdigest()

    def verify_on_chain(self, hash_value: str, reference_hash: str) -> bool:
        """
        Smart Contract ê¸°ë°˜ í•´ì‹œ ê²€ì¦

        Args:
            hash_value: ê³„ì‚°ëœ í•´ì‹œ
            reference_hash: On-chain ì°¸ì¡° í•´ì‹œ

        Returns:
            is_valid: ê²€ì¦ ê²°ê³¼
        """
        return hash_value == reference_hash

    def mean_reversion_signal(self,
                              prices: pd.Series,
                              window: int = 20,
                              threshold: float = 2.0) -> str:
        """
        Mean Reversion í€€íŠ¸ ì‹ í˜¸ ìƒì„±

        Args:
            prices: ê°€ê²© ì‹œê³„ì—´
            window: ì´ë™í‰ê·  ìœˆë„ìš°
            threshold: Z-score ì„ê³„ê°’ (ì˜ˆ: Â±2.0)

        Returns:
            signal: 'BUY' / 'SELL' / 'HOLD'
        """
        mean = prices.rolling(window).mean()
        std = prices.rolling(window).std()
        z_score = (prices - mean) / std

        latest_z = z_score.iloc[-1]

        if latest_z < -threshold:
            return "BUY"
        elif latest_z > threshold:
            return "SELL"
        else:
            return "HOLD"

    def backtest_strategy(self,
                         prices: pd.DataFrame,
                         initial_capital: float = 100000) -> Dict:
        """
        Mean Reversion ì „ëµ ë°±í…ŒìŠ¤íŠ¸

        Returns:
            results: {'total_return': float, 'sharpe_ratio': float, 'max_drawdown': float}
        """
        # ë°±í…ŒìŠ¤íŠ¸ ë¡œì§ êµ¬í˜„
        pass

# ì‚¬ìš© ì˜ˆì‹œ:
# poi = ProofOfIndex(divisor=100.0)
# index_value = poi.calculate_index(prices={'BTC': 50000, 'ETH': 3000},
#                                    quantities={'BTC': 1.0, 'ETH': 10.0})
# hash_val = poi.hash_index_weights({'BTC': 0.6, 'ETH': 0.4})
# signal = poi.mean_reversion_signal(btc_prices)
```

**ë³´ì™„ ì‘ì—…:**
1. `lib/proof_of_index.py` ì‹ ê·œ ìƒì„± (3-5ì¼)
2. Mean Reversion ë°±í…ŒìŠ¤íŠ¸ ì—”ì§„ (3-5ì¼)
3. On-chain ë°ì´í„° ìˆ˜ì§‘ (Chainlink/Pyth ì˜¤ë¼í´) (5-7ì¼)
4. Smart Contract ê²€ì¦ ë¡œì§ (ì˜µì…˜, ì‹œë®¬ë ˆì´ì…˜)

**ìš°ì„ ìˆœìœ„:** â­â­ (ì¤‘ê¸° - 2-3ì£¼)

---

### 4. AI/ML ê¸°ìˆ  ê¸°ì´ˆ

#### 4.1 Convolution ê¸°ë°˜ ì‹œê³„ì—´ íŒ¨í„´ íƒì§€

**í‰ê°€:** âŒ **0% êµ¬í˜„ (CNN ëª¨ë“ˆ ì‹ ê·œ í•„ìš”)**

**ëˆ„ë½ëœ ì „ì²´ ê¸°ëŠ¥:**
```python
# ğŸ”´ MISSING: CNN íŒ¨í„´ íƒì§€ ëª¨ë“ˆ
# lib/cnn_pattern_detector.py (ì‹ ê·œ ìƒì„±, ~500ì¤„)

import numpy as np
import pandas as pd
from typing import Tuple, List

class CNNPatternDetector:
    """
    Convolution ê¸°ë°˜ ì‹œê³„ì—´ íŒ¨í„´ íƒì§€

    ë°°ê²½:
    - ì£¼ì‹ ê°€ê²© heatmapì—ì„œ íŒ¨í„´ ìë™ ì¶”ì¶œ
    - ê¸°ìˆ ì  ì§€í‘œ ìë™í™” (í—¤ë“œì•¤ìˆ„ë”, ì‚¼ê° ìˆ˜ë ´ ë“±)
    """

    def __init__(self, filter_size: Tuple[int, int] = (3, 3)):
        self.filter_size = filter_size
        self.filters = self._initialize_filters()

    def _initialize_filters(self) -> Dict[str, np.ndarray]:
        """
        í•„í„° ì´ˆê¸°í™” (Edge Detection, Momentum ë“±)

        Returns:
            filters: {'edge': array, 'momentum': array, ...}
        """
        filters = {}

        # Edge Detection (Sobel)
        filters['edge_x'] = np.array([
            [-1, 0, 1],
            [-2, 0, 2],
            [-1, 0, 1]
        ])

        filters['edge_y'] = np.array([
            [-1, -2, -1],
            [ 0,  0,  0],
            [ 1,  2,  1]
        ])

        # Momentum Filter (ê°€ê²© ìƒìŠ¹ íŒ¨í„´)
        filters['momentum'] = np.array([
            [-1, -1, -1],
            [ 0,  0,  0],
            [ 1,  1,  1]
        ])

        return filters

    def conv2d(self,
               input_grid: np.ndarray,
               filter_kernel: np.ndarray,
               stride: int = 1) -> np.ndarray:
        """
        2D Convolution ì—°ì‚°

        Args:
            input_grid: ì…ë ¥ ì´ë¯¸ì§€ (H Ã— W)
            filter_kernel: í•„í„° (F_h Ã— F_w)
            stride: ìŠ¬ë¼ì´ë”© ê°„ê²©

        Returns:
            output_map: Feature Map ((H-F_h)/stride+1 Ã— (W-F_w)/stride+1)
        """
        H, W = input_grid.shape
        F_h, F_w = filter_kernel.shape

        # ì¶œë ¥ í¬ê¸° ê³„ì‚°
        out_H = (H - F_h) // stride + 1
        out_W = (W - F_w) // stride + 1

        output_map = np.zeros((out_H, out_W))

        for r in range(out_H):
            for c in range(out_W):
                # ìœˆë„ìš° ì¶”ì¶œ
                r_start = r * stride
                c_start = c * stride
                window = input_grid[r_start:r_start+F_h, c_start:c_start+F_w]

                # Element-wise ê³±ì…ˆ ë° í•©ì‚°
                output_map[r, c] = np.sum(window * filter_kernel)

        return output_map

    def generate_heatmap(self,
                        prices: pd.DataFrame,
                        window: int = 20) -> np.ndarray:
        """
        ê°€ê²© ì‹œê³„ì—´ â†’ 2D Heatmap ë³€í™˜

        Args:
            prices: ê°€ê²© ë°ì´í„° (columns: tickers, index: dates)
            window: ì‹œê°„ ìœˆë„ìš°

        Returns:
            heatmap: 2D ì´ë¯¸ì§€ (tickers Ã— time)
        """
        # ì •ê·œí™” (0-255 ë²”ìœ„)
        normalized = (prices - prices.min()) / (prices.max() - prices.min()) * 255
        heatmap = normalized.values.T  # Transpose (tickers as rows)

        return heatmap

    def detect_patterns(self, prices: pd.DataFrame) -> Dict[str, List]:
        """
        íŒ¨í„´ íƒì§€ (í—¤ë“œì•¤ìˆ„ë”, ì‚¼ê° ìˆ˜ë ´ ë“±)

        Returns:
            patterns: {'head_and_shoulders': [...], 'triangle': [...]}
        """
        heatmap = self.generate_heatmap(prices)

        patterns = {}

        # Edge Detection
        edge_x = self.conv2d(heatmap, self.filters['edge_x'])
        edge_y = self.conv2d(heatmap, self.filters['edge_y'])
        edge_magnitude = np.sqrt(edge_x**2 + edge_y**2)

        # Momentum Detection
        momentum = self.conv2d(heatmap, self.filters['momentum'])

        # íŒ¨í„´ ì‹ë³„ (ë‹¨ìˆœ ì„ê³„ê°’ ê¸°ë°˜)
        patterns['strong_edges'] = np.where(edge_magnitude > 200)
        patterns['momentum_zones'] = np.where(momentum > 100)

        return patterns

    def validate_output_size(self,
                            input_shape: Tuple[int, int],
                            filter_shape: Tuple[int, int],
                            stride: int) -> Tuple[int, int]:
        """
        ì¶œë ¥ í¬ê¸° ê²€ì¦

        Formula: output = (input - filter) / stride + 1
        """
        H, W = input_shape
        F_h, F_w = filter_shape

        out_H = (H - F_h) // stride + 1
        out_W = (W - F_w) // stride + 1

        return (out_H, out_W)

# ì‚¬ìš© ì˜ˆì‹œ:
# detector = CNNPatternDetector()
# heatmap = detector.generate_heatmap(prices_df)
# patterns = detector.detect_patterns(prices_df)
# edge_map = detector.conv2d(heatmap, detector.filters['edge_x'])
```

**ë³´ì™„ ì‘ì—…:**
1. `lib/cnn_pattern_detector.py` ì‹ ê·œ ìƒì„± (5-7ì¼)
2. ê¸°ìˆ ì  ì§€í‘œ íŒ¨í„´ ë¼ì´ë¸ŒëŸ¬ë¦¬ êµ¬ì¶• (7-10ì¼)
3. ë°±í…ŒìŠ¤íŠ¸ í†µí•© (3-5ì¼)

**ìš°ì„ ìˆœìœ„:** â­ (ì¥ê¸° - 3-4ì£¼)

---

#### 4.2 LLM ë„ë©”ì¸ íŠ¹í™”

**í‰ê°€:** âš ï¸ **50% êµ¬í˜„ (Fine-tuning, Multimodal ë¯¸êµ¬í˜„)**

**í˜„ì¬ êµ¬í˜„ëœ ê¸°ëŠ¥:**
- âœ… Claude/Perplexity API ì‚¬ìš© (`agents/orchestrator.py`)
- âœ… í† ë¡  í”„ë¡œí† ì½œ (`core/debate.py`)
- âš ï¸ íŒ©íŠ¸ì²´í‚¹ (`autonomous_agent.py`, í¸í–¥ì„± íƒì§€ ë¯¸í¡)

**ë³´ì™„ ì‘ì—…:**
1. ê²½ì œí•™/ê¸ˆìœµ ë„ë©”ì¸ Fine-tuning ë°ì´í„°ì…‹ ìˆ˜ì§‘ (10K+ ìƒ˜í”Œ) (2-3ì£¼)
2. Supervised Fine-Tuning (SFT) íŒŒì´í”„ë¼ì¸ êµ¬ì¶• (3-4ì£¼)
3. Vision Transformer ê¸°ë°˜ ì°¨íŠ¸ í•´ì„ ëª¨ë“ˆ (4-6ì£¼)
4. Bias Detection ìë™í™” (2-3ì£¼)

**ìš°ì„ ìˆœìœ„:** â­â­ (ì¤‘ê¸° - 2-3ê°œì›”)

---

### 5. ê²½ì œí•™ í†µí•© ë° ì¸ê³¼ê´€ê³„ ë¶„ì„ (í‰ê·  80% êµ¬í˜„)

#### 5.1 Causality vs. Correlation: ì¸ê³¼ê´€ê³„ ë„¤íŠ¸ì›Œí¬

**í‰ê°€:** âš ï¸ **65% êµ¬í˜„ (GARCH, Private Info Score ëˆ„ë½)**

**í˜„ì¬ êµ¬í˜„ëœ ê¸°ëŠ¥:**
- âœ… Granger Causality (`causality_graph.py`, ë¼ì¸ 1-1099)
- âœ… Shock Propagation (`shock_propagation_graph.py`, ë¼ì¸ 1-897)
- âœ… Sector Rotation (GMM) (`regime_analyzer.py`)
- âš ï¸ Information Flow (ë¶€ë¶„, `etf_flow_analyzer.py`)

**ëˆ„ë½ëœ í•µì‹¬ ê¸°ëŠ¥:**
```python
# ğŸ”´ MISSING: GARCH Model
# lib/regime_analyzer.pyì— ì¶”ê°€ (~450ì¤„ â†’ ~600ì¤„)

from arch import arch_model

class GARCHModel:
    """
    GARCH (Generalized Autoregressive Conditional Heteroskedasticity)
    ì‹œë³€ ë³€ë™ì„± ëª¨ë¸ë§
    """

    def __init__(self, p: int = 1, q: int = 1):
        """
        Args:
            p: ARCH í•­ ì°¨ìˆ˜
            q: GARCH í•­ ì°¨ìˆ˜

        Model:
            sigma_tÂ² = Ï‰ + Î±Â·ÎµÂ²_{t-1} + Î²Â·ÏƒÂ²_{t-1}
        """
        self.p = p
        self.q = q
        self.model = None
        self.fitted_model = None

    def fit(self, returns: pd.Series) -> Dict:
        """
        GARCH ëª¨ë¸ í”¼íŒ…

        Returns:
            params: {'omega': float, 'alpha': float, 'beta': float}
        """
        # GARCH(p,q) ëª¨ë¸
        self.model = arch_model(returns, vol='Garch', p=self.p, q=self.q)
        self.fitted_model = self.model.fit(disp='off')

        params = {
            'omega': self.fitted_model.params['omega'],
            'alpha': self.fitted_model.params['alpha[1]'] if self.p > 0 else 0,
            'beta': self.fitted_model.params['beta[1]'] if self.q > 0 else 0
        }

        return params

    def forecast(self, horizon: int = 20) -> pd.Series:
        """
        ë‹¤ì¤‘ ê¸°ê°„ ë³€ë™ì„± ì˜ˆì¸¡

        Returns:
            volatility_forecast: ì˜ˆì¸¡ëœ ì¡°ê±´ë¶€ ë¶„ì‚°
        """
        if self.fitted_model is None:
            raise ValueError("Model not fitted yet. Call fit() first.")

        forecast = self.fitted_model.forecast(horizon=horizon)
        volatility = np.sqrt(forecast.variance.values[-1, :])

        return pd.Series(volatility, index=range(1, horizon+1))

# ğŸ”´ MISSING: Information Flow ëª¨ë“ˆ
# lib/information_flow.py (ì‹ ê·œ ìƒì„±, ~300ì¤„)

class InformationFlowAnalyzer:
    """
    ì •ë³´ í”Œë¡œìš° ë¶„ì„ (ê±°ë˜ëŸ‰ ì´ìƒ íƒì§€, Private Information Score)
    """

    def __init__(self, ma_window: int = 20, threshold: float = 5.0):
        self.ma_window = ma_window
        self.threshold = threshold

    def detect_abnormal_volume(self, volume: pd.Series) -> pd.DataFrame:
        """
        ê±°ë˜ëŸ‰ ì´ìƒ íƒì§€

        Rule: volume[t] > MA(volume, 20) * 5

        Returns:
            abnormal_dates: DataFrame with columns ['date', 'volume', 'ma', 'ratio']
        """
        ma = volume.rolling(self.ma_window).mean()
        ratio = volume / ma

        abnormal = ratio > self.threshold

        results = pd.DataFrame({
            'date': volume.index[abnormal],
            'volume': volume[abnormal].values,
            'ma': ma[abnormal].values,
            'ratio': ratio[abnormal].values
        })

        return results

    def calculate_private_info_score(self,
                                     buy_volume: pd.Series,
                                     sell_volume: pd.Series) -> pd.Series:
        """
        Private Information Extraction Score

        Formula: (volume_buy - volume_sell) / total_volume

        Interpretation:
        - > 0: Buy pressure (ì •ë³´ ìš°ìœ„ ë§¤ìˆ˜ì„¸)
        - < 0: Sell pressure (ì •ë³´ ìš°ìœ„ ë§¤ë„ì„¸)
        """
        total_volume = buy_volume + sell_volume
        score = (buy_volume - sell_volume) / total_volume

        return score

    def estimate_capm(self,
                     asset_returns: pd.Series,
                     market_returns: pd.Series) -> Dict:
        """
        CAPM Regression: E[R_i] = Alpha + Beta * E[R_m]

        Returns:
            {'alpha': float, 'beta': float, 'r_squared': float}
        """
        from sklearn.linear_model import LinearRegression

        # NaN ì œê±°
        mask = ~(asset_returns.isna() | market_returns.isna())
        X = market_returns[mask].values.reshape(-1, 1)
        y = asset_returns[mask].values

        model = LinearRegression()
        model.fit(X, y)

        alpha = model.intercept_
        beta = model.coef_[0]
        r_squared = model.score(X, y)

        return {
            'alpha': alpha,
            'beta': beta,
            'r_squared': r_squared
        }
```

**ë³´ì™„ ì‘ì—…:**
1. GARCH ëª¨ë¸ êµ¬í˜„ (`regime_analyzer.py`) (3-5ì¼)
2. Information Flow ëª¨ë“ˆ ì‹ ê·œ ìƒì„± (`lib/information_flow.py`) (3-5ì¼)
3. CAPM Alpha/Beta ìë™ ê³„ì‚° í†µí•© (2-3ì¼)

**ìš°ì„ ìˆœìœ„:** â­â­â­ (ë‹¨ê¸° - 1-2ì£¼)

---

#### 5.2 Whitening (Explainability) ê°•í™”

**í‰ê°€:** âœ… **95% êµ¬í˜„ (ê±°ì˜ ì™„ì„±)**

**í˜„ì¬ êµ¬í˜„ëœ ê¸°ëŠ¥:**
- âœ… ê²½ì œí•™ì  í•´ì„ ë ˆì´ì–´ (`whitening_engine.py`, 1000+ ì¤„)
- âœ… ì¸ê³¼ê´€ê³„ ì²´ì¸ ì¶”ì  (`causality_graph.py` + `shock_propagation_graph.py`)
- âœ… íŒ©íŠ¸ì²´í‚¹ í†µí•© (`autonomous_agent.py`)

**ì†Œí­ ë³´ì™„ ì‘ì—… (ì˜µì…˜):**
- ê·¸ë˜í”„ ì‹œê°í™” ê°œì„  (D3.js/Graphviz ë Œë”ë§) (3-5ì¼)
- ì‹¤ì‹œê°„ Whitening (ìŠ¤íŠ¸ë¦¬ë° ë°ì´í„°) (5-7ì¼)

**ìš°ì„ ìˆœìœ„:** â­ (ì™„ë£Œ, ì˜µì…˜ ê°œì„ ë§Œ)

---

## ğŸ”¥ ì¦‰ì‹œ ì‹¤í–‰ ê°€ëŠ¥í•œ ì‘ì—… ë¦¬ìŠ¤íŠ¸ (1ì£¼ ì´ë‚´)

### Week 1: HFT ë¯¸ì„¸êµ¬ì¡° ê°•í™” (ìµœìš°ì„ )

**íŒŒì¼:** `/home/tj/projects/autoai/eimas/lib/microstructure.py` (1749ì¤„)

**ì‘ì—…:**
1. âœ… Tick Rule êµ¬í˜„ (1-2ì¼)
   ```python
   def tick_rule_classification(prices: pd.Series) -> pd.Series:
       # Buy/Sell/Neutral ë¶„ë¥˜
   ```

2. âœ… Kyle's Lambda êµ¬í˜„ (2-3ì¼)
   ```python
   def kyles_lambda(price_changes: pd.Series, signed_volume: pd.Series) -> float:
       # OLS íšŒê·€ë¡œ Lambda ì¶”ì •
   ```

3. âœ… Volume Clock Sampling êµ¬í˜„ (2-3ì¼)
   ```python
   def volume_clock_sampling(df: pd.DataFrame, volume_bucket: float) -> pd.DataFrame:
       # VPIN ì •í™•ë„ í–¥ìƒ
   ```

**ì˜ˆìƒ ì¶”ê°€ ì½”ë“œ:** ~200-300ì¤„
**ìš°ì„ ìˆœìœ„:** â­â­â­â­

---

### Week 1-2: HRP Systemic Similarity (ë‹¨ê¸°)

**íŒŒì¼:** `/home/tj/projects/autoai/eimas/lib/graph_clustered_portfolio.py` (1524ì¤„)

**ì‘ì—…:**
```python
def compute_systemic_similarity(distance_matrix: np.ndarray) -> np.ndarray:
    """D_bar[i,j] = sqrt(sum((D[k,i] - D[k,j])Â²))"""
    # ìì‚° ê°„ ìƒí˜¸ì‘ìš© ê°•ë„ ì •ëŸ‰í™”
```

**ì˜ˆìƒ ì¶”ê°€ ì½”ë“œ:** ~50-100ì¤„
**ìš°ì„ ìˆœìœ„:** â­â­â­

---

### Week 1-2: GARCH + Information Flow (ë‹¨ê¸°)

**íŒŒì¼ 1:** `/home/tj/projects/autoai/eimas/lib/regime_analyzer.py` (~450ì¤„)
**ì‘ì—…:** GARCH í´ë˜ìŠ¤ ì¶”ê°€ (~100-150ì¤„)

**íŒŒì¼ 2:** `/home/tj/projects/autoai/eimas/lib/information_flow.py` (ì‹ ê·œ)
**ì‘ì—…:** InformationFlowAnalyzer í´ë˜ìŠ¤ ìƒì„± (~300ì¤„)

**ìš°ì„ ìˆœìœ„:** â­â­â­

---

## ğŸŸ¡ ì¤‘ê¸° ì‹¤í–‰ ì‘ì—… ë¦¬ìŠ¤íŠ¸ (2-4ì£¼)

### Month 1: í´ëŸ¬ìŠ¤í„°ë§ ë³´ì™„

**íŒŒì¼:** `/home/tj/projects/autoai/eimas/lib/graph_clustered_portfolio.py`

**ì‘ì—…:**
1. DBSCAN ì´ìƒì¹˜ íƒì§€ (2-3ì¼)
2. DTW ì‹ ê·œ ëª¨ë“ˆ (`lib/time_series_similarity.py`) (3-5ì¼)
3. Silhouette/Davies-Bouldin í‰ê°€ (1-2ì¼)

**ì˜ˆìƒ ì¶”ê°€ ì½”ë“œ:** ~200-300ì¤„

**ìš°ì„ ìˆœìœ„:** â­â­

---

### Month 1-2: Proof-of-Index ëª¨ë“ˆ

**íŒŒì¼:** `/home/tj/projects/autoai/eimas/lib/proof_of_index.py` (ì‹ ê·œ, ~400ì¤„)

**ì‘ì—…:**
1. ProofOfIndex í´ë˜ìŠ¤ êµ¬í˜„ (3-5ì¼)
2. Mean Reversion ë°±í…ŒìŠ¤íŠ¸ (3-5ì¼)
3. On-chain ë°ì´í„° ì—°ë™ (5-7ì¼)

**ìš°ì„ ìˆœìœ„:** â­â­

---

## ğŸŸ  ì¥ê¸° ì‹¤í–‰ ì‘ì—… ë¦¬ìŠ¤íŠ¸ (1-3ê°œì›”)

### Month 2-3: CNN íŒ¨í„´ íƒì§€

**íŒŒì¼:** `/home/tj/projects/autoai/eimas/lib/cnn_pattern_detector.py` (ì‹ ê·œ, ~500ì¤„)

**ì‘ì—…:**
1. CNNPatternDetector í´ë˜ìŠ¤ (5-7ì¼)
2. ê¸°ìˆ ì  ì§€í‘œ íŒ¨í„´ ë¼ì´ë¸ŒëŸ¬ë¦¬ (7-10ì¼)
3. ë°±í…ŒìŠ¤íŠ¸ í†µí•© (3-5ì¼)

**ìš°ì„ ìˆœìœ„:** â­

---

### Month 2-3: LLM Fine-tuning

**ì‘ì—…:**
1. ê²½ì œí•™/ê¸ˆìœµ ë°ì´í„°ì…‹ ìˆ˜ì§‘ (2-3ì£¼)
2. SFT íŒŒì´í”„ë¼ì¸ êµ¬ì¶• (3-4ì£¼)
3. Vision Transformer ì°¨íŠ¸ í•´ì„ (4-6ì£¼)

**ìš°ì„ ìˆœìœ„:** â­â­

---

## ğŸ“Š ìµœì¢… ìš”ì•½í‘œ

| ì¹´í…Œê³ ë¦¬ | êµ¬í˜„ë„ | ì¦‰ì‹œ í•„ìš” (1ì£¼) | ë‹¨ê¸° (2-4ì£¼) | ì¤‘ê¸° (1-2ê°œì›”) | ì¥ê¸° (3-6ê°œì›”) |
|---------|-------|---------------|-------------|---------------|---------------|
| **í¬íŠ¸í´ë¦¬ì˜¤ ìµœì í™”** | 65% | Systemic Similarity | DBSCAN, DTW | - | - |
| **ì‹œì¥ ë¯¸ì„¸êµ¬ì¡°** | 40% | Tick Rule, Kyle's Lambda, Volume Clock | - | - | - |
| **ë¸”ë¡ì²´ì¸** | 30% | - | - | Proof-of-Index | - |
| **AI/ML** | 25% | - | - | LLM Fine-tuning | CNN íŒ¨í„´ íƒì§€ |
| **ê²½ì œí•™ í†µí•©** | 80% | GARCH, Info Flow | - | - | - |
| **Whitening** | 95% | (ì™„ë£Œ) | - | - | - |

---

## ğŸ“ ì‹ ê·œ ìƒì„± í•„ìš” íŒŒì¼

| íŒŒì¼ëª… | ì˜ˆìƒ í¬ê¸° | ìš°ì„ ìˆœìœ„ | ì˜ˆìƒ ì‘ì—… ì‹œê°„ |
|-------|---------|---------|----------------|
| `lib/information_flow.py` | ~300ì¤„ | â­â­â­ | 3-5ì¼ |
| `lib/proof_of_index.py` | ~400ì¤„ | â­â­ | 2-3ì£¼ |
| `lib/time_series_similarity.py` | ~200ì¤„ | â­â­ | 3-5ì¼ |
| `lib/cnn_pattern_detector.py` | ~500ì¤„ | â­ | 3-4ì£¼ |

---

## ğŸ¯ ê¶Œì¥ ì‹¤í–‰ ìˆœì„œ

### Phase 1 (Week 1-2): í•µì‹¬ ë³´ì™„
1. âœ… Tick Rule + Kyle's Lambda + Volume Clock (`microstructure.py`)
2. âœ… Systemic Similarity (`graph_clustered_portfolio.py`)
3. âœ… GARCH ëª¨ë¸ (`regime_analyzer.py`)
4. âœ… Information Flow ëª¨ë“ˆ ì‹ ê·œ ìƒì„±

**ì˜ˆìƒ ì´ ì‘ì—… ì‹œê°„:** 10-14ì¼

---

### Phase 2 (Week 3-6): ê¸°ëŠ¥ í™•ì¥
5. DBSCAN + DTW + í´ëŸ¬ìŠ¤í„°ë§ í‰ê°€
6. Proof-of-Index ëª¨ë“ˆ ì‹ ê·œ ìƒì„±
7. LLM Fine-tuning ë°ì´í„°ì…‹ ìˆ˜ì§‘ ì‹œì‘

**ì˜ˆìƒ ì´ ì‘ì—… ì‹œê°„:** 4-6ì£¼

---

### Phase 3 (Month 3-6): ê³ ê¸‰ ê¸°ëŠ¥
8. CNN íŒ¨í„´ íƒì§€ ëª¨ë“ˆ
9. LLM Fine-tuning íŒŒì´í”„ë¼ì¸
10. Vision Transformer ì°¨íŠ¸ í•´ì„

**ì˜ˆìƒ ì´ ì‘ì—… ì‹œê°„:** 2-3ê°œì›”

---

## ğŸ“Œ í•µì‹¬ ë°œê²¬ì‚¬í•­

### ê°•ì  (Keep)
- âœ… **Whitening & ì¸ê³¼ê´€ê³„ ë¶„ì„**: 95% ì™„ì„±ë„ (ì„¸ê³„ì  ìˆ˜ì¤€)
- âœ… **Stablecoin ë¦¬ìŠ¤í¬ í‰ê°€**: ë‹¤ì°¨ì› í‰ê°€ ì™„ë¹„
- âœ… **MST & HRP**: ê¸°ë³¸ ê³¨ê²© ì™„ì„±

### ë³´ì™„ í•„ìš” (Improve)
- âš ï¸ **HFT ë¯¸ì„¸êµ¬ì¡°**: Tick Rule, Kyle's Lambda ëˆ„ë½
- âš ï¸ **HRP**: Systemic Similarity ë¯¸êµ¬í˜„
- âš ï¸ **GARCH**: ì‹œë³€ ë³€ë™ì„± ëª¨ë¸ ë¶€ì¬

### ì‹ ê·œ í•„ìš” (Add)
- âŒ **Proof-of-Index**: ë¸”ë¡ì²´ì¸ ì¸ë±ìŠ¤ ì „ì²´ ëª¨ë“ˆ
- âŒ **CNN íŒ¨í„´ íƒì§€**: ë”¥ëŸ¬ë‹ ê¸°ë°˜ ê¸°ìˆ ì  ë¶„ì„
- âŒ **LLM Fine-tuning**: ê²½ì œí•™ ë„ë©”ì¸ íŠ¹í™”

---

**ë¬¸ì„œ ì‘ì„±:** Claude Code (Explore ì—ì´ì „íŠ¸)
**ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸:** 2026-01-24 23:30 KST
**ì†ŒìŠ¤:** EIMAS ì‹œìŠ¤í…œ (v2.1.2) + todolist.md ë¹„êµ ë¶„ì„
