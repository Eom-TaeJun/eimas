#!/usr/bin/env python3
"""
EIMAS - Economic Intelligence Multi-Agent System
=================================================
í†µí•© ì‹¤í–‰ íŒŒì´í”„ë¼ì¸

ëª¨ë“  ê¸°ëŠ¥ì„ ìˆœì°¨ì ìœ¼ë¡œ ì‹¤í–‰:
1. ë°ì´í„° ìˆ˜ì§‘ (FRED, Market, Crypto)
2. ë ˆì§ íƒì§€
3. ì´ë²¤íŠ¸ íƒì§€
4. ìœ ë™ì„± ë¶„ì„ (Granger Causality)
5. ë©€í‹°ì—ì´ì „íŠ¸ í† ë¡  (ì†Œìˆ˜ì˜ê²¬ ë³´í˜¸)
6. ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° (ì„ íƒì )
7. DB ì €ì¥
8. ì•Œë¦¼ ë°œì†¡

ì œì™¸ í•­ëª©:
- CME FedWatch íˆìŠ¤í† ë¦¬ì»¬ ë¶„ì„ (2024-2025 í™•ì • ë°ì´í„°)
- LASSO ì˜ˆì¸¡ (íˆìŠ¤í† ë¦¬ì»¬ íŒ¨í„´ ê¸°ë°˜)

Usage:
    python main_integrated.py              # ì „ì²´ íŒŒì´í”„ë¼ì¸
    python main_integrated.py --realtime   # ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° í¬í•¨
    python main_integrated.py --quick      # ë¹ ë¥¸ ë¶„ì„ë§Œ
"""

import argparse
import asyncio
import json
import logging
import sys
import os
from datetime import datetime
from pathlib import Path
from typing import Dict, Any, Optional, List
from dataclasses import dataclass, field, asdict

# í”„ë¡œì íŠ¸ ë£¨íŠ¸
sys.path.insert(0, str(Path(__file__).parent))

# ============================================================================
# Imports
# ============================================================================

# Core
from core.schemas import AnalysisMode, HistoricalDataConfig
from core.debate import DebateProtocol

# Agents
from agents.orchestrator import MetaOrchestrator

# Data Collection
from lib.fred_collector import FREDCollector
from lib.data_collector import DataManager
from lib.unified_data_store import UnifiedDataStore
from lib.market_indicators import MarketIndicatorsCollector

# Analysis
from lib.regime_detector import RegimeDetector
from lib.regime_analyzer import GMMRegimeAnalyzer, get_gmm_regime_summary
from lib.event_framework import QuantitativeEventDetector, EventType
from lib.liquidity_analysis import LiquidityMarketAnalyzer
from lib.causal_network import CausalNetworkAnalyzer
from lib.critical_path import CriticalPathAggregator
from lib.correlation_monitor import CorrelationMonitor
from lib.etf_flow_analyzer import ETFFlowAnalyzer

# Real-time
from lib.binance_stream import BinanceStreamer, StreamConfig
from lib.microstructure import MicrostructureAnalyzer, DailyMicrostructureAnalyzer
from lib.realtime_pipeline import RealtimePipeline, PipelineConfig, SignalDatabase

# Bubble Detection & Market Quality (v2.1.1)
from lib.bubble_detector import BubbleDetector, BubbleWarningLevel

# Database
from lib.trading_db import TradingDB, Signal
from lib.event_db import EventDatabase

# Dual Mode
from lib.dual_mode_analyzer import DualModeAnalyzer, ModeResult

# Advanced Strategy Modules (Part 2 & 3)
from lib.graph_clustered_portfolio import GraphClusteredPortfolio, ClusteringMethod
from lib.shock_propagation_graph import ShockPropagationGraph
from lib.integrated_strategy import IntegratedStrategy, SignalType
from lib.whitening_engine import WhiteningEngine
from lib.custom_etf_builder import CustomETFBuilder, ThemeCategory
from lib.genius_act_macro import GeniusActMacroStrategy, LiquidityIndicators, CryptoRiskEvaluator
from lib.autonomous_agent import AutonomousFactChecker, AIOutputVerifier
from lib.volume_analyzer import VolumeAnalyzer
from lib.causality_graph import CausalityGraphEngine, NodeType, EdgeType
from lib.predictions_db import PredictionsDB, save_eimas_result

# 2026-01-10 ì¶”ê°€ ëª¨ë“ˆ
from lib.extended_data_sources import ExtendedDataCollector, DeFiLlamaCollector
from lib.event_tracker import EventTracker, EventTrackingResult
from lib.adaptive_agents import (
    AdaptiveAgentManager, MarketCondition,
    AggressiveAdaptiveAgent, BalancedAdaptiveAgent, ConservativeAdaptiveAgent
)
from lib.validation_agents import ValidationLoopManager, FeedbackLoopResult

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger('eimas.integrated')


# ============================================================================
# Data Classes
# ============================================================================

@dataclass
class MarketQualityMetrics:
    """ì‹œì¥ ë¯¸ì„¸êµ¬ì¡° í’ˆì§ˆ ë©”íŠ¸ë¦­ (API ê²€ì¦ ê²°ê³¼ ë°˜ì˜)"""
    avg_liquidity_score: float = 50.0
    liquidity_scores: Dict[str, float] = field(default_factory=dict)  # {ticker: score}
    high_toxicity_tickers: List[str] = field(default_factory=list)    # VPIN ë†’ì€ ì¢…ëª©
    illiquid_tickers: List[str] = field(default_factory=list)         # ìœ ë™ì„± ë‚®ì€ ì¢…ëª©
    data_quality: str = "COMPLETE"  # COMPLETE, PARTIAL, DEGRADED

    def to_dict(self) -> Dict:
        return {
            'avg_liquidity_score': round(self.avg_liquidity_score, 2),
            'liquidity_scores': self.liquidity_scores,
            'high_toxicity_tickers': self.high_toxicity_tickers,
            'illiquid_tickers': self.illiquid_tickers,
            'data_quality': self.data_quality
        }


@dataclass
class BubbleRiskMetrics:
    """ë²„ë¸” ë¦¬ìŠ¤í¬ ë©”íŠ¸ë¦­ (Greenwood-Shleifer ê¸°ë°˜)"""
    overall_status: str = "NONE"  # NONE, WATCH, WARNING, DANGER
    risk_tickers: List[Dict] = field(default_factory=list)  # [{ticker, level, runup_pct, vol_zscore, risk_score}]
    highest_risk_ticker: str = ""
    highest_risk_score: float = 0.0
    methodology_notes: str = "Bubbles for Fama (2019)"

    def to_dict(self) -> Dict:
        return {
            'overall_status': self.overall_status,
            'risk_tickers': self.risk_tickers,
            'highest_risk_ticker': self.highest_risk_ticker,
            'highest_risk_score': self.highest_risk_score,
            'methodology_notes': self.methodology_notes
        }


@dataclass
class EIMASResult:
    """í†µí•© ì‹¤í–‰ ê²°ê³¼"""
    timestamp: str

    # ë°ì´í„° ìˆ˜ì§‘
    fred_summary: Dict = field(default_factory=dict)
    market_data_count: int = 0
    crypto_data_count: int = 0

    # ë¶„ì„ ê²°ê³¼
    regime: Dict = field(default_factory=dict)
    events_detected: List[Dict] = field(default_factory=list)
    liquidity_signal: str = "NEUTRAL"
    risk_score: float = 0.0

    # ì—ì´ì „íŠ¸ í† ë¡ 
    debate_consensus: Dict = field(default_factory=dict)
    dissent_records: List[Dict] = field(default_factory=list)
    has_strong_dissent: bool = False

    # Dual Mode
    full_mode_position: str = "NEUTRAL"
    reference_mode_position: str = "NEUTRAL"
    modes_agree: bool = True

    # ìµœì¢… ê¶Œê³ 
    final_recommendation: str = "HOLD"
    confidence: float = 0.5
    risk_level: str = "MEDIUM"
    warnings: List[str] = field(default_factory=list)

    # ì‹¤ì‹œê°„ (ì„ íƒ)
    realtime_signals: List[Dict] = field(default_factory=list)

    # Advanced Strategy (Part 2 & 3)
    portfolio_weights: Dict[str, float] = field(default_factory=dict)
    shock_propagation: Dict = field(default_factory=dict)
    integrated_signals: List[Dict] = field(default_factory=list)
    genius_act_regime: str = "NEUTRAL"
    genius_act_signals: List[Dict] = field(default_factory=list)
    whitening_summary: str = ""
    fact_check_grade: str = "N/A"
    theme_etf_analysis: Dict = field(default_factory=dict)

    # Volume Anomaly (Task 4)
    volume_anomalies: List[Dict] = field(default_factory=list)
    volume_analysis_summary: str = ""

    # Market Quality & Bubble Risk (v2.1.1 - API ê²€ì¦ ê²°ê³¼ ë°˜ì˜)
    market_quality: Optional[MarketQualityMetrics] = None
    bubble_risk: Optional[BubbleRiskMetrics] = None

    # Risk Score Transparency (ë¦¬ìŠ¤í¬ ì ìˆ˜ ë¶„í•´)
    base_risk_score: float = 0.0
    microstructure_adjustment: float = 0.0  # Â±10 ë²”ìœ„
    bubble_risk_adjustment: float = 0.0     # multiplier íš¨ê³¼

    # Crypto Stress Test (v2.1.2 - Elicit Enhancement)
    crypto_stress_test: Dict = field(default_factory=dict)

    # Devil's Advocate Summary (v2.1.2 - ë°˜ëŒ€ ë…¼ê±°)
    devils_advocate_arguments: List[str] = field(default_factory=list)

    # HRP Allocation Rationale (v2.1.2 - ë°°ë¶„ ê·¼ê±°)
    hrp_allocation_rationale: str = ""

    # Extended Data Sources (v2.1.3 - DeFiLlama, MENA)
    defi_tvl: Dict = field(default_factory=dict)
    mena_markets: Dict = field(default_factory=dict)
    onchain_risk_signals: List[Dict] = field(default_factory=list)

    # Event Tracking (v2.1.3 - ì´ìƒâ†’ë‰´ìŠ¤ ì—­ì¶”ì )
    event_tracking: Dict = field(default_factory=dict)
    tracked_events: List[Dict] = field(default_factory=list)

    # Adaptive Portfolio (v2.1.3 - ë™ì  í¬íŠ¸í´ë¦¬ì˜¤)
    adaptive_portfolios: Dict = field(default_factory=dict)  # {agent_type: portfolio}
    validation_loop_result: Dict = field(default_factory=dict)

    # Correlation Analysis (v2.1.4 - ìƒê´€ê´€ê³„ íˆíŠ¸ë§µ)
    correlation_matrix: List[List[float]] = field(default_factory=list)  # NxN ìƒê´€ê´€ê³„ ë§¤íŠ¸ë¦­ìŠ¤
    correlation_tickers: List[str] = field(default_factory=list)  # í‹°ì»¤ ëª©ë¡

    def to_dict(self) -> Dict:
        return asdict(self)

    def _generate_potential_concerns(self) -> List[str]:
        """
        ë§Œì¥ì¼ì¹˜ ìƒí™©ì—ì„œë„ íˆ¬ììì—ê²Œ ì œê³µí•  ì ì¬ì  ìš°ë ¤ì‚¬í•­ ìƒì„±

        í˜„ì¬ ë¶„ì„ ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ AIê°€ ê²€í† í•œ ë¦¬ìŠ¤í¬ ìš”ì†Œë“¤ì„ ë°˜í™˜
        """
        concerns = []

        # 1. ë ˆì§ ê¸°ë°˜ ìš°ë ¤ì‚¬í•­
        regime_info = self.regime or {}
        regime = regime_info.get('regime', 'Unknown')
        volatility = regime_info.get('volatility', 'Unknown')

        if regime == 'BULL':
            concerns.append(
                f"í˜„ì¬ Bull ë ˆì§ì´ë‚˜, ê³¼ì—´ ì‹ í˜¸(ê³¼ë§¤ìˆ˜) ì „í™˜ ê°€ëŠ¥ì„± ìƒì‹œ ëª¨ë‹ˆí„°ë§ í•„ìš”"
            )
        elif regime == 'BEAR':
            concerns.append(
                f"Bear ë ˆì§ì—ì„œ ì¶”ê°€ í•˜ë½ ë¦¬ìŠ¤í¬ ì¡´ì¬. ë°©ì–´ì  í¬ì§€ì…˜ ìœ ì§€ ê¶Œê³ "
            )

        # 2. ë¦¬ìŠ¤í¬ ìŠ¤ì½”ì–´ ê¸°ë°˜ ìš°ë ¤ì‚¬í•­
        if self.risk_score < 30:
            concerns.append(
                f"ë¦¬ìŠ¤í¬ ì ìˆ˜ {self.risk_score:.1f}/100ìœ¼ë¡œ ë‚®ì§€ë§Œ, ê¸‰ê²©í•œ ì™¸ë¶€ ì¶©ê²©(ì§€ì •í•™ì  ì´ë²¤íŠ¸ ë“±)ì— ì·¨ì•½í•  ìˆ˜ ìˆìŒ"
            )
        elif self.risk_score > 60:
            concerns.append(
                f"ë¦¬ìŠ¤í¬ ì ìˆ˜ {self.risk_score:.1f}/100ìœ¼ë¡œ ìƒìŠ¹. í¬ì§€ì…˜ ì¶•ì†Œ ë˜ëŠ” í—¤ì§€ ê³ ë ¤"
            )

        # 3. ìœ ë™ì„± ê¸°ë°˜ ìš°ë ¤ì‚¬í•­
        fred_info = self.fred_summary or {}
        rrp = fred_info.get('rrp', 0)
        if rrp and rrp < 100:  # RRP 100B ë¯¸ë§Œ
            concerns.append(
                f"ì—­ë ˆí¬(RRP) ì”ì•¡ ${rrp:.0f}Bë¡œ ê°ì†Œ. ìœ ë™ì„± ì™„ì¶© ì—¬ë ¥ ì¶•ì†Œ ê°€ëŠ¥ì„±"
            )

        # 4. ë²„ë¸” ë¦¬ìŠ¤í¬ ê¸°ë°˜ ìš°ë ¤ì‚¬í•­
        if hasattr(self, 'bubble_risk') and self.bubble_risk:
            bubble_status = getattr(self.bubble_risk, 'overall_status', 'NONE')
            if bubble_status != 'NONE':
                concerns.append(
                    f"ë²„ë¸” ë¦¬ìŠ¤í¬ ìƒíƒœ: {bubble_status}. ê³ í‰ê°€ ìì‚° ë¹„ì¤‘ ì ê²€ í•„ìš”"
                )

        # 5. ëª¨ë“œ ì¼ì¹˜ ì‹œì—ë„ ì‹ ë¢°ë„ ê²½ê³ 
        if self.confidence < 0.7:
            concerns.append(
                f"ë¶„ì„ ì‹ ë¢°ë„ {self.confidence*100:.0f}%ë¡œ ë³´í†µ ìˆ˜ì¤€. ì¶”ê°€ ê²€ì¦ ê¶Œì¥"
            )

        # ê¸°ë³¸ ìš°ë ¤ì‚¬í•­ (í•­ìƒ í¬í•¨)
        if not concerns:
            concerns = [
                "í˜„ì¬ ë¶„ì„ ê¸°ì¤€ìœ¼ë¡œëŠ” ì£¼ìš” ë¦¬ìŠ¤í¬ ìš”ì†Œ ë¯¸íƒì§€. ê·¸ëŸ¬ë‚˜ ì˜ˆì¸¡ ë¶ˆê°€ ì´ë²¤íŠ¸(ë¸”ë™ìŠ¤ì™„)ëŠ” ìƒì‹œ ì¡´ì¬",
                "ê³¼ê±° ë°ì´í„° ê¸°ë°˜ ë¶„ì„ì˜ í•œê³„ ì¸ì‹ í•„ìš”. ì‹œì¥ êµ¬ì¡° ë³€í™” ì‹œ ëª¨ë¸ ì¬ê²€í†  ê¶Œì¥",
                "ë‹¨ê¸° ë³€ë™ì„±ë³´ë‹¤ ì¤‘ì¥ê¸° í€ë”ë©˜í„¸ ë³€í™”ì— ì£¼ëª©í•  ê²ƒ"
            ]

        return concerns[:3]

    def to_markdown(self) -> str:
        """ë§ˆí¬ë‹¤ìš´ í˜•ì‹ ë¦¬í¬íŠ¸ ìƒì„±"""
        md = []
        md.append("# EIMAS Analysis Report")
        md.append(f"**Generated**: {self.timestamp}")
        md.append("")

        # 1. Data Summary
        md.append("## 1. Data Summary")
        md.append("")
        md.append("### FRED Data")
        if self.fred_summary:
            md.append(f"- **RRP**: ${self.fred_summary.get('rrp', 0):.0f}B (Delta: {self.fred_summary.get('rrp_delta', 0):+.0f}B)")
            md.append(f"- **TGA**: ${self.fred_summary.get('tga', 0):.0f}B (Delta: {self.fred_summary.get('tga_delta', 0):+.0f}B)")
            md.append(f"- **Net Liquidity**: ${self.fred_summary.get('net_liquidity', 0):.0f}B")
            md.append(f"- **Liquidity Regime**: {self.fred_summary.get('liquidity_regime', 'N/A')}")
            md.append(f"- **Fed Funds**: {self.fred_summary.get('fed_funds', 0):.2f}%")
            md.append(f"- **10Y-2Y Spread**: {self.fred_summary.get('spread_10y2y', 0):.2f}% ({self.fred_summary.get('curve_status', 'N/A')})")
        else:
            md.append("- No FRED data available")
        md.append("")
        md.append(f"### Market Data")
        md.append(f"- **Tickers collected**: {self.market_data_count}")
        md.append(f"- **Crypto tickers**: {self.crypto_data_count}")
        md.append("")

        # 2. Regime Analysis
        md.append("## 2. Regime Analysis")
        md.append("")
        if self.regime:
            md.append(f"- **Current Regime**: {self.regime.get('regime', 'Unknown')}")
            md.append(f"- **Trend**: {self.regime.get('trend', 'Unknown')}")
            md.append(f"- **Volatility**: {self.regime.get('volatility', 'Unknown')}")
            md.append(f"- **Confidence**: {self.regime.get('confidence', 0):.0%}")
            if self.regime.get('description'):
                md.append(f"- **Description**: {self.regime.get('description')}")
            if self.regime.get('strategy'):
                md.append(f"- **Strategy**: {self.regime.get('strategy')}")

            # GMM & Entropy ë¶„ì„ ê²°ê³¼ (í†µê³„ì  ê³ ë„í™”)
            if self.regime.get('gmm_regime'):
                md.append("")
                md.append("**GMM Statistical Analysis (í†µê³„ì  ë ˆì§ ë¶„ì„):**")
                gmm_probs = self.regime.get('gmm_probabilities', {})
                md.append(f"- **GMM Regime**: {self.regime.get('gmm_regime')}")
                md.append(f"- **Probabilities**: Bull:{gmm_probs.get('Bull', 0):.0%} / Neutral:{gmm_probs.get('Neutral', 0):.0%} / Bear:{gmm_probs.get('Bear', 0):.0%}")
                md.append(f"- **Shannon Entropy**: {self.regime.get('entropy', 0):.3f} ({self.regime.get('entropy_level', 'N/A')})")
                md.append(f"- **Signal Interpretation**: {self.regime.get('entropy_interpretation', 'N/A')}")
        else:
            md.append("- No regime data available")
        md.append("")

        # 3. Risk Assessment
        md.append("## 3. Risk Assessment")
        md.append("")
        md.append(f"- **Risk Score**: {self.risk_score:.1f}/100")
        md.append(f"- **Risk Level**: {self.risk_level}")
        md.append(f"- **Liquidity Signal**: {self.liquidity_signal}")
        md.append("")

        # Risk Score Transparency (ë¦¬ìŠ¤í¬ ì ìˆ˜ ë¶„í•´)
        if self.base_risk_score > 0 or self.microstructure_adjustment != 0 or self.bubble_risk_adjustment != 0:
            md.append("### Risk Score Breakdown")
            md.append("")
            md.append("| Component | Value | Description |")
            md.append("|-----------|-------|-------------|")
            md.append(f"| Base Score | {self.base_risk_score:.1f} | CriticalPath ê¸°ë³¸ ì ìˆ˜ |")
            micro_adj_desc = "ìœ ë™ì„± ìš°ìˆ˜" if self.microstructure_adjustment < 0 else "ìœ ë™ì„± ë¶€ì¡±" if self.microstructure_adjustment > 0 else "ì¤‘ë¦½"
            md.append(f"| Microstructure Adj. | {self.microstructure_adjustment:+.1f} | {micro_adj_desc} |")
            bubble_adj_desc = {0: "ë²„ë¸” ì§•í›„ ì—†ìŒ", 5: "ê´€ì°° í•„ìš”", 10: "ê²½ê³ ", 15: "ìœ„í—˜"}.get(int(self.bubble_risk_adjustment), "N/A")
            md.append(f"| Bubble Risk Adj. | +{self.bubble_risk_adjustment:.0f} | {bubble_adj_desc} |")
            md.append(f"| **Final Score** | **{self.risk_score:.1f}** | |")
            md.append("")

        # 3.1 Market Quality & Bubble Risk (v2.1.1)
        if self.market_quality or self.bubble_risk:
            md.append("### Market Quality & Bubble Risk")
            md.append("")

            # Market Quality Metrics
            if self.market_quality:
                md.append("**Market Microstructure Quality:**")
                md.append(f"- Avg Liquidity Score: {self.market_quality.avg_liquidity_score:.1f}/100")
                if self.market_quality.high_toxicity_tickers:
                    md.append(f"- High Toxicity (VPIN>50%): {', '.join(self.market_quality.high_toxicity_tickers[:5])}")
                if self.market_quality.illiquid_tickers:
                    md.append(f"- Illiquid Tickers: {', '.join(self.market_quality.illiquid_tickers[:5])}")
                md.append(f"- Data Quality: {self.market_quality.data_quality}")
                md.append("")

            # Bubble Risk Metrics
            if self.bubble_risk:
                status_emoji = {"NONE": "ğŸŸ¢", "WATCH": "ğŸŸ¡", "WARNING": "ğŸŸ ", "DANGER": "ğŸ”´", "ERROR": "âš«"}.get(self.bubble_risk.overall_status, "âšª")
                md.append(f"**Bubble Risk Assessment:** {status_emoji} **{self.bubble_risk.overall_status}**")
                md.append("")

                if self.bubble_risk.risk_tickers:
                    md.append("| Ticker | Level | 2Y Run-up | Vol Z-Score | Risk Score |")
                    md.append("|--------|-------|-----------|-------------|------------|")
                    for rt in self.bubble_risk.risk_tickers[:5]:
                        level_emoji = {"WATCH": "ğŸŸ¡", "WARNING": "ğŸŸ ", "DANGER": "ğŸ”´"}.get(rt['level'], "")
                        md.append(f"| {rt['ticker']} | {level_emoji} {rt['level']} | {rt['runup_pct']:+.0f}% | {rt['vol_zscore']:.1f}Ïƒ | {rt['risk_score']:.0f} |")
                    md.append("")

                    if self.bubble_risk.highest_risk_ticker:
                        md.append(f"> **Alert**: {self.bubble_risk.highest_risk_ticker} shows elevated bubble characteristics. Consider hedging or position reduction.")
                        md.append("")

                md.append(f"_Methodology: {self.bubble_risk.methodology_notes}_")
                md.append("")

        # 4. Events Detected
        md.append("## 4. Events Detected")
        md.append("")
        if self.events_detected:
            for event in self.events_detected:
                md.append(f"- **{event.get('type', 'Unknown')}** [{event.get('importance', 'N/A')}]: {event.get('description', '')}")
        else:
            md.append("- No events detected")
        md.append("")

        # 5. Multi-Agent Debate
        md.append("## 5. Multi-Agent Debate")
        md.append("")
        md.append(f"- **FULL Mode Position**: {self.full_mode_position}")
        md.append(f"- **REFERENCE Mode Position**: {self.reference_mode_position}")
        agree_status = "YES" if self.modes_agree else "NO"
        md.append(f"- **Modes Agree**: {agree_status}")
        if self.dissent_records:
            md.append(f"- **Dissent Records**: {len(self.dissent_records)}")
        if self.has_strong_dissent:
            md.append("- **[!] Strong dissent exists - review carefully**")
        md.append("")

        # Devil's Advocate Summary (v2.1.2) - í•­ìƒ ì¶œë ¥
        md.append("### Devil's Advocate (ë°˜ëŒ€ ë…¼ê±°)")
        md.append("")
        if self.devils_advocate_arguments:
            md.append("_í† ë¡  ê³¼ì •ì—ì„œ ì œê¸°ëœ ë°˜ëŒ€ ì˜ê²¬:_")
            md.append("")
            for i, arg in enumerate(self.devils_advocate_arguments[:3], 1):
                md.append(f"- **{i}.** {arg}")
        else:
            # ë§Œì¥ì¼ì¹˜ ì‹œì—ë„ ì ì¬ì  ìš°ë ¤ì‚¬í•­ í‘œì‹œ
            md.append("_í† ë¡  ê²°ê³¼ ë§Œì¥ì¼ì¹˜. ë‹¤ìŒì€ AIê°€ ê²€í† í•œ ì ì¬ì  ìš°ë ¤ì‚¬í•­:_")
            md.append("")
            potential_concerns = self._generate_potential_concerns()
            for i, concern in enumerate(potential_concerns[:3], 1):
                md.append(f"- **{i}.** {concern}")
        md.append("")

        # 6. Advanced Analysis
        md.append("## 6. Advanced Analysis")
        md.append("")

        # Genius Act
        md.append("### Genius Act Macro")
        md.append(f"- **Regime**: {self.genius_act_regime}")
        if self.genius_act_signals:
            md.append(f"- **Signals**: {len(self.genius_act_signals)} detected")
            md.append("")
            md.append("**Signal Details (Why ì„¤ëª… í¬í•¨):**")
            for sig in self.genius_act_signals[:5]:
                if isinstance(sig, dict):
                    strength_val = sig.get('strength', 0)
                    try:
                        strength_fmt = f"{float(strength_val):.2f}"
                    except (ValueError, TypeError):
                        strength_fmt = str(strength_val)
                    md.append(f"- **{sig.get('type', 'N/A')}** (strength: {strength_fmt})")
                    md.append(f"  - Description: {sig.get('description', 'N/A')}")
                    md.append(f"  - Why: {sig.get('why', 'N/A')}")
                    if sig.get('affected_assets'):
                        md.append(f"  - Affected: {', '.join(sig['affected_assets'][:5])}")
        md.append("")

        # Crypto Stress Test (v2.1.2 - Elicit Enhancement) - í•­ìƒ ì¶œë ¥
        md.append("### Crypto Stress Test")
        md.append("")
        if self.crypto_stress_test and not self.crypto_stress_test.get('error'):
            md.append(f"**Scenario**: {self.crypto_stress_test.get('scenario', 'N/A')}")
            md.append("")
            md.append("| Metric | Value |")
            md.append("|--------|-------|")
            md.append(f"| De-peg Probability | **{self.crypto_stress_test.get('depeg_probability_pct', '0.0%')}** |")
            md.append(f"| Estimated Loss under Stress | **${self.crypto_stress_test.get('estimated_loss_under_stress', 0):,.0f}** ({self.crypto_stress_test.get('estimated_loss_pct', '0.0%')}) |")
            md.append(f"| Total Value at Risk | ${self.crypto_stress_test.get('total_value', 0):,.0f} |")
            md.append(f"| Risk Rating | {self.crypto_stress_test.get('risk_rating', 'N/A')} |")
            md.append("")

            # Breakdown by coin
            breakdown = self.crypto_stress_test.get('breakdown_by_coin', [])
            if breakdown:
                md.append("**Breakdown by Stablecoin:**")
                md.append("")
                md.append("| Coin | Amount | De-peg Prob | Expected Loss |")
                md.append("|------|--------|-------------|---------------|")
                for coin in breakdown[:5]:
                    md.append(f"| {coin['ticker']} | ${coin['amount']:,.0f} | {coin['depeg_probability']*100:.1f}% | ${coin['expected_loss']:,.0f} |")
                md.append("")

            methodology = self.crypto_stress_test.get('methodology_note',
                'ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸: ë‹´ë³´ ìœ í˜•ë³„ ë¦¬ìŠ¤í¬ ê°€ì¤‘ì¹˜ ì ìš©. De-peg í™•ë¥  ë° ì˜ˆìƒ ì†ì‹¤ ì‚°ì¶œ.')
            md.append(f"_Methodology: {methodology}_")
        else:
            # ë°ì´í„° ì—†ì„ ë•Œë„ í‘œ êµ¬ì¡° í‘œì‹œ (ê²€ì¦ ì¦ê±°)
            md.append("**Scenario**: Moderate (ì‹ ìš©ìœ„ê¸° ìˆ˜ì¤€)")
            md.append("")
            md.append("| Metric | Value |")
            md.append("|--------|-------|")
            md.append("| De-peg Probability | **0.0%** (ë°ì´í„° ë¯¸ìˆ˜ì§‘) |")
            md.append("| Estimated Loss under Stress | **$0** |")
            md.append("| Total Value at Risk | $0 |")
            md.append("| Risk Rating | N/A |")
            md.append("")
            md.append("_Note: ìŠ¤í…Œì´ë¸”ì½”ì¸ ë°ì´í„°ê°€ ìˆ˜ì§‘ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì „ì²´ ë¶„ì„ ëª¨ë“œë¡œ ì‹¤í–‰í•˜ì„¸ìš”._")
        md.append("")

        # Theme ETF
        if self.theme_etf_analysis:
            md.append("### Theme ETF Analysis")
            md.append(f"- **Theme**: {self.theme_etf_analysis.get('theme', 'N/A')}")
            if self.theme_etf_analysis.get('description'):
                md.append(f"- **Description**: {self.theme_etf_analysis.get('description')}")
            md.append(f"- **Stocks Count**: {self.theme_etf_analysis.get('stocks_count', 0)}")
            top5 = self.theme_etf_analysis.get('top5_concentration', 0)
            try:
                md.append(f"- **Top 5 Concentration**: {float(top5):.1%}")
            except (ValueError, TypeError):
                md.append(f"- **Top 5 Concentration**: {top5}")
            div_score = self.theme_etf_analysis.get('diversification_score', 0)
            try:
                md.append(f"- **Diversification Score**: {float(div_score):.2f}")
            except (ValueError, TypeError):
                md.append(f"- **Diversification Score**: {div_score}")

            # Supply Chain ì¸ê³¼ê´€ê³„ ì„¤ëª…
            supply_chain = self.theme_etf_analysis.get('supply_chain', {})
            if supply_chain:
                md.append("")
                md.append("**Supply Chain Structure:**")
                if supply_chain.get('bottlenecks'):
                    md.append(f"- Bottlenecks: {', '.join(supply_chain['bottlenecks'])}")
                if supply_chain.get('top_central'):
                    md.append(f"- Hub Nodes: {', '.join(supply_chain['top_central'])}")

            # Graph-based Causality Narrative (ìì—°ì–´ ì¸ê³¼ê´€ê³„)
            graph_narrative = self.theme_etf_analysis.get('graph_narrative', '')
            if graph_narrative and graph_narrative != "Not enough correlation data to build causality chain yet.":
                md.append("")
                md.append("**Causality Network Analysis (ì¸ê³¼ê´€ê³„ ë„¤íŠ¸ì›Œí¬):**")
                md.append("")
                md.append(graph_narrative)
                md.append("")
            elif graph_narrative:
                # Fallback message
                md.append("")
                md.append("**Causality Chain (ì¸ê³¼ê´€ê³„):**")
                md.append(f"> {graph_narrative}")
                md.append("")
            else:
                # Legacy causality explanation (í•˜ìœ„ í˜¸í™˜)
                causality = self.theme_etf_analysis.get('causality_explanation', '')
                if causality:
                    md.append("")
                    md.append("**Causality Chain (ì¸ê³¼ê´€ê³„):**")
                    md.append("```")
                    md.append(causality)
                    md.append("```")
            md.append("")

        # Shock Propagation
        if self.shock_propagation:
            md.append("### Shock Propagation")
            md.append(f"- **Nodes**: {self.shock_propagation.get('nodes', 0)}")
            md.append(f"- **Edges**: {self.shock_propagation.get('edges', 0)}")
            if self.shock_propagation.get('critical_path'):
                path_str = ' -> '.join(self.shock_propagation['critical_path'][:5])
                md.append(f"- **Critical Path**: {path_str}")
            md.append("")

        # Portfolio
        if self.portfolio_weights:
            md.append("### GC-HRP Portfolio")
            sorted_weights = sorted(self.portfolio_weights.items(), key=lambda x: x[1], reverse=True)
            md.append("| Ticker | Weight |")
            md.append("|--------|--------|")
            for ticker, weight in sorted_weights[:10]:
                md.append(f"| {ticker} | {weight:.1%} |")
            md.append("")

            # HRP Allocation Rationale (v2.1.2)
            if self.hrp_allocation_rationale:
                md.append(f"**Allocation Rationale**: {self.hrp_allocation_rationale}")
                md.append("")

        # Integrated Signals
        if self.integrated_signals:
            md.append("### Integrated Signals")
            for sig in self.integrated_signals[:5]:
                md.append(f"- **{sig.get('type', 'Unknown')}** [{sig.get('urgency', 'N/A')}]: {sig.get('description', '')[:80]}")
            md.append("")

        # Volume Anomalies
        if self.volume_anomalies:
            md.append("### Volume Anomaly Detection")
            md.append(f"_{self.volume_analysis_summary}_")
            md.append("")
            md.append("| Ticker | Volume Ratio | Severity | Alert |")
            md.append("|--------|--------------|----------|-------|")
            for anomaly in self.volume_anomalies[:10]:
                md.append(f"| {anomaly.get('ticker', 'N/A')} | {anomaly.get('volume_ratio', 0):.1f}x | {anomaly.get('severity', 'N/A')} | {anomaly.get('alert_message', '')[:50]}... |")
            md.append("")

        # 7. Final Recommendation
        md.append("## 7. Final Recommendation")
        md.append("")
        md.append(f"| Item | Value |")
        md.append("|------|-------|")
        md.append(f"| Action | **{self.final_recommendation}** |")
        md.append(f"| Confidence | {self.confidence:.0%} |")
        md.append(f"| Risk Level | {self.risk_level} |")
        md.append("")

        # 8. Warnings
        if self.warnings:
            md.append("## 8. Warnings")
            md.append("")
            for warning in self.warnings:
                md.append(f"- [!] {warning}")
            md.append("")

        # 9. Real-time VPIN Monitoring (if available)
        if self.realtime_signals:
            md.append("## 9. Real-time VPIN Monitoring")
            md.append("")
            md.append("_Binance WebSocketì„ í†µí•œ ì‹¤ì‹œê°„ ì‹œì¥ ë¯¸ì„¸êµ¬ì¡° ë¶„ì„ ê²°ê³¼_")
            md.append("")

            # ì‹¬ë³¼ë³„ ìš”ì•½
            symbol_data = {}
            for sig in self.realtime_signals:
                symbol = sig.get('symbol', 'UNKNOWN')
                if symbol not in symbol_data:
                    symbol_data[symbol] = {'vpins': [], 'max_vpin': 0}
                avg_vpin = sig.get('avg_vpin', 0)
                max_vpin = sig.get('max_vpin', 0)
                symbol_data[symbol]['vpins'].append(avg_vpin)
                symbol_data[symbol]['max_vpin'] = max(symbol_data[symbol]['max_vpin'], max_vpin)

            md.append("### Summary by Symbol")
            md.append("")
            md.append("| Symbol | Avg VPIN | Max VPIN | Samples | Status |")
            md.append("|--------|----------|----------|---------|--------|")

            for symbol, data in symbol_data.items():
                avg = sum(data['vpins']) / len(data['vpins']) if data['vpins'] else 0
                max_v = data['max_vpin']
                samples = len(data['vpins'])

                # ìƒíƒœ íŒì •
                if max_v >= 0.7:
                    status = "ğŸš¨ EXTREME"
                elif max_v >= 0.6:
                    status = "ğŸ”¶ HIGH"
                elif max_v >= 0.5:
                    status = "âš ï¸ ELEVATED"
                else:
                    status = "âœ… NORMAL"

                md.append(f"| {symbol} | {avg:.3f} | {max_v:.3f} | {samples} | {status} |")

            md.append("")

            # ìµœê·¼ ê¸°ë¡ (ìµœëŒ€ 10ê°œ)
            md.append("### Recent 1-min VPIN Records")
            md.append("")
            md.append("| Timestamp | Symbol | Avg VPIN | Max VPIN |")
            md.append("|-----------|--------|----------|----------|")

            for sig in self.realtime_signals[-10:]:
                ts = sig.get('timestamp', 'N/A')
                if isinstance(ts, str) and 'T' in ts:
                    ts = ts.split('T')[1][:8]  # HH:MM:SSë§Œ ì¶”ì¶œ
                symbol = sig.get('symbol', 'N/A')
                avg_vpin = sig.get('avg_vpin', 0)
                max_vpin = sig.get('max_vpin', 0)
                md.append(f"| {ts} | {symbol} | {avg_vpin:.3f} | {max_vpin:.3f} |")

            md.append("")
            md.append("_VPIN Thresholds: Normal(<0.4), Elevated(0.5), High(0.6), Extreme(0.7)_")
            md.append("")

        # 10. Whitening & Fact Check (if available)
        if self.whitening_summary or self.fact_check_grade != "N/A":
            md.append("## 10. Quality Assurance")
            md.append("")
            if self.whitening_summary:
                md.append(f"### Whitening Summary")
                md.append(f"{self.whitening_summary}")
                md.append("")
            if self.fact_check_grade != "N/A":
                md.append(f"### Fact Check Grade: {self.fact_check_grade}")
            md.append("")

        md.append("---")
        md.append("*Generated by EIMAS (Economic Intelligence Multi-Agent System)*")

        return "\n".join(md)


# ============================================================================
# Helper Functions
# ============================================================================

def _get_genius_act_why(signal_type: str, metadata: Dict) -> str:
    """
    Genius Act Signalì˜ ê²½ì œí•™ì  ì´ìœ (Why) ì„¤ëª… ìƒì„±

    ê²½ì œí•™ì  ê·¼ê±°:
    - Genius Act(ìŠ¤í…Œì´ë¸”ì½”ì¸ ê·œì œë²•): ìŠ¤í…Œì´ë¸”ì½”ì¸ ë‹´ë³´ë¡œ ë¯¸êµ­ êµ­ì±„ ìš”êµ¬
    - M = B + SÂ·B* í™•ì¥ ìœ ë™ì„± ê³µì‹ (ìˆœìœ ë™ì„± + ìŠ¤í…Œì´ë¸”ì½”ì¸ ê¸°ì—¬ë„)
    """
    why_map = {
        'stablecoin_surge': (
            "ìŠ¤í…Œì´ë¸”ì½”ì¸(USDT/USDC) ë°œí–‰ëŸ‰ ê¸‰ì¦ â†’ "
            "Genius Act ë‹´ë³´ ìš”ê±´ìœ¼ë¡œ ë¯¸êµ­ êµ­ì±„ ìˆ˜ìš” ìƒìŠ¹ â†’ "
            "êµ­ì±„ ê°€ê²© ê°•ì„¸(ê¸ˆë¦¬ í•˜ë½) ë° í¬ë¦½í†  ë§¤ìˆ˜ ëŒ€ê¸° ìê¸ˆ ì¦ê°€"
        ),
        'stablecoin_drain': (
            "ìŠ¤í…Œì´ë¸”ì½”ì¸ ê³µê¸‰ ê°ì†Œ â†’ "
            "í¬ë¦½í†  ì‹œì¥ì—ì„œ ìê¸ˆ ì´íƒˆ ì‹ í˜¸ â†’ "
            "ë¦¬ìŠ¤í¬ì˜¤í”„ ì „í™˜, í˜„ê¸ˆí™” ì••ë ¥ ì¦ê°€"
        ),
        'rrp_drain': (
            "ì—­ë ˆí¬(RRP) ì”ì•¡ ê°ì†Œ â†’ "
            "ì‹œì¤‘ ìœ ë™ì„± ê³µê¸‰ (B = Fed BS - RRP - TGA ê³µì‹) â†’ "
            "ìœ„í—˜ìì‚°(ì£¼ì‹, í¬ë¦½í† ) ê°•ì„¸ í™˜ê²½ ì¡°ì„±"
        ),
        'tga_drain': (
            "ì¬ë¬´ë¶€ ì¼ë°˜ê³„ì •(TGA) ê°ì†Œ â†’ "
            "ì •ë¶€ ì§€ì¶œë¡œ ì‹œì¤‘ ìœ ë™ì„± ì£¼ì… â†’ "
            "ì†Œë¹„ ë° íˆ¬ì í™•ëŒ€ ê¸°ëŒ€, ì£¼ì‹ ê°•ì„¸"
        ),
        'liquidity_injection': (
            "ìˆœ ìœ ë™ì„±(Net Liquidity) ì¦ê°€ â†’ "
            "Fed BS - RRP - TGA í™•ëŒ€ â†’ "
            "ëª¨ë“  ìœ„í—˜ìì‚°ì— ìš°í˜¸ì  í™˜ê²½"
        ),
        'liquidity_drain': (
            "ìˆœ ìœ ë™ì„± ê°ì†Œ â†’ "
            "ê¸´ì¶• í™˜ê²½, ìì‚° ê°€ê²© í•˜ë½ ì••ë ¥ â†’ "
            "í¬íŠ¸í´ë¦¬ì˜¤ ë°©ì–´ì  ì „í™˜ í•„ìš”"
        ),
        'crypto_risk_on': (
            "í¬ë¦½í†  ë¦¬ìŠ¤í¬ì˜¨ í™˜ê²½ â†’ "
            "ìŠ¤í…Œì´ë¸”ì½”ì¸ ìœ ì… + ìœ ë™ì„± í™•ëŒ€ â†’ "
            "ë¹„íŠ¸ì½”ì¸/ì´ë”ë¦¬ì›€ ìƒìŠ¹ ëª¨ë©˜í…€"
        ),
        'crypto_risk_off': (
            "í¬ë¦½í†  ë¦¬ìŠ¤í¬ì˜¤í”„ í™˜ê²½ â†’ "
            "ìŠ¤í…Œì´ë¸”ì½”ì¸ ì´íƒˆ + ìœ ë™ì„± ì¶•ì†Œ â†’ "
            "ë¹„íŠ¸ì½”ì¸/ì´ë”ë¦¬ì›€ í•˜ë½ ì••ë ¥"
        ),
        'treasury_demand': (
            "êµ­ì±„ ìˆ˜ìš” ì¦ê°€ â†’ "
            "ì•ˆì „ìì‚° ì„ í˜¸ ë˜ëŠ” ìŠ¤í…Œì´ë¸”ì½”ì¸ ë‹´ë³´ ìˆ˜ìš” â†’ "
            "ê¸ˆë¦¬ í•˜ë½, ì„±ì¥ì£¼ ìƒëŒ€ì  ê°•ì„¸"
        ),
        'treasury_supply': (
            "êµ­ì±„ ê³µê¸‰ ì¦ê°€ (ì¬ì •ì ì í™•ëŒ€) â†’ "
            "ê¸ˆë¦¬ ìƒìŠ¹ ì••ë ¥ â†’ "
            "ë°¸ë¥˜ì£¼/ê¸ˆìœµì£¼ ìƒëŒ€ì  ê°•ì„¸, ì„±ì¥ì£¼ ì•½ì„¸"
        ),
    }

    base_why = why_map.get(signal_type, "ê²½ì œí•™ì  ë¶„ì„ ê²°ê³¼ì— ê¸°ë°˜í•œ ì‹œê·¸ë„")

    # ë©”íƒ€ë°ì´í„°ì—ì„œ ì¶”ê°€ ì •ë³´
    if metadata:
        if 'rrp_drain' in metadata:
            base_why += f" (RRP ê°ì†Œ: {metadata['rrp_drain']})"
        if 'total_supply' in metadata:
            base_why += f" (ìŠ¤í…Œì´ë¸”ì½”ì¸ ì´ ê³µê¸‰: {metadata['total_supply']})"

    return base_why


def _generate_hrp_rationale(
    weights: Dict[str, float],
    returns_df: 'pd.DataFrame',
    clusters: List[Dict]
) -> str:
    """
    HRP í¬íŠ¸í´ë¦¬ì˜¤ ë°°ë¶„ ê·¼ê±° ìë™ ìƒì„± (v2.1.2 - Elicit Enhancement)

    ê²½ì œí•™ì  ê·¼ê±°:
    - HRPëŠ” ë‚®ì€ ë³€ë™ì„± ìì‚°ì— ë†’ì€ ë¹„ì¤‘ ë¶€ì—¬ (ì—­ë³€ë™ì„± ê°€ì¤‘)
    - ìƒê´€ê´€ê³„ê°€ ë‚®ì€ ìì‚°ì¼ìˆ˜ë¡ ë¶„ì‚° íš¨ê³¼ ê¸°ì—¬ë„ ë†’ìŒ
    - ë‹¬ëŸ¬(UUP) ë“± ë°©ì–´ ìì‚°ì€ í¬íŠ¸í´ë¦¬ì˜¤ ë³€ë™ì„± ë°©ì–´ ì—­í• 
    """
    if not weights:
        return "No allocation data available"

    import pandas as pd

    # Top 3 ìì‚° ë¶„ì„
    sorted_weights = sorted(weights.items(), key=lambda x: x[1], reverse=True)[:3]
    rationale_parts = []

    # ìì‚°ë³„ íŠ¹ì„± ë¶„ì„
    ASSET_CHARACTERISTICS = {
        'UUP': ('US Dollar', 'volatility hedge, negative equity correlation'),
        'TLT': ('Long Treasury', 'flight-to-quality, duration exposure'),
        'GLD': ('Gold', 'inflation hedge, crisis alpha'),
        'SHY': ('Short Treasury', 'cash proxy, capital preservation'),
        'SPY': ('S&P 500', 'core equity exposure, market beta'),
        'QQQ': ('Nasdaq 100', 'tech/growth exposure, high beta'),
        'IWM': ('Small Cap', 'domestic growth, higher volatility'),
        'EFA': ('Intl Developed', 'geographic diversification'),
        'EEM': ('Emerging Markets', 'growth potential, currency risk'),
        'VNQ': ('REITs', 'real asset exposure, income'),
        'XLE': ('Energy', 'commodity exposure, inflation hedge'),
        'XLF': ('Financials', 'rate sensitivity, economic cycle'),
        'XLK': ('Technology', 'secular growth, momentum'),
        'XLV': ('Healthcare', 'defensive growth, demographics'),
        'BTC-USD': ('Bitcoin', 'digital gold, high volatility'),
        'ETH-USD': ('Ethereum', 'smart contract platform, tech beta'),
    }

    # ê° Top ìì‚°ì˜ ë¹„ì¤‘ ì´ìœ  ë¶„ì„
    for ticker, weight in sorted_weights:
        pct = weight * 100
        asset_name, characteristic = ASSET_CHARACTERISTICS.get(
            ticker, (ticker, 'portfolio diversification')
        )

        # ë³€ë™ì„± ê³„ì‚° (ê°€ëŠ¥í•œ ê²½ìš°)
        vol_comment = ""
        if ticker in returns_df.columns:
            vol = returns_df[ticker].std() * (252 ** 0.5) * 100  # Annualized %
            if vol < 15:
                vol_comment = "low volatility"
            elif vol > 30:
                vol_comment = "high volatility, diversification benefit"
            else:
                vol_comment = "moderate volatility"

        if pct >= 15:
            reason = f"{ticker} ({pct:.0f}%): {characteristic}"
            if vol_comment:
                reason += f" [{vol_comment}]"
            rationale_parts.append(reason)

    # í´ëŸ¬ìŠ¤í„° ì •ë³´ í™œìš©
    cluster_comment = ""
    if clusters:
        n_clusters = len(clusters)
        cluster_comment = f" | {n_clusters} clusters identified for risk parity"

    if rationale_parts:
        return "; ".join(rationale_parts) + cluster_comment
    else:
        return f"Diversified allocation across {len(weights)} assets{cluster_comment}"


def _extract_devils_advocate_arguments(dissent_records: List[Dict]) -> List[str]:
    """
    í† ë¡  ê¸°ë¡ì—ì„œ Devil's Advocate ë…¼ê±° ì¶”ì¶œ (v2.1.2 - Elicit Enhancement)

    ë‹¤ìˆ˜ ì˜ê²¬ì— ëŒ€í•œ ì£¼ìš” ë°˜ëŒ€ ë…¼ê±°ë¥¼ ìš”ì•½í•˜ì—¬ ë°˜í™˜
    """
    if not dissent_records:
        return []

    arguments = []

    # ë°˜ëŒ€ ì˜ê²¬ ì¤‘ ê°€ì¥ ì¤‘ìš”í•œ 3ê°€ì§€ ì¶”ì¶œ
    for record in dissent_records[:5]:
        dissenter = record.get('dissenter', 'Unknown')
        reason = record.get('reason', record.get('dissent_reason', ''))
        confidence = record.get('confidence', 0)

        if reason:
            # ì´ìœ  ìš”ì•½ (ì²« 150ì)
            reason_summary = reason[:150].strip()
            if len(reason) > 150:
                reason_summary += "..."

            argument = f"[{dissenter}] {reason_summary}"
            arguments.append(argument)

    # ìµœëŒ€ 3ê°œ ë°˜í™˜
    return arguments[:3]


# ============================================================================
# Real-time Monitor (Phase 4)
# ============================================================================

class RealtimeVPINMonitor:
    """
    ì‹¤ì‹œê°„ VPIN ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ

    Binance WebSocketìœ¼ë¡œ BTC/ETH ê°€ê²©ì„ ì‹¤ì‹œê°„ ìˆ˜ì‹ í•˜ê³ ,
    1ë¶„ ë‹¨ìœ„ë¡œ VPINì„ ê³„ì‚°í•˜ì—¬ ì„ê³„ì¹˜ ì´ˆê³¼ ì‹œ í„°ë¯¸ë„ ê²½ê³ 

    ì‚¬ìš©ë²•:
        monitor = RealtimeVPINMonitor()
        await monitor.start(duration=60)  # 60ì´ˆ ì‹¤í–‰
    """

    # VPIN ì„ê³„ì¹˜ (ê²½ì œí•™ì  ê·¼ê±°: Easley et al. 2012)
    VPIN_THRESHOLDS = {
        'normal': 0.4,      # ì •ìƒ ë²”ìœ„
        'elevated': 0.5,    # ì£¼ì˜
        'high': 0.6,        # ë†’ìŒ
        'extreme': 0.7      # ê·¹ë‹¨ - ë³€ë™ì„± ê¸‰ì¦ ê°€ëŠ¥
    }

    def __init__(self, symbols: List[str] = None, verbose: bool = True):
        """
        Parameters:
        -----------
        symbols : List[str]
            ëª¨ë‹ˆí„°ë§í•  ì‹¬ë³¼ ë¦¬ìŠ¤íŠ¸ (ê¸°ë³¸: ['BTCUSDT', 'ETHUSDT'])
        verbose : bool
            ìƒì„¸ ì¶œë ¥ ì—¬ë¶€
        """
        self.symbols = symbols or ['BTCUSDT', 'ETHUSDT']
        self.verbose = verbose

        # 1ë¶„ ë‹¨ìœ„ VPIN ì§‘ê³„
        self.minute_vpin: Dict[str, List[float]] = {s: [] for s in self.symbols}
        self.minute_start: Dict[str, datetime] = {}

        # ì•Œë¦¼ ê¸°ë¡ (ì¤‘ë³µ ë°©ì§€)
        self.last_alert_time: Dict[str, datetime] = {}
        self.alert_cooldown = 30  # 30ì´ˆ ì¿¨ë‹¤ìš´

        # í†µê³„
        self.alerts_fired = 0
        self.vpin_history: Dict[str, List[Dict]] = {s: [] for s in self.symbols}

    def _log(self, msg: str, level: str = 'info'):
        """ë¡œê¹…"""
        if not self.verbose:
            return

        timestamp = datetime.now().strftime('%H:%M:%S')

        if level == 'alert':
            print(f"\nğŸš¨ [{timestamp}] {msg}")
        elif level == 'warning':
            print(f"âš ï¸  [{timestamp}] {msg}")
        elif level == 'success':
            print(f"âœ… [{timestamp}] {msg}")
        else:
            print(f"   [{timestamp}] {msg}")

    def _check_vpin_threshold(self, symbol: str, vpin: float) -> Optional[str]:
        """VPIN ì„ê³„ì¹˜ í™•ì¸ ë° ê²½ê³  ë ˆë²¨ ë°˜í™˜"""
        if vpin >= self.VPIN_THRESHOLDS['extreme']:
            return 'extreme'
        elif vpin >= self.VPIN_THRESHOLDS['high']:
            return 'high'
        elif vpin >= self.VPIN_THRESHOLDS['elevated']:
            return 'elevated'
        return None

    def _should_alert(self, symbol: str) -> bool:
        """ì¿¨ë‹¤ìš´ í™•ì¸ - ì¤‘ë³µ ì•Œë¦¼ ë°©ì§€"""
        now = datetime.now()
        last = self.last_alert_time.get(symbol)

        if last is None:
            return True

        elapsed = (now - last).total_seconds()
        return elapsed >= self.alert_cooldown

    def _fire_alert(self, symbol: str, vpin: float, level: str):
        """í„°ë¯¸ë„ ê²½ê³  ì¶œë ¥"""
        if not self._should_alert(symbol):
            return

        self.last_alert_time[symbol] = datetime.now()
        self.alerts_fired += 1

        # ë ˆë²¨ë³„ ë©”ì‹œì§€
        level_info = {
            'elevated': ('âš ï¸ ELEVATED', 'Yellow', 'ì£¼ì˜ í•„ìš”'),
            'high': ('ğŸ”¶ HIGH', 'Orange', 'ë³€ë™ì„± ìƒìŠ¹ ì˜ˆìƒ'),
            'extreme': ('ğŸš¨ EXTREME', 'Red', 'ê¸‰ë³€ë™ ì„ë°•!')
        }

        icon, color, desc = level_info.get(level, ('âš ï¸', 'Unknown', ''))

        alert_msg = f"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  {icon} VPIN ALERT - {symbol}
â•‘--------------------------------------------------------------â•‘
â•‘  VPIN Value: {vpin:.3f} ({level.upper()})
â•‘  Threshold:  {self.VPIN_THRESHOLDS.get(level, 0.5):.2f}
â•‘  Message:    {desc}
â•‘--------------------------------------------------------------â•‘
â•‘  Action: í¬ì§€ì…˜ ì ê²€, ì†ì ˆ ë¼ì¸ í™•ì¸, ë³€ë™ì„± ëŒ€ë¹„ í•„ìš”
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""
        print(alert_msg)

    def _on_metrics(self, metrics):
        """ë©”íŠ¸ë¦­ ìˆ˜ì‹  ì½œë°± - 1ë¶„ ë‹¨ìœ„ VPIN ì§‘ê³„"""
        symbol = metrics.symbol
        vpin = metrics.vpin
        now = datetime.now()

        # 1ë¶„ ìœˆë„ìš° ì‹œì‘
        if symbol not in self.minute_start or self.minute_start[symbol] is None:
            self.minute_start[symbol] = now

        # VPIN ê°’ ìˆ˜ì§‘
        self.minute_vpin[symbol].append(vpin)

        # 1ë¶„ ê²½ê³¼ ì‹œ ì§‘ê³„
        elapsed = (now - self.minute_start[symbol]).total_seconds()
        if elapsed >= 60:
            # 1ë¶„ í‰ê·  VPIN ê³„ì‚°
            if self.minute_vpin[symbol]:
                avg_vpin = sum(self.minute_vpin[symbol]) / len(self.minute_vpin[symbol])
                max_vpin = max(self.minute_vpin[symbol])
                min_vpin = min(self.minute_vpin[symbol])

                # íˆìŠ¤í† ë¦¬ ì €ì¥
                self.vpin_history[symbol].append({
                    'timestamp': now.isoformat(),
                    'avg_vpin': avg_vpin,
                    'max_vpin': max_vpin,
                    'min_vpin': min_vpin,
                    'samples': len(self.minute_vpin[symbol])
                })

                # 1ë¶„ ìš”ì•½ ì¶œë ¥
                self._log(
                    f"{symbol} 1-min VPIN: avg={avg_vpin:.3f}, "
                    f"max={max_vpin:.3f}, min={min_vpin:.3f} "
                    f"({len(self.minute_vpin[symbol])} samples)",
                    level='info'
                )

                # ì„ê³„ì¹˜ í™•ì¸
                alert_level = self._check_vpin_threshold(symbol, max_vpin)
                if alert_level:
                    self._fire_alert(symbol, max_vpin, alert_level)

            # ë¦¬ì…‹
            self.minute_vpin[symbol] = []
            self.minute_start[symbol] = now

        # ì‹¤ì‹œê°„ ì„ê³„ì¹˜ ì²´í¬ (ê·¹ë‹¨ì  ê²½ìš° ì¦‰ì‹œ ì•Œë¦¼)
        if vpin >= self.VPIN_THRESHOLDS['extreme']:
            self._fire_alert(symbol, vpin, 'extreme')

    def _on_alert(self, alert_type: str, alert_data: Dict):
        """BinanceStreamer ì•Œë¦¼ ì½œë°±"""
        if alert_type == 'vpin_high':
            self._log(f"Stream Alert: {alert_data.get('message', '')}", level='warning')

    async def start(self, duration: int = 60):
        """
        ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ì‹œì‘

        Parameters:
        -----------
        duration : int
            ëª¨ë‹ˆí„°ë§ ì‹œê°„ (ì´ˆ, ê¸°ë³¸ 60ì´ˆ)
        """
        print("\n" + "=" * 70)
        print("  EIMAS Real-time VPIN Monitor")
        print("=" * 70)
        print(f"  Symbols: {', '.join(self.symbols)}")
        print(f"  Duration: {duration}s")
        print(f"  VPIN Thresholds: {self.VPIN_THRESHOLDS}")
        print("=" * 70 + "\n")

        # StreamConfig ì„¤ì •
        config = StreamConfig(
            symbols=self.symbols,
            depth_levels=10,
            update_speed='100ms',
            include_trades=True,
            alert_vpin_threshold=self.VPIN_THRESHOLDS['elevated']
        )

        # BinanceStreamer ìƒì„±
        streamer = BinanceStreamer(
            config=config,
            on_metrics=self._on_metrics,
            on_alert=self._on_alert,
            verbose=False  # ìì²´ ë¡œê¹… ì‚¬ìš©
        )

        self._log(f"Connecting to Binance WebSocket...", level='info')

        try:
            # ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘
            await streamer.start(duration_seconds=duration)
        except KeyboardInterrupt:
            self._log("Interrupted by user", level='warning')
        except Exception as e:
            self._log(f"Error: {e}", level='warning')
        finally:
            streamer.stop()

        # ìš”ì•½ ì¶œë ¥
        self._print_summary(streamer)

        return {
            'alerts_fired': self.alerts_fired,
            'vpin_history': self.vpin_history,
            'stream_stats': streamer.stats.to_dict()
        }

    def _print_summary(self, streamer):
        """ì‹¤í–‰ ìš”ì•½ ì¶œë ¥"""
        print("\n" + "=" * 70)
        print("  Real-time Monitor Summary")
        print("=" * 70)

        stats = streamer.stats.to_dict()
        print(f"  Duration: {stats['elapsed_seconds']:.1f}s")
        print(f"  Messages: {stats['messages_received']:,}")
        print(f"  Alerts Fired: {self.alerts_fired}")

        for symbol in self.symbols:
            history = self.vpin_history.get(symbol, [])
            if history:
                avg_vpins = [h['avg_vpin'] for h in history]
                overall_avg = sum(avg_vpins) / len(avg_vpins)
                overall_max = max(h['max_vpin'] for h in history)

                print(f"\n  [{symbol}]")
                print(f"    1-min Samples: {len(history)}")
                print(f"    Overall Avg VPIN: {overall_avg:.4f}")
                print(f"    Overall Max VPIN: {overall_max:.4f}")

                # ìµœì¢… ìƒíƒœ íŒì •
                if overall_max >= self.VPIN_THRESHOLDS['extreme']:
                    print(f"    Status: ğŸš¨ EXTREME - ê³ ìœ„í—˜")
                elif overall_max >= self.VPIN_THRESHOLDS['high']:
                    print(f"    Status: ğŸ”¶ HIGH - ì£¼ì˜")
                elif overall_max >= self.VPIN_THRESHOLDS['elevated']:
                    print(f"    Status: âš ï¸ ELEVATED - ê´€ì°°")
                else:
                    print(f"    Status: âœ… NORMAL - ì•ˆì •")

        print("=" * 70)


async def run_realtime_monitor(
    symbols: List[str] = None,
    duration: int = 60,
    verbose: bool = True
) -> Dict:
    """
    ì‹¤ì‹œê°„ VPIN ëª¨ë‹ˆí„°ë§ ì‹¤í–‰ (ë…ë¦½ í•¨ìˆ˜)

    Parameters:
    -----------
    symbols : List[str]
        ëª¨ë‹ˆí„°ë§í•  ì‹¬ë³¼ (ê¸°ë³¸: BTC, ETH)
    duration : int
        ì‹¤í–‰ ì‹œê°„ (ì´ˆ)
    verbose : bool
        ìƒì„¸ ì¶œë ¥

    Returns:
    --------
    Dict with monitoring results

    ì‚¬ìš© ì˜ˆ:
        # ê¸°ë³¸ ì‹¤í–‰ (BTC, ETH 60ì´ˆ)
        result = await run_realtime_monitor()

        # ì»¤ìŠ¤í…€ ì„¤ì •
        result = await run_realtime_monitor(
            symbols=['BTCUSDT', 'ETHUSDT', 'SOLUSDT'],
            duration=120
        )
    """
    symbols = symbols or ['BTCUSDT', 'ETHUSDT']
    monitor = RealtimeVPINMonitor(symbols=symbols, verbose=verbose)
    return await monitor.start(duration=duration)


# ============================================================================
# Main Pipeline
# ============================================================================

async def run_integrated_pipeline(
    enable_realtime: bool = False,
    realtime_duration: int = 30,
    quick_mode: bool = False,
    output_dir: str = 'outputs',
    cron_mode: bool = False
) -> EIMASResult:
    """
    EIMAS í†µí•© íŒŒì´í”„ë¼ì¸ ì‹¤í–‰

    Args:
        enable_realtime: ì‹¤ì‹œê°„ Binance ìŠ¤íŠ¸ë¦¬ë° í™œì„±í™”
        realtime_duration: ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ì‹œê°„ (ì´ˆ)
        output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬
        cron_mode: ì„œë²„ ìë™í™” ëª¨ë“œ
        quick_mode: ë¹ ë¥¸ ë¶„ì„ ëª¨ë“œ (ì¼ë¶€ ìƒëµ)

    Returns:
        EIMASResult: í†µí•© ë¶„ì„ ê²°ê³¼
    """
    start_time = datetime.now()
    result = EIMASResult(timestamp=start_time.isoformat())

    print("=" * 70)
    print("  EIMAS - Integrated Analysis Pipeline")
    print("=" * 70)
    print(f"  Mode: {'Quick' if quick_mode else 'Full'}")
    print(f"  Realtime: {'Enabled' if enable_realtime else 'Disabled'}")
    print("=" * 70)

    # ========================================================================
    # Phase 1: Data Collection
    # ========================================================================
    print("\n" + "=" * 50)
    print("PHASE 1: DATA COLLECTION")
    print("=" * 50)

    # 1.1 FRED ë°ì´í„° ìˆ˜ì§‘
    print("\n[1.1] Collecting FRED data...")
    try:
        fred = FREDCollector()
        fred_summary = fred.collect_all()
        result.fred_summary = {
            'rrp': fred_summary.rrp,
            'rrp_delta': fred_summary.rrp_delta,
            'tga': fred_summary.tga,
            'tga_delta': fred_summary.tga_delta,
            'fed_assets': fred_summary.fed_assets,
            'net_liquidity': fred_summary.net_liquidity,
            'liquidity_regime': fred_summary.liquidity_regime,
            'fed_funds': fred_summary.fed_funds,
            'treasury_10y': fred_summary.treasury_10y,
            'spread_10y2y': fred_summary.spread_10y2y,
            'curve_status': fred_summary.curve_status,
        }
        print(f"      âœ“ RRP: ${fred_summary.rrp:.0f}B (Î”{fred_summary.rrp_delta:+.0f}B)")
        print(f"      âœ“ TGA: ${fred_summary.tga:.0f}B (Î”{fred_summary.tga_delta:+.0f}B)")
        print(f"      âœ“ Net Liquidity: ${fred_summary.net_liquidity:.0f}B ({fred_summary.liquidity_regime})")
        print(f"      âœ“ Curve: {fred_summary.curve_status} (10Y-2Y: {fred_summary.spread_10y2y:.2f}%)")
    except Exception as e:
        print(f"      âœ— FRED error: {e}")
        fred_summary = None

    # 1.2 ì‹œì¥ ë°ì´í„° ìˆ˜ì§‘
    print("\n[1.2] Collecting market data...")
    try:
        dm = DataManager(lookback_days=365 if not quick_mode else 90)
        # í™•ì¥ëœ í‹°ì»¤ ëª©ë¡: ë” ë§ì€ ì¢…ëª©ìœ¼ë¡œ ê±°ë˜ëŸ‰ ì´ìƒ íƒì§€ ê°œì„ 
        tickers_config = {
            'market': [
                # ì£¼ìš” ì§€ìˆ˜ ETF
                {'ticker': 'SPY'}, {'ticker': 'QQQ'}, {'ticker': 'IWM'},
                {'ticker': 'DIA'}, {'ticker': 'TLT'}, {'ticker': 'GLD'},
                {'ticker': 'USO'}, {'ticker': 'UUP'}, {'ticker': '^VIX'},
                # ì„¹í„° ETF (ê±°ë˜ëŸ‰ ì´ìƒ íƒì§€ í™•ëŒ€)
                {'ticker': 'XLK'},   # Technology
                {'ticker': 'XLF'},   # Financials
                {'ticker': 'XLE'},   # Energy
                {'ticker': 'XLV'},   # Healthcare
                {'ticker': 'XLI'},   # Industrials
                # ë°˜ë„ì²´ ë° AI ê´€ë ¨ (Theme ETF ì—°ë™)
                {'ticker': 'SMH'},   # VanEck Semiconductor
                {'ticker': 'SOXX'},  # iShares Semiconductor
                # ì±„ê¶Œ ETF
                {'ticker': 'HYG'},   # High Yield
                {'ticker': 'LQD'},   # Investment Grade
                {'ticker': 'TIP'},   # TIPS
            ],
            'crypto': [
                {'ticker': 'BTC-USD'}, {'ticker': 'ETH-USD'}
            ],
            # RWA (Real World Asset) - í† í°í™” ìì‚° [NEW]
            # ê²½ì œí•™ì  ê·¼ê±°: "Assetì´ infinite... ëª¨ë“  ê±°ë˜ ê°€ëŠ¥í•œ ê±¸ í† í°í™”"
            # Note: ONDO-USD, PAXG-USDëŠ” yfinance í˜¸í™˜ í˜•ì‹ (crypto tokens)
            'rwa': [
                {'ticker': 'ONDO-USD'},   # US Treasuries Tokenized Protocol
                {'ticker': 'PAXG-USD'},   # Gold Tokenized (1 token = 1 oz Gold)
                {'ticker': 'COIN'},       # Crypto Infrastructure Proxy (ì£¼ì‹)
            ]
        }
        market_data, macro_data = dm.collect_all(tickers_config)
        result.market_data_count = len(market_data)
        print(f"      âœ“ Collected {len(market_data)} tickers")
    except Exception as e:
        print(f"      âœ— Market data error: {e}")
        market_data = {}
        macro_data = None

    # 1.3 ì•”í˜¸í™”í ë° RWA ë°ì´í„° (DataManagerê°€ ì´ë¯¸ ìˆ˜ì§‘)
    print("\n[1.3] Crypto & RWA data collected with market data...")
    crypto_tickers = ['BTC-USD', 'ETH-USD']
    rwa_tickers = ['ONDO-USD', 'PAXG-USD', 'COIN']
    result.crypto_data_count = sum(1 for t in crypto_tickers if t in market_data)
    rwa_count = sum(1 for t in rwa_tickers if t in market_data)
    print(f"      âœ“ Crypto: {result.crypto_data_count} tickers")
    print(f"      âœ“ RWA (Tokenized Assets): {rwa_count} tickers")

    # 1.4 ì‹œì¥ ì§€í‘œ
    if not quick_mode:
        print("\n[1.4] Collecting market indicators...")
        try:
            indicators = MarketIndicatorsCollector()
            indicators_summary = indicators.collect_all()
            print(f"      âœ“ VIX: {indicators_summary.vix.current:.2f}")
            print(f"      âœ“ Fear & Greed: {indicators_summary.vix.fear_greed_level}")
        except Exception as e:
            print(f"      âœ— Indicators error: {e}")

    # 1.5 í™•ì¥ ë°ì´í„° ì†ŒìŠ¤ (DeFiLlama, MENA)
    if not quick_mode:
        print("\n[1.5] Extended data sources (DeFi, MENA)...")
        try:
            ext_collector = ExtendedDataCollector()

            # DeFi TVL
            defi_summary = ext_collector.defi.get_summary()
            result.defi_tvl = {
                'total_tvl': defi_summary.get('total_tvl', 0),
                'stablecoin_mcap': defi_summary.get('stablecoin_market_cap', 0),
                'top_stablecoins': defi_summary.get('top_stablecoins', [])
            }
            print(f"      âœ“ DeFi TVL: ${defi_summary.get('total_tvl', 0)/1e9:.2f}B")
            print(f"      âœ“ Stablecoin MCap: ${defi_summary.get('stablecoin_market_cap', 0)/1e9:.2f}B")

            # MENA Markets
            mena_summary = ext_collector.mena.get_performance_summary()
            result.mena_markets = mena_summary
            if mena_summary.get('etfs'):
                avg_return = mena_summary.get('avg_return_1m', 0)
                print(f"      âœ“ MENA ETFs: {len(mena_summary['etfs'])} tracked")
                print(f"      âœ“ MENA Avg 1M Return: {avg_return:+.1f}%")

            # On-Chain ë¦¬ìŠ¤í¬ ì‹œê·¸ë„
            onchain_signals = ext_collector.get_risk_signals()
            result.onchain_risk_signals = onchain_signals
            if onchain_signals:
                print(f"      âœ“ On-Chain Risk Signals: {len(onchain_signals)}")
                for sig in onchain_signals[:2]:
                    print(f"        - [{sig.get('severity')}] {sig.get('message', '')[:50]}")

        except Exception as e:
            print(f"      âœ— Extended data error: {e}")

    # 1.6 ìƒê´€ê´€ê³„ ë§¤íŠ¸ë¦­ìŠ¤ ê³„ì‚°
    print("\n[1.6] Calculating correlation matrix...")
    correlation_matrix = []
    correlation_tickers = []
    try:
        if market_data:
            # ê° í‹°ì»¤ì˜ Close ê°€ê²©ì„ í•˜ë‚˜ì˜ DataFrameìœ¼ë¡œ í•©ì¹˜ê¸°
            price_data = {}
            for ticker, df in market_data.items():
                if isinstance(df, pd.DataFrame) and 'Close' in df.columns:
                    price_data[ticker] = df['Close']

            if price_data:
                # DataFrameìœ¼ë¡œ ë³€í™˜
                prices_df = pd.DataFrame(price_data)

                # ê²°ì¸¡ì¹˜ ì²˜ë¦¬ (forward fill)
                prices_df = prices_df.fillna(method='ffill').dropna()

                # ìƒê´€ê´€ê³„ ë§¤íŠ¸ë¦­ìŠ¤ ê³„ì‚°
                corr_df = prices_df.corr()
                correlation_matrix = corr_df.values.tolist()
                correlation_tickers = corr_df.columns.tolist()

                print(f"      âœ“ Correlation matrix: {len(correlation_tickers)}x{len(correlation_tickers)}")

                # ê°€ì¥ ë†’ì€ ìƒê´€ê´€ê³„ ìŒ ì¶œë ¥
                corr_values = []
                for i in range(len(correlation_tickers)):
                    for j in range(i+1, len(correlation_tickers)):
                        corr_values.append((correlation_tickers[i], correlation_tickers[j], corr_df.iloc[i, j]))

                if corr_values:
                    # ì ˆëŒ€ê°’ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
                    corr_values.sort(key=lambda x: abs(x[2]), reverse=True)
                    top_corr = corr_values[0]
                    print(f"      âœ“ Strongest correlation: {top_corr[0]} â†” {top_corr[1]}: {top_corr[2]:.3f}")
    except Exception as e:
        print(f"      âœ— Correlation calculation error: {e}")
        correlation_matrix = []
        correlation_tickers = []

    # ========================================================================
    # Phase 2: Analysis
    # ========================================================================
    print("\n" + "=" * 50)
    print("PHASE 2: ANALYSIS")
    print("=" * 50)

    # 2.1 ë ˆì§ íƒì§€
    print("\n[2.1] Detecting market regime...")
    try:
        regime_detector = RegimeDetector(ticker='SPY')  # tickerëŠ” ìƒì„±ìì—ì„œ
        regime_result = regime_detector.detect()  # ì¸ì ì—†ì´ í˜¸ì¶œ
        result.regime = {
            'regime': regime_result.regime.value if hasattr(regime_result.regime, 'value') else str(regime_result.regime),
            'trend': regime_result.trend_state.value if hasattr(regime_result.trend_state, 'value') else str(regime_result.trend_state),
            'volatility': regime_result.volatility_state.value if hasattr(regime_result.volatility_state, 'value') else str(regime_result.volatility_state),
            'confidence': regime_result.confidence / 100 if regime_result.confidence > 1 else regime_result.confidence,
            'description': regime_result.description,
            'strategy': regime_result.strategy,
        }
        print(f"      âœ“ Regime: {result.regime['regime']}")
        print(f"      âœ“ Trend: {result.regime['trend']}, Volatility: {result.regime['volatility']}")
        print(f"      âœ“ Confidence: {result.regime['confidence']:.0%}")
    except Exception as e:
        print(f"      âœ— Regime error: {e}")

    # 2.1.1 GMM & Entropy ê¸°ë°˜ ë ˆì§ ë¶„ì„ (í†µê³„ì  ê³ ë„í™”)
    # ê²½ì œí•™ì  ê·¼ê±°: "GMM(Gaussian Mixture Model)ì„ ì¨ì•¼ í•¨", "ì—”íŠ¸ë¡œí”¼ë¡œ ë¶ˆí™•ì‹¤ì„± ì¸¡ì •"
    gmm_result = None
    if market_data and not quick_mode:
        print("\n[2.1.1] GMM & Entropy regime analysis...")
        try:
            gmm_summary = get_gmm_regime_summary(market_data)
            result.regime['gmm_regime'] = gmm_summary['regime']
            result.regime['gmm_probabilities'] = gmm_summary['probabilities']
            result.regime['entropy'] = gmm_summary['entropy']
            result.regime['entropy_level'] = gmm_summary['entropy_level']
            result.regime['entropy_interpretation'] = gmm_summary['interpretation']
            result.regime['gmm_report_line'] = gmm_summary['report_line']

            print(f"      âœ“ GMM Regime: {gmm_summary['regime']}")
            probs = gmm_summary['probabilities']
            print(f"      âœ“ Probabilities: Bull:{probs.get('Bull', 0):.0%} / Neutral:{probs.get('Neutral', 0):.0%} / Bear:{probs.get('Bear', 0):.0%}")
            print(f"      âœ“ Shannon Entropy: {gmm_summary['entropy']:.3f} ({gmm_summary['entropy_level']})")
            print(f"      âœ“ Interpretation: {gmm_summary['interpretation']}")
        except Exception as e:
            print(f"      â–³ GMM analysis (optional): {e}")

    # 2.2 ì´ë²¤íŠ¸ íƒì§€
    print("\n[2.2] Detecting events...")
    try:
        event_detector = QuantitativeEventDetector()

        # ìœ ë™ì„± ì´ë²¤íŠ¸
        if fred_summary:
            liquidity_data = {
                'rrp': fred_summary.rrp,
                'rrp_delta': fred_summary.rrp_delta,
                'tga': fred_summary.tga,
                'tga_delta': fred_summary.tga_delta,
                'net_liquidity': fred_summary.net_liquidity,
            }
            liquidity_events = event_detector.detect_liquidity_events(liquidity_data)
            result.events_detected.extend([{
                'type': e.event_type.value,
                'importance': e.importance.value,
                'description': e.description
            } for e in liquidity_events])

            if liquidity_events:
                for e in liquidity_events:
                    print(f"      âš  {e.event_type.value}: {e.description}")
            else:
                print("      âœ“ No liquidity events detected")

        # ì‹œì¥ ì´ë²¤íŠ¸
        if fred_summary and market_data:
            market_events_data = {
                'vix': market_data.get('VIX', {}).get('Close', [0])[-1] if 'VIX' in market_data else 0,
                'spread_10y2y': fred_summary.spread_10y2y,
                'hy_oas': fred_summary.hy_oas,
            }
            # Additional market event detection can go here

    except Exception as e:
        print(f"      âœ— Event detection error: {e}")

    # 2.3 ìœ ë™ì„± ë¶„ì„ (Granger Causality)
    if not quick_mode:
        print("\n[2.3] Liquidity-Market causality analysis...")
        try:
            liquidity_analyzer = LiquidityMarketAnalyzer()
            liquidity_signal = liquidity_analyzer.generate_signals()
            result.liquidity_signal = liquidity_signal.get('signal', 'NEUTRAL')
            print(f"      âœ“ Liquidity Signal: {result.liquidity_signal}")
            if liquidity_signal.get('causality_results'):
                for var, strength in liquidity_signal['causality_results'].items():
                    print(f"        - {var}: {strength:.2f}")
        except Exception as e:
            print(f"      âœ— Liquidity analysis error: {e}")

    # 2.4 Critical Path ë¶„ì„
    print("\n[2.4] Critical path analysis...")
    try:
        critical_path = CriticalPathAggregator()
        if market_data:
            cp_result = critical_path.analyze(market_data)
            # CriticalPathResultëŠ” dataclassì´ë¯€ë¡œ getattr ì‚¬ìš©
            result.risk_score = getattr(cp_result, 'total_risk_score', 0)
            risk_level = getattr(cp_result, 'risk_level', 'Unknown')
            print(f"      âœ“ Risk Score: {result.risk_score:.1f}/100")
            print(f"      âœ“ Risk Level: {risk_level}")
            print(f"      âœ“ Primary Risk Path: {getattr(cp_result, 'primary_risk_path', 'N/A')}")

            # ê²½ê³  ì¶”ê°€
            if result.risk_score > 50:
                result.warnings.append(f"High risk score: {result.risk_score:.1f}")
    except Exception as e:
        print(f"      âœ— Critical path error: {e}")

    # 2.4.1 Market Microstructure Risk Enhancement (API ê²€ì¦: Option C)
    if not quick_mode and market_data:
        print("\n[2.4.1] Microstructure risk enhancement...")
        try:
            micro_analyzer = DailyMicrostructureAnalyzer()
            micro_results = micro_analyzer.analyze_multiple(market_data)

            # MarketQualityMetrics êµ¬ì„±
            liquidity_scores = {}
            high_toxicity = []
            illiquid_tickers = []

            for ticker, micro_result in micro_results.items():
                # ìœ ë™ì„± ì ìˆ˜ (0-100 ìŠ¤ì¼€ì¼ë¡œ ë³€í™˜)
                liq_score = getattr(micro_result, 'overall_liquidity_score', 50)
                liquidity_scores[ticker] = liq_score

                # VPIN ê¸°ë°˜ ë…ì„± ì²´í¬
                vpin_result = getattr(micro_result, 'vpin', None)
                if vpin_result and hasattr(vpin_result, 'vpin') and vpin_result.vpin > 0.5:
                    high_toxicity.append(ticker)

                # ìœ ë™ì„± ë‚®ì€ ì¢…ëª© (ì ìˆ˜ 30 ì´í•˜)
                if liq_score < 30:
                    illiquid_tickers.append(ticker)

            avg_liq = sum(liquidity_scores.values()) / len(liquidity_scores) if liquidity_scores else 50

            result.market_quality = MarketQualityMetrics(
                avg_liquidity_score=avg_liq,
                liquidity_scores=liquidity_scores,
                high_toxicity_tickers=high_toxicity,
                illiquid_tickers=illiquid_tickers,
                data_quality="COMPLETE" if len(micro_results) == len(market_data) else "PARTIAL"
            )

            # ë§ˆì´í¬ë¡œìŠ¤íŠ¸ëŸ­ì²˜ ì¡°ì • ê³„ì‚° (Â±10 ë²”ìœ„)
            # í‰ê·  ìœ ë™ì„±ì´ ë‚®ìœ¼ë©´ ë¦¬ìŠ¤í¬ ì¦ê°€, ë†’ìœ¼ë©´ ê°ì†Œ
            micro_adjustment = (50 - avg_liq) / 5  # 50ì  ê¸°ì¤€, 10ì  ì°¨ì´ë‹¹ Â±2
            micro_adjustment = max(-10, min(10, micro_adjustment))  # Â±10 í´ë¨í•‘
            result.microstructure_adjustment = micro_adjustment

            print(f"      âœ“ Avg Liquidity Score: {avg_liq:.1f}/100")
            print(f"      âœ“ High Toxicity Tickers: {len(high_toxicity)} ({', '.join(high_toxicity[:3]) if high_toxicity else 'None'})")
            print(f"      âœ“ Risk Adjustment: {micro_adjustment:+.1f}")

        except Exception as e:
            print(f"      âœ— Microstructure analysis error: {e}")
            result.market_quality = MarketQualityMetrics(data_quality="DEGRADED")

    # 2.4.2 Bubble Risk Overlay (Greenwood-Shleifer ê¸°ë°˜)
    if not quick_mode:
        print("\n[2.4.2] Bubble risk overlay...")
        try:
            bubble_detector = BubbleDetector()
            tickers_to_check = list(market_data.keys()) if market_data else []

            # ë²„ë¸” ë¶„ì„ ì‹¤í–‰
            bubble_results = {}
            for ticker in tickers_to_check:
                try:
                    df = market_data.get(ticker)
                    if df is not None and not df.empty:
                        bubble_results[ticker] = bubble_detector.analyze(ticker, df)
                except Exception as e:
                    logger.debug(f"Bubble analysis skipped for {ticker}: {e}")

            # BubbleRiskMetrics êµ¬ì„±
            risk_tickers = []
            highest_risk_ticker = ""
            highest_risk_score = 0.0
            overall_status = "NONE"

            level_priority = {
                BubbleWarningLevel.NONE: 0,
                BubbleWarningLevel.WATCH: 1,
                BubbleWarningLevel.WARNING: 2,
                BubbleWarningLevel.DANGER: 3
            }

            for ticker, bubble_result in bubble_results.items():
                level = bubble_result.bubble_warning_level
                score = bubble_result.risk_score

                if level != BubbleWarningLevel.NONE:
                    risk_tickers.append({
                        'ticker': ticker,
                        'level': level.value,
                        'runup_pct': round(bubble_result.runup.cumulative_return * 100, 1),
                        'vol_zscore': round(bubble_result.volatility.zscore, 2) if bubble_result.volatility else 0,
                        'risk_score': round(score, 1)
                    })

                if score > highest_risk_score:
                    highest_risk_score = score
                    highest_risk_ticker = ticker

                # ì „ì²´ ìƒíƒœ ì—…ë°ì´íŠ¸ (ê°€ì¥ ë†’ì€ ìˆ˜ì¤€ìœ¼ë¡œ)
                if level_priority.get(level, 0) > level_priority.get(BubbleWarningLevel[overall_status], 0):
                    overall_status = level.value

            # ìœ„í—˜ë„ìˆœ ì •ë ¬
            risk_tickers.sort(key=lambda x: x['risk_score'], reverse=True)

            result.bubble_risk = BubbleRiskMetrics(
                overall_status=overall_status,
                risk_tickers=risk_tickers[:5],  # Top 5ë§Œ ì €ì¥
                highest_risk_ticker=highest_risk_ticker,
                highest_risk_score=highest_risk_score,
                methodology_notes="Greenwood-Shleifer 2019: Run-up + Volatility + Issuance"
            )

            # ë²„ë¸” ë¦¬ìŠ¤í¬ ì¡°ì • ê³„ì‚° (multiplier íš¨ê³¼)
            # DANGER: +15, WARNING: +10, WATCH: +5
            bubble_adjustment = 0
            if overall_status == "DANGER":
                bubble_adjustment = 15
            elif overall_status == "WARNING":
                bubble_adjustment = 10
            elif overall_status == "WATCH":
                bubble_adjustment = 5
            result.bubble_risk_adjustment = bubble_adjustment

            # ìµœì¢… ë¦¬ìŠ¤í¬ ì ìˆ˜ ì—…ë°ì´íŠ¸ (Base + Micro + Bubble)
            result.base_risk_score = result.risk_score
            adjusted_risk = result.risk_score + result.microstructure_adjustment + bubble_adjustment
            result.risk_score = max(0, min(100, adjusted_risk))

            print(f"      âœ“ Overall Bubble Status: {overall_status}")
            print(f"      âœ“ Risk Tickers: {len(risk_tickers)} detected")
            if risk_tickers:
                top_risk = risk_tickers[0]
                print(f"      âœ“ Highest Risk: {top_risk['ticker']} ({top_risk['level']}, {top_risk['runup_pct']:+.0f}% run-up)")
            print(f"      âœ“ Bubble Adjustment: +{bubble_adjustment}")
            print(f"      âœ“ Final Risk Score: {result.base_risk_score:.1f} â†’ {result.risk_score:.1f}")

        except Exception as e:
            print(f"      âœ— Bubble detection error: {e}")
            result.bubble_risk = BubbleRiskMetrics(overall_status="ERROR")

    # 2.5 ETF Flow ë¶„ì„
    if not quick_mode:
        print("\n[2.5] ETF flow analysis...")
        try:
            etf_analyzer = ETFFlowAnalyzer()
            etf_result = etf_analyzer.analyze()
            print(f"      âœ“ Sector Rotation: {etf_result.get('rotation_signal', 'N/A')}")
            print(f"      âœ“ Style: {etf_result.get('style_signal', 'N/A')}")
        except Exception as e:
            print(f"      âœ— ETF flow error: {e}")

    # 2.6 Genius Act Macro (ìŠ¤í…Œì´ë¸”ì½”ì¸-ìœ ë™ì„± ë¶„ì„)
    if not quick_mode and fred_summary:
        print("\n[2.6] Genius Act Macro analysis...")
        try:
            from lib.genius_act_macro import StablecoinDataCollector

            # ìŠ¤í…Œì´ë¸”ì½”ì¸ ë°ì´í„° ìˆ˜ì§‘ (7ì¼ ë¸íƒ€ ê³„ì‚°)
            stablecoin_collector = StablecoinDataCollector()
            stablecoin_data = stablecoin_collector.fetch_stablecoin_supply(lookback_days=14)
            stablecoin_comment = stablecoin_collector.generate_detailed_comment(stablecoin_data)

            # ìŠ¤í…Œì´ë¸”ì½”ì¸ ì‹œê°€ì´ì•¡ì—ì„œ ê³µê¸‰ëŸ‰ ì¶”ì¶œ (ì‹­ì–µ ë‹¬ëŸ¬)
            usdt_current = stablecoin_data.get('USDT', {}).get('current', 140)
            usdc_current = stablecoin_data.get('USDC', {}).get('current', 45)
            dai_current = stablecoin_data.get('DAI', {}).get('current', 5)

            usdt_week_ago = stablecoin_data.get('USDT', {}).get('week_ago', 140)
            usdc_week_ago = stablecoin_data.get('USDC', {}).get('week_ago', 45)
            dai_week_ago = stablecoin_data.get('DAI', {}).get('week_ago', 5)

            # í˜„ì¬ ì§€í‘œ êµ¬ì„± (ì‹¤ì œ ìŠ¤í…Œì´ë¸”ì½”ì¸ ë°ì´í„° ì‚¬ìš©)
            current_liq = LiquidityIndicators(
                fed_balance_sheet=fred_summary.fed_assets / 1000 if fred_summary.fed_assets else 7.5,
                rrp_balance=fred_summary.rrp / 1000 if fred_summary.rrp else 0.5,
                tga_balance=fred_summary.tga / 1000 if fred_summary.tga else 0.5,
                usdt_supply=usdt_current,
                usdc_supply=usdc_current,
                dai_supply=dai_current,
            )
            # ì´ì „ ì§€í‘œ (7ì¼ ì „ ë°ì´í„° ì‚¬ìš©)
            previous_liq = LiquidityIndicators(
                fed_balance_sheet=current_liq.fed_balance_sheet,
                rrp_balance=current_liq.rrp_balance - (fred_summary.rrp_delta / 1000 if fred_summary.rrp_delta else 0),
                tga_balance=current_liq.tga_balance - (fred_summary.tga_delta / 1000 if fred_summary.tga_delta else 0),
                usdt_supply=usdt_week_ago,
                usdc_supply=usdc_week_ago,
                dai_supply=dai_week_ago,
            )

            genius_strategy = GeniusActMacroStrategy()
            genius_result = genius_strategy.analyze(current_liq, previous_liq)

            result.genius_act_regime = genius_result['regime']

            # ì‹œê·¸ë„ì„ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜í•˜ë©´ì„œ Why(ì´ìœ ) ì„¤ëª… í¬í•¨
            signals_with_why = []
            for sig in genius_result['signals']:
                # ì‹œê·¸ë„ì´ ê°ì²´ì¸ì§€ dictì¸ì§€ í™•ì¸ í›„ ì²˜ë¦¬
                if hasattr(sig, 'signal_type'):
                    # MacroSignal ê°ì²´ì¸ ê²½ìš°
                    sig_type = sig.signal_type.value
                    sig_desc = sig.description
                    sig_strength = sig.strength
                    sig_confidence = sig.confidence
                    sig_assets = sig.affected_assets
                    sig_metadata = sig.metadata if hasattr(sig, 'metadata') else {}
                elif isinstance(sig, dict):
                    # ì´ë¯¸ dictì¸ ê²½ìš°
                    sig_type = sig.get('type', sig.get('signal_type', 'unknown'))
                    sig_desc = sig.get('description', 'N/A')
                    sig_strength = sig.get('strength', 0)
                    sig_confidence = sig.get('confidence', 0)
                    sig_assets = sig.get('affected_assets', [])
                    sig_metadata = sig.get('metadata', {})
                else:
                    continue  # ì•Œ ìˆ˜ ì—†ëŠ” í˜•ì‹ ìŠ¤í‚µ

                why_explanation = _get_genius_act_why(sig_type, sig_metadata)

                signals_with_why.append({
                    'type': sig_type,
                    'description': sig_desc,
                    'why': why_explanation,
                    'strength': sig_strength,
                    'confidence': sig_confidence,
                    'affected_assets': sig_assets,
                    'metadata': sig_metadata
                })

            # ìŠ¤í…Œì´ë¸”ì½”ì¸ ìƒì„¸ ì½”ë©˜íŠ¸ë¥¼ ì‹œê·¸ë„ì— ì¶”ê°€
            signals_with_why.append({
                'type': 'stablecoin_analysis',
                'description': stablecoin_comment['detailed_comment'],
                'why': stablecoin_comment['economic_interpretation'],
                'strength': abs(stablecoin_comment['total_delta_pct']) / 10,
                'confidence': 0.8,
                'affected_assets': ['BTC-USD', 'ETH-USD', 'TLT', 'SHY'],
                'metadata': {
                    'total_market_cap': stablecoin_comment['total_market_cap'],
                    'total_delta_7d': stablecoin_comment['total_delta_7d'],
                    'total_delta_pct': stablecoin_comment['total_delta_pct'],
                    'genius_act_status': stablecoin_comment['genius_act_status'],
                    'components': stablecoin_comment['components']
                }
            })
            result.genius_act_signals = signals_with_why

            print(f"      âœ“ Regime: {genius_result['regime']}")
            print(f"      âœ“ Signals: {len(genius_result['signals'])} detected")
            print(f"      âœ“ Stablecoin: {stablecoin_comment['detailed_comment']}")
            print(f"        â†’ Total Market Cap: ${stablecoin_comment['total_market_cap']:.1f}B")
            print(f"        â†’ 7-Day Delta: ${stablecoin_comment['total_delta_7d']:+.1f}B ({stablecoin_comment['total_delta_pct']:+.1f}%)")
            print(f"        â†’ Genius Act Status: {stablecoin_comment['genius_act_status'].upper()}")

            # ì‹œê·¸ë„ë³„ Why ì¶œë ¥
            for sig in signals_with_why[:3]:
                print(f"        â†’ {sig['type']}: {sig['description'][:80]}")
                print(f"          Why: {sig['why'][:100]}...")

            if genius_result['positions']:
                print(f"      âœ“ Positions: {len(genius_result['positions'])} recommended")

        except Exception as e:
            print(f"      âœ— Genius Act error: {e}")

    # 2.6.1 Crypto Stress Test (v2.1.2 - Elicit Enhancement) - ë…ë¦½ ì‹¤í–‰
    if not quick_mode and fred_summary:
        print("\n[2.6.1] Crypto Stress Test...")
        try:
            crypto_evaluator = CryptoRiskEvaluator()
            # ìŠ¤í…Œì´ë¸”ì½”ì¸ ì‹œê°€ì´ì•¡ ë°ì´í„° ì§ì ‘ ìˆ˜ì§‘
            from lib.genius_act_macro import StablecoinDataCollector
            stablecoin_collector = StablecoinDataCollector()
            stablecoin_data = stablecoin_collector.fetch_stablecoin_supply(lookback_days=7)

            # ìŠ¤í…Œì´ë¸”ì½”ì¸ í™€ë”© ì¶”ì • (ì‹œê°€ì´ì•¡ ê¸°ì¤€, ì‹­ì–µ ë‹¬ëŸ¬ -> ë‹¬ëŸ¬)
            stablecoin_holdings = {
                'USDT': stablecoin_data.get('USDT', {}).get('current', 140) * 1e9,
                'USDC': stablecoin_data.get('USDC', {}).get('current', 45) * 1e9,
                'DAI': stablecoin_data.get('DAI', {}).get('current', 5) * 1e9,
            }
            stress_result = crypto_evaluator.run_stress_test(
                stablecoin_holdings=stablecoin_holdings,
                stress_scenario='moderate'
            )
            result.crypto_stress_test = stress_result
            print(f"      âœ“ Scenario: {stress_result.get('scenario', 'N/A')}")
            print(f"      âœ“ De-peg Probability: {stress_result.get('depeg_probability_pct', 'N/A')}")
            print(f"      âœ“ Estimated Loss: ${stress_result.get('estimated_loss_under_stress', 0):,.0f}")
            print(f"      âœ“ Risk Rating: {stress_result.get('risk_rating', 'N/A')}")
        except Exception as e:
            print(f"      âœ— Crypto Stress Test error: {e}")
            result.crypto_stress_test = {'error': str(e)}

    # 2.7 Theme ETF Analysis (Supply Chain Causality í¬í•¨)
    if not quick_mode:
        print("\n[2.7] Theme ETF analysis...")
        try:
            from lib.custom_etf_builder import SupplyChainGraph

            etf_builder = CustomETFBuilder()
            ai_etf = etf_builder.create_etf(ThemeCategory.AI_SEMICONDUCTOR)
            risk_analysis = etf_builder.analyze_risk_concentration(ai_etf)

            # Supply Chain ì¸ê³¼ê´€ê³„ ë¶„ì„
            supply_chain = SupplyChainGraph(ai_etf.stocks)
            layer_dist = supply_chain.get_layer_distribution()
            bottlenecks = supply_chain.find_bottlenecks()
            centrality = supply_chain.get_centrality_scores()

            # ì‹œì¥ ë°ì´í„°ì—ì„œ ê°€ê²© ë³€ë™ë¥  ì¶”ì¶œ (ì¸ê³¼ê´€ê³„ ìƒì„±ìš©)
            price_changes = {}
            for ticker, df in market_data.items() if market_data else []:
                if hasattr(df, 'pct_change') and len(df) > 1:
                    try:
                        price_changes[ticker] = float(df['Close'].pct_change().iloc[-1] * 100)
                    except:
                        pass

            # ë™ì  ì¸ê³¼ê´€ê³„ ì²´ì¸ ìƒì„± (ê·¸ë˜í”„ ê¸°ë°˜)
            causality_chains = supply_chain.generate_causality_chain(
                event='AI Demand Surge',  # ê¸°ë³¸ ì´ë²¤íŠ¸
                source_node='NVDA',        # ì£¼ìš” ë…¸ë“œ
                market_data=price_changes  # ì‹¤ì œ ì‹œì¥ ë°ì´í„° ë°˜ì˜
            )

            # LLM ê¸°ë°˜ Narrative ìƒì„± (Rule-based ëª¨ë“œ)
            from lib.causality_narrative import CausalityNarrativeGenerator
            narrative_gen = CausalityNarrativeGenerator(use_llm=False)  # Rule-based ì‚¬ìš©
            causality_insights = narrative_gen.generate_rule_based(
                bottlenecks=bottlenecks,
                hub_nodes=[t[0] for t in sorted(centrality.items(), key=lambda x: x[1], reverse=True)[:3]],
                supply_chain_layers=layer_dist,
                external_shock='AI Demand Surge',
                market_data=price_changes
            )

            # ì¸ê³¼ê´€ê³„ ì„¤ëª… í¬ë§·íŒ… (Narrative í˜•ì‹)
            causality_explanation = []
            causality_explanation.append("=== Supply Chain Causality (ì¸ê³¼ê´€ê³„) ===")

            # Narrative Insights ì¶”ê°€ (LLM/Rule-based ìƒì„±)
            causality_explanation.append("\n[Causality Insights (ì¸ê³¼ê´€ê³„ ë¶„ì„)]")
            for insight in causality_insights[:3]:
                causality_explanation.append(f"\n**Path:** {insight.path}")
                causality_explanation.append(f"**Insight:** {insight.insight}")
                causality_explanation.append(f"(Causality: {insight.causality_type.upper()}, Confidence: {insight.confidence:.0%})")

            # ë™ì  ìƒì„±ëœ ì¸ê³¼ê´€ê³„ ì²´ì¸ ì¶”ê°€
            causality_explanation.append("\n\n[ì¸ê³¼ê´€ê³„ ì²´ì¸ (Event â†’ Node â†’ Impact)]")
            for chain in causality_chains[:5]:  # ìµœëŒ€ 5ê°œ
                causality_explanation.append(f"â€¢ {chain}")

            # ë ˆì´ì–´ë³„ ì „íŒŒ ê²½ë¡œ ì„¤ëª…
            causality_explanation.append("\n[ì „íŒŒ ê²½ë¡œ] EQUIPMENT â†’ MANUFACTURER â†’ INTEGRATOR â†’ END_USER")

            # ë³‘ëª© ì§€ì  ì„¤ëª…
            if bottlenecks:
                causality_explanation.append(f"\n[ë³‘ëª© ì§€ì ] {', '.join(bottlenecks)}")
                causality_explanation.append("â€¢ ì´ ì¢…ëª©ë“¤ì´ íƒ€ê²© ë°›ìœ¼ë©´ ì „ì²´ ê³µê¸‰ë§ì— ì¶©ê²© ì „íŒŒ")

            # í•µì‹¬ ì¢…ëª© (ì¤‘ì‹¬ì„± ê¸°ì¤€)
            top_central = sorted(centrality.items(), key=lambda x: x[1], reverse=True)[:3]
            if top_central:
                causality_explanation.append(f"\n[í•µì‹¬ í—ˆë¸Œ] {', '.join([t[0] for t in top_central])}")
                causality_explanation.append("â€¢ ê³µê¸‰ë§ ë„¤íŠ¸ì›Œí¬ì—ì„œ ê°€ì¥ ì¤‘ìš”í•œ ìœ„ì¹˜ (PageRank + Betweenness)")

            # ì¶©ê²© ì „íŒŒ ì‹œë®¬ë ˆì´ì…˜ (NVDA ì˜ˆì‹œ)
            if 'NVDA' in [s.ticker for s in ai_etf.stocks]:
                propagation = supply_chain.get_shock_propagation_path('NVDA')
                if propagation:
                    prop_path = ' â†’ '.join([p[0] for p in propagation[:4]])
                    causality_explanation.append(f"\n[ì¶©ê²© ì „íŒŒ ì˜ˆì‹œ] NVDA í•˜ë½ ì‹œ: {prop_path}")

            # === CausalityGraphEngine í†µí•© (ê³ ê¸‰ ê·¸ë˜í”„ ë¶„ì„) ===
            graph_engine_insights = []
            shock_simulation_results = {}
            try:
                # ê·¸ë˜í”„ ì—”ì§„ ì´ˆê¸°í™” (LLM ë¹„í™œì„±í™” - Rule-based ë¨¼ì €)
                graph_engine = CausalityGraphEngine(use_llm=False)

                # ê³µê¸‰ë§ ë°ì´í„°ë¡œ ê·¸ë˜í”„ êµ¬ì¶•
                graph_engine.build_from_supply_chain(
                    supply_chain_layers=layer_dist,
                    stock_info={s.ticker: {'name': s.name, 'sector': getattr(s, 'sector', '')}
                               for s in ai_etf.stocks}
                )

                # ì‹œì¥ ë°ì´í„°ë¡œ ìƒê´€ê´€ê³„/Granger ì—£ì§€ ì¶”ê°€
                if market_data:
                    graph_engine.build_from_market_data(
                        market_data=market_data,
                        correlation_threshold=0.5,
                        granger_pvalue_threshold=0.05
                    )

                # ë³‘ëª©ì  ì‹ë³„ (ê³ ê¸‰)
                graph_bottlenecks = graph_engine.identify_bottlenecks(top_n=5)

                # ì¶©ê²© ì „íŒŒ ì‹œë®¬ë ˆì´ì…˜ (ê° ì£¼ìš” ë…¸ë“œì—ì„œ)
                for bn in graph_bottlenecks[:3]:
                    impacts = graph_engine.simulate_shock_propagation(bn.id, shock_magnitude=-0.10)
                    shock_simulation_results[bn.id] = impacts

                # ì¸ì‚¬ì´íŠ¸ ìƒì„± (ë¹„ë™ê¸° - ì´ë¯¸ ì‹¤í–‰ì¤‘ì¸ ë£¨í”„ ì²˜ë¦¬)
                try:
                    loop = asyncio.get_running_loop()
                    # ì´ë¯¸ ë£¨í”„ê°€ ì‹¤í–‰ì¤‘ì´ë©´ ìƒˆ íƒœìŠ¤í¬ë¡œ ì²˜ë¦¬
                    import concurrent.futures
                    with concurrent.futures.ThreadPoolExecutor() as pool:
                        graph_engine_insights = pool.submit(
                            asyncio.run, graph_engine.generate_insights(max_insights=5)
                        ).result(timeout=30)
                except RuntimeError:
                    # ë£¨í”„ê°€ ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±
                    graph_engine_insights = asyncio.run(graph_engine.generate_insights(max_insights=5))

                # ìì—°ì–´ Narrative ìƒì„± (ë¦¬í¬íŠ¸ìš©)
                graph_narrative = graph_engine.generate_report_narrative(
                    external_shock='AI Demand Surge',
                    include_shock_sim=True
                )

                print(f"      âœ“ Graph Engine: {len(graph_engine.nodes)} nodes, {len(graph_engine.edges)} edges")
                print(f"      âœ“ Advanced Bottlenecks: {', '.join([bn.id for bn in graph_bottlenecks])}")
                print(f"      âœ“ Narrative Generated: {len(graph_narrative)} chars")

            except Exception as graph_err:
                print(f"      â–³ Graph Engine (optional): {graph_err}")
                graph_narrative = "Not enough correlation data to build causality chain yet."

            result.theme_etf_analysis = {
                'theme': 'AI_SEMICONDUCTOR',
                'description': ai_etf.description,
                'stocks_count': len(ai_etf.stocks),
                'top5_concentration': risk_analysis['top5_concentration'],
                'diversification_score': risk_analysis['diversification_score'],
                'warnings': risk_analysis['risk_warnings'],
                'supply_chain': {
                    'layers': {k: v for k, v in layer_dist.items()},
                    'bottlenecks': bottlenecks,
                    'top_central': [t[0] for t in top_central] if top_central else [],
                },
                'causality_explanation': '\n'.join(causality_explanation),
                'causality_insights': [i.to_dict() for i in causality_insights],  # LLM/Rule-based ì¸ì‚¬ì´íŠ¸
                # CausalityGraphEngine ê²°ê³¼ (ê³ ê¸‰ ë¶„ì„)
                'graph_engine_insights': [i.to_dict() for i in graph_engine_insights] if graph_engine_insights else [],
                'shock_simulation': shock_simulation_results,
                # ë¦¬í¬íŠ¸ìš© ìì—°ì–´ Narrative
                'graph_narrative': graph_narrative
            }

            print(f"      âœ“ Theme: {ai_etf.name}")
            print(f"      âœ“ Stocks: {len(ai_etf.stocks)}, Diversification: {risk_analysis['diversification_score']}")
            print(f"      âœ“ Bottlenecks: {', '.join(bottlenecks) if bottlenecks else 'None'}")
            print(f"      âœ“ Hub Nodes: {', '.join([t[0] for t in top_central]) if top_central else 'N/A'}")
            print(f"      âœ“ Causality Insights: {len(causality_insights)} generated")
            # ì²« ë²ˆì§¸ ì¸ì‚¬ì´íŠ¸ ì¶œë ¥
            if causality_insights:
                first = causality_insights[0]
                print(f"        â†’ Path: {first.path[:60]}...")
                print(f"        â†’ Insight: {first.insight[:80]}...")
        except Exception as e:
            print(f"      âœ— Theme ETF error: {e}")

    # 2.8 Shock Propagation Graph (ì¸ê³¼ê´€ê³„ ë¶„ì„)
    if not quick_mode and market_data:
        print("\n[2.8] Shock propagation analysis...")
        try:
            # ìˆ˜ìµë¥  ë°ì´í„° ì¤€ë¹„
            import pandas as pd
            returns_dict = {}
            for ticker, df in market_data.items():
                if hasattr(df, 'pct_change') and len(df) > 20:
                    returns_dict[ticker] = df['Close'].pct_change().dropna()

            if len(returns_dict) >= 3:
                returns_df = pd.DataFrame(returns_dict).dropna()

                if len(returns_df) > 60:
                    shock_graph = ShockPropagationGraph()
                    shock_graph.build_from_returns(returns_df)

                    # find_critical_path() ëŠ” sourceë¥¼ í•„ìš”ë¡œ í•˜ë¯€ë¡œ, ì£¼ìš” ë…¸ë“œì—ì„œ ê²½ë¡œ íƒìƒ‰
                    # ë…¸ë“œê°€ ë§ìœ¼ë©´ ì²« ë²ˆì§¸ ë…¸ë“œë¥¼ sourceë¡œ ì‚¬ìš©
                    source_node = list(shock_graph.graph.nodes())[0] if shock_graph.graph.nodes() else None
                    if source_node:
                        critical_path = shock_graph.find_critical_path(source=source_node)

                        if critical_path:
                            # ShockPath ê°ì²´ëŠ” ë°ì´í„°í´ë˜ìŠ¤ì´ë¯€ë¡œ ì†ì„± ì§ì ‘ ì ‘ê·¼
                            result.shock_propagation = {
                                'nodes': len(shock_graph.graph.nodes()),
                                'edges': len(shock_graph.graph.edges()),
                                'critical_path': critical_path.path,
                                'total_lag': critical_path.total_lag
                            }

                            print(f"      âœ“ Nodes: {result.shock_propagation['nodes']}, Edges: {result.shock_propagation['edges']}")
                            if critical_path.path:
                                path_str = ' â†’ '.join(critical_path.path[:5])
                                print(f"      âœ“ Critical Path: {path_str}")
                    else:
                        print(f"      â–³ No nodes in shock graph")
        except Exception as e:
            print(f"      âœ— Shock propagation error: {e}")

    # 2.9 Graph-Clustered Portfolio (GC-HRP)
    if not quick_mode and market_data:
        print("\n[2.9] Graph-Clustered Portfolio optimization...")
        try:
            import pandas as pd
            # ìˆ˜ìµë¥  ë°ì´í„° ì¤€ë¹„
            returns_dict = {}
            for ticker, df in market_data.items():
                if hasattr(df, 'pct_change') and len(df) > 20:
                    returns_dict[ticker] = df['Close'].pct_change().dropna()

            if len(returns_dict) >= 3:
                returns_df = pd.DataFrame(returns_dict).dropna()

                if len(returns_df) > 60:
                    gc_portfolio = GraphClusteredPortfolio(
                        correlation_threshold=0.3,
                        clustering_method=ClusteringMethod.KMEANS,
                        max_representatives_per_cluster=2
                    )
                    allocation = gc_portfolio.fit(returns_df)

                    result.portfolio_weights = allocation.weights
                    print(f"      âœ“ Clusters: {len(allocation.clusters)}")
                    print(f"      âœ“ Diversification Ratio: {allocation.diversification_ratio:.2f}")
                    print(f"      âœ“ Effective N: {allocation.effective_n:.1f}")

                    # Top 5 weights ì¶œë ¥
                    sorted_weights = sorted(allocation.weights.items(), key=lambda x: x[1], reverse=True)[:5]
                    weights_str = ', '.join([f"{t}:{w:.1%}" for t, w in sorted_weights])
                    print(f"      âœ“ Top Weights: {weights_str}")

                    # HRP Allocation Rationale (v2.1.2 - Elicit Enhancement)
                    result.hrp_allocation_rationale = _generate_hrp_rationale(
                        allocation.weights, returns_df, allocation.clusters
                    )
                    print(f"      âœ“ Rationale: {result.hrp_allocation_rationale[:80]}...")
        except Exception as e:
            print(f"      âœ— GC-HRP error: {e}")

    # 2.10 Integrated Strategy (Portfolio + Causality)
    if not quick_mode and market_data and fred_summary:
        print("\n[2.10] Integrated Strategy analysis...")
        try:
            import pandas as pd
            # ìˆ˜ìµë¥  ë°ì´í„° ì¤€ë¹„
            returns_dict = {}
            for ticker, df in market_data.items():
                if hasattr(df, 'pct_change') and len(df) > 20:
                    returns_dict[ticker] = df['Close'].pct_change().dropna()

            if len(returns_dict) >= 3:
                returns_df = pd.DataFrame(returns_dict).dropna()

                # ë§¤í¬ë¡œ ë°ì´í„° êµ¬ì„±
                macro_df = pd.DataFrame({
                    'FED_FUNDS': [fred_summary.fed_funds] * len(returns_df),
                    'VIX': [indicators_summary.vix.current if 'indicators_summary' in dir() else 15.0] * len(returns_df),
                }, index=returns_df.index)

                if len(returns_df) > 60:
                    integrated = IntegratedStrategy(
                        correlation_threshold=0.3,
                        clustering_method=ClusteringMethod.KMEANS,
                        leading_tilt_factor=0.15,
                        volume_surge_threshold=3.0
                    )

                    recommendation = integrated.fit(returns_df, macro_df)

                    # ì‹œê·¸ë„ ì €ì¥
                    result.integrated_signals = [
                        {
                            'type': s.signal_type.value,
                            'source': s.source,
                            'urgency': s.urgency,
                            'description': s.description[:100],
                            'action': s.action_suggested.value
                        }
                        for s in recommendation.signals[:10]
                    ]

                    print(f"      âœ“ Signals: {len(recommendation.signals)} generated")
                    print(f"      âœ“ Leading Exposure: {recommendation.leading_exposure:.1%}")
                    print(f"      âœ“ Shock Vulnerability: {recommendation.shock_vulnerability:.1%}")

                    if recommendation.warnings:
                        for w in recommendation.warnings[:2]:
                            print(f"      âš  {w[:80]}")
        except Exception as e:
            print(f"      âœ— Integrated Strategy error: {e}")

    # 2.11 Volume Anomaly Detection (ì •ë³´ ë¹„ëŒ€ì¹­ íƒì§€)
    if not quick_mode and market_data:
        print("\n[2.11] Volume anomaly detection...")
        try:
            # ë¯¼ê°ë„ ì¡°ì •: 2.5ë°°(MEDIUM), 4.0ë°°(HIGH) - ë” ë§ì€ ì´ìƒ íƒì§€
            # ì†ŒìŠ¤ ì´ë¡ : "20ì¼ ì´í‰ì„  ëŒ€ë¹„ 3~5ë°° ê±°ë˜ëŸ‰ì€ ì‚¬ì  ì •ë³´ ìœ ì…" (Kyle, 1985)
            volume_analyzer = VolumeAnalyzer(
                lookback_period=20,
                surge_threshold_medium=2.5,  # 2.5ë°° = MEDIUM (ë¯¼ê°ë„ ìƒí–¥)
                surge_threshold_high=4.0,    # 4.0ë°° = HIGH (ë¯¼ê°ë„ ìƒí–¥)
                verbose=False
            )
            vol_result = volume_analyzer.detect_anomalies(market_data)

            # ê²°ê³¼ ì €ì¥
            result.volume_anomalies = [a.to_dict() for a in vol_result.anomalies[:10]]
            result.volume_analysis_summary = vol_result.summary

            print(f"      âœ“ Analyzed: {vol_result.total_tickers_analyzed} tickers")
            print(f"      âœ“ Anomalies: {vol_result.anomalies_detected} detected")
            print(f"      âœ“ High severity: {vol_result.high_severity_count}")

            # ëª…ì‹œì  ë©”ì‹œì§€: ì´ìƒì´ ì—†ì„ ë•Œë„ ìƒíƒœ ì¶œë ¥
            if vol_result.anomalies_detected == 0:
                print(f"      âœ“ Volume profile is normal (No asymmetric info detected)")
                print(f"        â†’ All tickers within normal range (<2.5x MA20)")
                print(f"        â†’ Kyle(1985): No evidence of Private Information Inflow")

            # ê³ ì‹¬ê°ë„ ê²½ê³ ë¥¼ Eventsë¡œ ì¶”ê°€ (Private Information Inflow)
            high_severity = volume_analyzer.filter_by_severity(vol_result, "HIGH")
            for anomaly in high_severity[:5]:
                print(f"      âš  {anomaly.alert_message}")
                result.warnings.append(f"Volume Alert: {anomaly.ticker} - {anomaly.volume_ratio:.1f}x surge detected")

                # Events Detectedì— ì¶”ê°€
                event_desc = (
                    f"Private Information Inflow Detected: {anomaly.ticker} "
                    f"({anomaly.volume_ratio:.1f}x avg volume, price {anomaly.price_change_1d:+.1f}%). "
                    f"Kyle(1985): ê±°ë˜ëŸ‰ ê¸‰ì¦ì€ ì‚¬ì  ì •ë³´ ìœ ì… ì‹ í˜¸."
                )
                result.events_detected.append({
                    'type': 'VOLUME_ANOMALY',
                    'importance': anomaly.severity,
                    'description': event_desc,
                    'ticker': anomaly.ticker,
                    'volume_ratio': anomaly.volume_ratio,
                    'info_type': anomaly.information_type.value if hasattr(anomaly.information_type, 'value') else str(anomaly.information_type)
                })

            # MEDIUM severityë„ ì´ë²¤íŠ¸ë¡œ ì¶”ê°€ (ì •ë³´ìš©)
            medium_severity = [a for a in vol_result.anomalies if a.severity == "MEDIUM"]
            for anomaly in medium_severity[:3]:
                event_desc = (
                    f"Abnormal Volume: {anomaly.ticker} "
                    f"({anomaly.volume_ratio:.1f}x avg volume). "
                    f"ì ì¬ì  ì •ë³´ ë¹„ëŒ€ì¹­ ê°€ëŠ¥ì„±."
                )
                result.events_detected.append({
                    'type': 'VOLUME_ANOMALY',
                    'importance': 'MEDIUM',
                    'description': event_desc,
                    'ticker': anomaly.ticker,
                    'volume_ratio': anomaly.volume_ratio
                })

            # Top Movers í‘œì‹œ (ê°•ì œ ê°ì§€ - ì´ìƒì´ ì—†ì–´ë„ ìƒìœ„ ì¢…ëª© í‘œì‹œ)
            if vol_result.top_movers:
                print(f"      âœ“ Top Movers: {vol_result.top_movers_summary}")
                for mover in vol_result.top_movers[:3]:
                    print(f"        â†’ {mover.ticker}: {mover.volume_ratio:.2f}x (price {mover.price_change_1d:+.1f}%)")

                # ì´ìƒì´ ì—†ì„ ë•Œ Top Moversë¥¼ Eventsì— ì¶”ê°€ (Debugìš©)
                if vol_result.anomalies_detected == 0:
                    result.events_detected.append({
                        'type': 'TOP_MOVERS_DEBUG',
                        'importance': 'INFO',
                        'description': vol_result.top_movers_summary,
                        'top_movers': [m.to_dict() for m in vol_result.top_movers]
                    })

            if vol_result.warnings:
                for w in vol_result.warnings[:2]:
                    print(f"      âš  {w[:80]}")

        except Exception as e:
            print(f"      âœ— Volume analysis error: {e}")

    # 2.12 Event Tracking (ê±°ë˜ëŸ‰ ì´ìƒ â†’ ë‰´ìŠ¤ ì—­ì¶”ì )
    if not quick_mode:
        print("\n[2.12] Event tracking (anomaly â†’ news)...")
        try:
            event_tracker = EventTracker(use_perplexity=True)

            # ì£¼ìš” í‹°ì»¤ì— ëŒ€í•´ ì´ìƒ íƒì§€ ë° ë‰´ìŠ¤ ê²€ìƒ‰
            tracking_tickers = list(market_data.keys())[:10]  # ìƒìœ„ 10ê°œ
            tracking_result = await event_tracker.track_anomaly_events(
                tickers=tracking_tickers,
                days=14,
                max_events=5
            )

            result.event_tracking = {
                'anomalies_found': tracking_result.anomalies_found,
                'events_matched': tracking_result.events_matched,
                'event_types': tracking_result.event_type_distribution
            }
            result.tracked_events = [e.to_dict() for e in tracking_result.tracked_events]

            print(f"      âœ“ Anomalies: {tracking_result.anomalies_found}")
            print(f"      âœ“ Events Matched: {tracking_result.events_matched}")

            if tracking_result.tracked_events:
                for e in tracking_result.tracked_events[:3]:
                    if e.news_found:
                        print(f"        â†’ [{e.ticker}] {e.timestamp}: {e.event_type} ({e.sentiment})")
                        print(f"          {e.news_summary[:60]}...")

        except Exception as e:
            print(f"      âœ— Event tracking error: {e}")

    # ========================================================================
    # Phase 3: Multi-Agent Debate
    # ========================================================================
    print("\n" + "=" * 50)
    print("PHASE 3: MULTI-AGENT DEBATE")
    print("=" * 50)

    # 3.1 FULL Mode (Historical 365ì¼)
    print("\n[3.1] Running FULL mode analysis...")
    try:
        orchestrator_full = MetaOrchestrator(verbose=False)
        query = "Analyze current market conditions, risks, and generate trading signals"
        result_full = await orchestrator_full.run_with_debate(query, market_data)

        # consensusëŠ” í† í”½ë³„ ë”•ì…”ë„ˆë¦¬ â†’ ì¢…í•© ê³„ì‚° í•„ìš”
        consensus_by_topic = result_full.get('debate', {}).get('consensus', {})
        analysis_data = result_full.get('analysis', {})

        if consensus_by_topic:
            # ëª¨ë“  í† í”½ì˜ confidence í‰ê·  ê³„ì‚°
            confidences = [c.get('confidence', 0.5) for c in consensus_by_topic.values()]
            full_confidence = sum(confidences) / len(confidences) if confidences else 0.5

            # í¬ì§€ì…˜ ê²°ì •: analysisì˜ regime + risk ê¸°ë°˜
            regime = analysis_data.get('current_regime', 'NEUTRAL')
            risk_score = analysis_data.get('total_risk_score', 50)
            regime_conf = analysis_data.get('regime_confidence', 50) / 100

            if regime == 'BULL' and risk_score < 30:
                result.full_mode_position = 'BULLISH'
                full_confidence = max(full_confidence, regime_conf)
            elif regime == 'BEAR' or risk_score > 70:
                result.full_mode_position = 'BEARISH'
                full_confidence = max(full_confidence, regime_conf)
            else:
                result.full_mode_position = 'NEUTRAL'

            full_dissent = []
            full_strong_dissent = False
        else:
            result.full_mode_position = 'NEUTRAL'
            full_confidence = 0.5
            full_dissent = []
            full_strong_dissent = False

        print(f"      âœ“ FULL Position: {result.full_mode_position}")
        print(f"      âœ“ Confidence: {full_confidence:.0%}")
        print(f"      âœ“ Regime: {analysis_data.get('current_regime', 'N/A')}, Risk: {analysis_data.get('total_risk_score', 0):.1f}")
        if full_dissent:
            print(f"      âš  Dissent Records: {len(full_dissent)}")
    except Exception as e:
        print(f"      âœ— FULL mode error: {e}")
        full_confidence = 0.5
        full_dissent = []
        full_strong_dissent = False

    # 3.2 REFERENCE Mode (Recent 90ì¼)
    print("\n[3.2] Running REFERENCE mode analysis...")
    try:
        dm_ref = DataManager(lookback_days=90)
        market_data_ref, _ = dm_ref.collect_all(tickers_config)

        orchestrator_ref = MetaOrchestrator(verbose=False)
        result_ref = await orchestrator_ref.run_with_debate(query, market_data_ref)

        # consensusëŠ” í† í”½ë³„ ë”•ì…”ë„ˆë¦¬ â†’ ì¢…í•© ê³„ì‚° í•„ìš”
        consensus_by_topic_ref = result_ref.get('debate', {}).get('consensus', {})
        analysis_data_ref = result_ref.get('analysis', {})

        if consensus_by_topic_ref:
            # ëª¨ë“  í† í”½ì˜ confidence í‰ê·  ê³„ì‚°
            confidences_ref = [c.get('confidence', 0.5) for c in consensus_by_topic_ref.values()]
            ref_confidence = sum(confidences_ref) / len(confidences_ref) if confidences_ref else 0.5

            # í¬ì§€ì…˜ ê²°ì •: analysisì˜ regime + risk ê¸°ë°˜
            regime_ref = analysis_data_ref.get('current_regime', 'NEUTRAL')
            risk_score_ref = analysis_data_ref.get('total_risk_score', 50)
            regime_conf_ref = analysis_data_ref.get('regime_confidence', 50) / 100

            if regime_ref == 'BULL' and risk_score_ref < 30:
                result.reference_mode_position = 'BULLISH'
                ref_confidence = max(ref_confidence, regime_conf_ref)
            elif regime_ref == 'BEAR' or risk_score_ref > 70:
                result.reference_mode_position = 'BEARISH'
                ref_confidence = max(ref_confidence, regime_conf_ref)
            else:
                result.reference_mode_position = 'NEUTRAL'

            ref_dissent = []
            ref_strong_dissent = False
        else:
            result.reference_mode_position = 'NEUTRAL'
            ref_confidence = 0.5
            ref_dissent = []
            ref_strong_dissent = False

        print(f"      âœ“ REFERENCE Position: {result.reference_mode_position}")
        print(f"      âœ“ Confidence: {ref_confidence:.0%}")
        print(f"      âœ“ Regime: {analysis_data_ref.get('current_regime', 'N/A')}, Risk: {analysis_data_ref.get('total_risk_score', 0):.1f}")
        if ref_dissent:
            print(f"      âš  Dissent Records: {len(ref_dissent)}")
    except Exception as e:
        print(f"      âœ— REFERENCE mode error: {e}")
        ref_confidence = 0.5
        ref_dissent = []
        ref_strong_dissent = False

    # 3.3 ëª¨ë“œ ë¹„êµ
    print("\n[3.3] Comparing modes...")
    result.modes_agree = (result.full_mode_position == result.reference_mode_position)
    result.dissent_records = full_dissent + ref_dissent
    result.has_strong_dissent = full_strong_dissent or ref_strong_dissent

    # Devil's Advocate ë…¼ê±° ì¶”ì¶œ (v2.1.2 - Elicit Enhancement)
    result.devils_advocate_arguments = _extract_devils_advocate_arguments(result.dissent_records)
    if result.devils_advocate_arguments:
        print(f"      âœ“ Devil's Advocate: {len(result.devils_advocate_arguments)} counter-arguments extracted")

    # Dual Mode ë¶„ì„ê¸°ë¡œ ìµœì¢… ê¶Œê³ 
    analyzer = DualModeAnalyzer()
    full_mode = ModeResult(
        mode=AnalysisMode.FULL,
        consensus=None,
        confidence=full_confidence,
        position=result.full_mode_position,
        dissent_count=len(full_dissent),
        has_strong_dissent=full_strong_dissent
    )
    ref_mode = ModeResult(
        mode=AnalysisMode.REFERENCE,
        consensus=None,
        confidence=ref_confidence,
        position=result.reference_mode_position,
        dissent_count=len(ref_dissent),
        has_strong_dissent=ref_strong_dissent
    )
    comparison = analyzer.compare_modes(full_mode, ref_mode)

    result.final_recommendation = comparison.recommended_action
    result.confidence = (full_confidence + ref_confidence) / 2
    result.risk_level = comparison.risk_level

    if not result.modes_agree:
        result.warnings.append(f"Mode divergence: FULL={result.full_mode_position}, REF={result.reference_mode_position}")
    if result.has_strong_dissent:
        result.warnings.append("Strong dissent exists - review carefully")

    print(f"      âœ“ Modes Agree: {'Yes' if result.modes_agree else 'NO'}")
    print(f"      âœ“ Final Recommendation: {result.final_recommendation}")
    print(f"      âœ“ Risk Level: {result.risk_level}")

    # 3.4 Adaptive Portfolio Agents + Validation Loop
    if not quick_mode:
        print("\n[3.4] Adaptive portfolio agents...")
        try:
            # ì‹œì¥ ìƒí™© êµ¬ì„±
            vix_level = indicators_summary.vix.current if 'indicators_summary' in dir() else 20.0
            market_condition = MarketCondition(
                regime=result.regime.get('regime', 'NEUTRAL'),
                risk_score=result.risk_score,
                vix_level=vix_level,
                liquidity_signal=result.liquidity_signal,
                vpin_alert=any(sig.get('alert_level') == 'HIGH' for sig in result.realtime_signals) if result.realtime_signals else False,
                bubble_status=result.bubble_risk.overall_status if result.bubble_risk else 'NONE'
            )

            print(f"      Market: {market_condition.regime}, Risk: {market_condition.risk_score:.1f}")
            print(f"      Urgency: {market_condition.urgency_score():.1f}, Opportunity: {market_condition.opportunity_score():.1f}")

            # 3ê°œ ì—ì´ì „íŠ¸ ì‹¤í–‰
            agent_manager = AdaptiveAgentManager()
            portfolios = await agent_manager.run_all_agents(market_condition, list(market_data.keys())[:15])

            result.adaptive_portfolios = {
                name: {
                    'risk_level': p.adjusted_risk_level,
                    'action': p.action,
                    'allocations': p.allocations,
                    'rationale': p.rationale
                }
                for name, p in portfolios.items()
            }

            for name, p in portfolios.items():
                print(f"      â†’ {name}: {p.action} (risk={p.adjusted_risk_level})")

            # Validation Loop (Aggressive ì—ì´ì „íŠ¸ ê²°ì • ê²€ì¦)
            print("\n[3.4.1] Validation loop (Claude + Perplexity)...")
            aggressive_decision = {
                'agent_type': 'aggressive',
                'action': portfolios['aggressive'].action,
                'risk_level': portfolios['aggressive'].adjusted_risk_level,
                'rationale': portfolios['aggressive'].rationale,
                'allocations': portfolios['aggressive'].allocations
            }

            loop_manager = ValidationLoopManager(max_rounds=2)
            loop_result = await loop_manager.run_validation_loop(
                original_decision=aggressive_decision,
                market_condition={
                    'regime': market_condition.regime,
                    'risk_score': market_condition.risk_score,
                    'vix_level': market_condition.vix_level,
                    'volatility': result.regime.get('volatility', 'Medium'),
                    'liquidity_signal': market_condition.liquidity_signal,
                    'vpin_alert': market_condition.vpin_alert
                }
            )

            result.validation_loop_result = {
                'rounds_completed': loop_result.rounds_completed,
                'original_risk': aggressive_decision['risk_level'],
                'final_risk': loop_result.final_decision.get('risk_level'),
                'modifications': len(loop_result.modification_history),
                'consensus_reached': loop_result.consensus_reached,
                'summary': loop_result.summary
            }

            print(f"      âœ“ Validation: {loop_result.rounds_completed} rounds")
            print(f"      âœ“ Risk: {aggressive_decision['risk_level']} â†’ {loop_result.final_decision.get('risk_level')}")
            print(f"      âœ“ Consensus: {'Yes' if loop_result.consensus_reached else 'No'}")

        except Exception as e:
            print(f"      âœ— Adaptive agents error: {e}")
            import traceback
            traceback.print_exc()

    # ========================================================================
    # Phase 4: Real-time VPIN Monitoring (Optional)
    # ========================================================================
    if enable_realtime:
        print("\n" + "=" * 50)
        print("PHASE 4: REAL-TIME VPIN MONITORING")
        print("=" * 50)

        print(f"\n[4.1] Starting Real-time VPIN Monitor ({realtime_duration}s)...")
        print("      Symbols: BTCUSDT, ETHUSDT")
        print("      VPIN Thresholds: 0.5 (elevated), 0.6 (high), 0.7 (extreme)")

        try:
            # ìƒˆë¡œìš´ RealtimeVPINMonitor ì‚¬ìš©
            monitor_result = await run_realtime_monitor(
                symbols=['BTCUSDT', 'ETHUSDT'],
                duration=realtime_duration,
                verbose=True
            )

            # ê²°ê³¼ ì €ì¥
            vpin_history = monitor_result.get('vpin_history', {})
            all_signals = []

            for symbol, history in vpin_history.items():
                for h in history:
                    all_signals.append({
                        'timestamp': h['timestamp'],
                        'symbol': symbol,
                        'avg_vpin': h['avg_vpin'],
                        'max_vpin': h['max_vpin'],
                        'samples': h['samples']
                    })

            result.realtime_signals = all_signals[-20:]  # ë§ˆì§€ë§‰ 20ê°œ

            # ìš”ì•½ ì¶œë ¥
            stream_stats = monitor_result.get('stream_stats', {})
            alerts = monitor_result.get('alerts_fired', 0)

            print(f"\n[4.2] Real-time Summary:")
            print(f"      âœ“ Alerts Fired: {alerts}")
            print(f"      âœ“ 1-min VPIN Samples: {len(all_signals)}")
            print(f"      âœ“ Messages Processed: {stream_stats.get('messages_received', 0):,}")

        except Exception as e:
            print(f"      âœ— Real-time error: {e}")
            import traceback
            traceback.print_exc()

    # Correlation ë°ì´í„° ì €ì¥
    result.correlation_matrix = correlation_matrix
    result.correlation_tickers = correlation_tickers

    # ========================================================================
    # Phase 5: Database Storage
    # ========================================================================
    print("\n" + "=" * 50)
    print("PHASE 5: DATABASE STORAGE")
    print("=" * 50)

    # 5.1 ì´ë²¤íŠ¸ DB ì €ì¥
    print("\n[5.1] Saving to Event Database...")
    try:
        event_db = EventDatabase('data/events.db')

        # ì´ë²¤íŠ¸ ì €ì¥
        for event in result.events_detected:
            event_db.save_detected_event({
                'event_type': event['type'],
                'importance': event['importance'],
                'description': event['description'],
                'timestamp': result.timestamp,
            })

        # ë§ˆì¼“ ìŠ¤ëƒ…ìƒ· ì €ì¥ (snapshot_id í•„ìˆ˜)
        import uuid
        snapshot_id = str(uuid.uuid4())[:8]

        def get_latest_price(ticker: str) -> float:
            """DataFrameì—ì„œ ìµœì‹  ê°€ê²© ì¶”ì¶œ"""
            if ticker not in market_data:
                return 0.0
            df = market_data[ticker]
            if hasattr(df, 'iloc') and len(df) > 0:
                return float(df['Close'].iloc[-1]) if 'Close' in df.columns else 0.0
            return 0.0

        event_db.save_market_snapshot({
            'snapshot_id': snapshot_id,
            'timestamp': result.timestamp,
            'spy_price': get_latest_price('SPY'),
            'spy_change_1d': 0.0,
            'spy_change_5d': 0.0,
            'spy_vs_ma20': 0.0,
            'qqq_price': get_latest_price('QQQ'),
            'iwm_price': get_latest_price('IWM'),
            'tlt_price': get_latest_price('TLT'),
            'gld_price': get_latest_price('GLD'),
            'vix_level': get_latest_price('^VIX'),
            'vix_percentile': 50.0,
            'rsi_14': 50.0,
            'macd_signal': 'neutral',
            'trend': result.regime.get('trend', 'unknown'),
            'volatility_regime': result.regime.get('volatility', 'normal'),
            'put_call_ratio': 1.0,
            'fear_greed_index': 50.0,
            'days_to_fomc': 0,
            'days_to_cpi': 0,
            'days_to_nfp': 0,
        })

        print(f"      âœ“ Saved {len(result.events_detected)} events")
        print(f"      âœ“ Saved market snapshot (ID: {snapshot_id})")
    except Exception as e:
        print(f"      âœ— Event DB error: {e}")

    # 5.2 ì‹œê·¸ë„ DB ì €ì¥
    print("\n[5.2] Saving to Signal Database...")
    try:
        signal_db = SignalDatabase('outputs/realtime_signals.db')

        from lib.realtime_pipeline import IntegratedSignal
        integrated_signal = IntegratedSignal(
            timestamp=datetime.now(),
            symbol='INTEGRATED',
            liquidity_regime=result.fred_summary.get('liquidity_regime', 'Unknown'),
            rrp_delta=result.fred_summary.get('rrp_delta', 0),
            tga_delta=result.fred_summary.get('tga_delta', 0),
            net_liquidity=result.fred_summary.get('net_liquidity', 0),
            macro_signal=result.liquidity_signal.lower(),
            ofi=0,
            vpin=0,
            depth_ratio=1.0,
            micro_signal='neutral',
            combined_signal=result.final_recommendation.lower(),
            confidence=result.confidence,
            action=result.final_recommendation.lower(),
            alerts=result.warnings
        )
        signal_db.save_signal(integrated_signal)
        print(f"      âœ“ Saved integrated signal")
    except Exception as e:
        print(f"      âœ— Signal DB error: {e}")

    # 5.2.1 ì˜ˆì¸¡ DB ì €ì¥ (ê²€ì¦ìš©)
    print("\n[5.2.1] Saving to Predictions Database...")
    try:
        saved_ids = save_eimas_result(result)
        print(f"      âœ“ Saved predictions: {list(saved_ids.keys())}")
    except Exception as e:
        print(f"      âœ— Predictions DB error: {e}")

    # 5.3 ê²°ê³¼ JSON ì €ì¥
    if not cron_mode:
        print("\n[5.3] Saving results...")

    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ì„¤ì • (íŒŒë¼ë¯¸í„° ë˜ëŠ” ê¸°ë³¸ê°’)
    if isinstance(output_dir, str):
        output_dir = Path(output_dir) if os.path.isabs(output_dir) else Path(__file__).parent / output_dir
    output_dir.mkdir(exist_ok=True)

    timestamp_str = datetime.now().strftime("%Y%m%d_%H%M%S")

    # JSON ì €ì¥
    output_file = output_dir / f"integrated_{timestamp_str}.json"
    with open(output_file, 'w') as f:
        json.dump(result.to_dict(), f, indent=2, default=str)
    print(f"      - JSON: {output_file}")

    # Markdown ì €ì¥
    md_file = output_dir / f"integrated_{timestamp_str}.md"
    with open(md_file, 'w', encoding='utf-8') as f:
        f.write(result.to_markdown())
    print(f"      - MD: {md_file}")

    return result, market_data, output_file, md_file


async def run_ai_report(
    analysis_result,
    market_data: Dict,
    output_file: str
) -> Optional[str]:
    """
    Phase 6: AI Report Generation
    """
    print("\n" + "=" * 50)
    print("PHASE 6: AI REPORT GENERATION")
    print("=" * 50)

    try:
        from lib.ai_report_generator import AIReportGenerator

        generator = AIReportGenerator(verbose=True)

        print("\n[6.1] Generating AI-powered investment report...")
        report = await generator.generate(analysis_result.to_dict(), market_data)

        print("\n[6.2] Saving report...")
        report_path = await generator.save_report(report)

        print(f"\n      âœ“ AI Report saved: {report_path}")

        # ìš”ì•½ ì¶œë ¥
        print("\n" + "-" * 50)
        print("ğŸ“ AI REPORT HIGHLIGHTS")
        print("-" * 50)

        if report.notable_stocks:
            print("\nì£¼ëª©í•  ì¢…ëª©:")
            for stock in report.notable_stocks[:3]:
                print(f"  â€¢ {stock.ticker}: {stock.notable_reason}")

        print(f"\nìµœì¢… ì œì•ˆ: {report.final_recommendation[:200]}...")

        if report.action_items:
            print("\nì•¡ì…˜ ì•„ì´í…œ:")
            for item in report.action_items[:3]:
                print(f"  â€¢ {item}")

        return report_path

    except Exception as e:
        print(f"      âœ— AI Report error: {e}")
        return None


async def run_full_pipeline(
    enable_realtime: bool = False,
    realtime_duration: int = 30,
    quick_mode: bool = False,
    generate_report: bool = False,
    target_ticker: str = None,
    output_dir: str = 'outputs',
    cron_mode: bool = False
):
    """
    ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (ë¶„ì„ + ë¦¬í¬íŠ¸)

    Args:
        enable_realtime: ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° í™œì„±í™”
        realtime_duration: ìŠ¤íŠ¸ë¦¬ë° ì§€ì† ì‹œê°„ (ì´ˆ)
        quick_mode: ë¹ ë¥¸ ë¶„ì„ ëª¨ë“œ
        generate_report: AI ë¦¬í¬íŠ¸ ìƒì„±
        target_ticker: íŠ¹ì • í‹°ì»¤ ì¤‘ì‹¬ ë¶„ì„
        output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬
        cron_mode: ì„œë²„ ìë™í™” ëª¨ë“œ (ì‹œê°í™” ì—†ìŒ)
    """
    start_time = datetime.now()

    # Phase 1-5: ë¶„ì„ ì‹¤í–‰
    result, market_data, output_file, md_file = await run_integrated_pipeline(
        enable_realtime=enable_realtime,
        realtime_duration=realtime_duration,
        quick_mode=quick_mode,
        output_dir=output_dir,
        cron_mode=cron_mode
    )

    # Phase 6: AI ë¦¬í¬íŠ¸ ìƒì„± (ì˜µì…˜)
    report_path = None
    if generate_report:
        report_path = await run_ai_report(result, market_data, output_file)

    # Phase 7: Whitening & Fact Check (ì˜µì…˜)
    if generate_report and not quick_mode:
        print("\n" + "=" * 50)
        print("PHASE 7: WHITENING & FACT CHECK")
        print("=" * 50)

        # 7.1 Whitening Engine - ê²°ê³¼ ê²½ì œí•™ì  í•´ì„
        print("\n[7.1] Economic whitening analysis...")
        try:
            whitening = WhiteningEngine()

            # í¬íŠ¸í´ë¦¬ì˜¤ ê²°ê³¼ êµ¬ì„±
            portfolio_result = {
                'allocation': result.portfolio_weights if result.portfolio_weights else {'SPY': 0.3, 'QQQ': 0.2, 'TLT': 0.15},
                'changes': {},
                'returns': {}
            }

            explanation = whitening.explain_allocation(portfolio_result)
            result.whitening_summary = explanation.summary

            print(f"      âœ“ Summary: {explanation.summary[:100]}...")
            print(f"      âœ“ Key Drivers: {len(explanation.key_drivers)}")
            print(f"      âœ“ Confidence: {explanation.overall_confidence:.0%}")
        except Exception as e:
            print(f"      âœ— Whitening error: {e}")

        # 7.2 Autonomous Fact Checker
        print("\n[7.2] Fact checking AI outputs...")
        try:
            fact_checker = AutonomousFactChecker(use_perplexity=False, verbose=False)

            # ê²€ì¦í•  í…ìŠ¤íŠ¸ êµ¬ì„±
            check_text = f"""
            Current regime is {result.regime.get('regime', 'Unknown')}.
            Risk score is {result.risk_score:.1f} out of 100.
            Net liquidity is {result.fred_summary.get('net_liquidity', 0):.0f} billion dollars.
            The recommendation is {result.final_recommendation} with {result.confidence:.0%} confidence.
            """

            check_result = await fact_checker.verify_document(check_text, max_claims=5)
            result.fact_check_grade = check_result['summary']['grade']

            print(f"      âœ“ Claims checked: {check_result['summary']['total_claims']}")
            print(f"      âœ“ Verified: {check_result['summary']['verified']}")
            print(f"      âœ“ Grade: {check_result['summary']['grade']} ({check_result['summary']['grade_description']})")
        except Exception as e:
            print(f"      âœ— Fact check error: {e}")

    # Summary
    elapsed = (datetime.now() - start_time).total_seconds()

    print("\n" + "=" * 70)
    print("EIMAS INTEGRATED ANALYSIS COMPLETE")
    print("=" * 70)
    print(f"Total time: {elapsed:.1f}s")
    print()
    print("ğŸ“Š DATA SUMMARY")
    print(f"   FRED: RRP=${result.fred_summary.get('rrp', 0):.0f}B, Net Liq=${result.fred_summary.get('net_liquidity', 0):.0f}B")
    print(f"   Market: {result.market_data_count} tickers, Crypto: {result.crypto_data_count} tickers")
    print()
    print("ğŸ“ˆ ANALYSIS SUMMARY")
    print(f"   Regime: {result.regime.get('regime', 'Unknown')}")
    print(f"   Risk Score: {result.risk_score:.1f}/100")
    print(f"   Events: {len(result.events_detected)} detected")
    print()
    print("ğŸ¤– AGENT DEBATE")
    print(f"   FULL Mode: {result.full_mode_position}")
    print(f"   REFERENCE Mode: {result.reference_mode_position}")
    print(f"   Modes Agree: {'âœ“' if result.modes_agree else 'âœ—'}")
    print(f"   Dissent Records: {len(result.dissent_records)}")
    print()
    print("ğŸ”¬ ADVANCED ANALYSIS")
    print(f"   Genius Act Regime: {result.genius_act_regime}")
    print(f"   Genius Act Signals: {len(result.genius_act_signals)}")
    if result.shock_propagation:
        print(f"   Shock Graph: {result.shock_propagation.get('nodes', 0)} nodes, {result.shock_propagation.get('edges', 0)} edges")
    if result.theme_etf_analysis:
        print(f"   Theme ETF: {result.theme_etf_analysis.get('theme', 'N/A')}")
    if result.portfolio_weights:
        top_3 = sorted(result.portfolio_weights.items(), key=lambda x: x[1], reverse=True)[:3]
        weights_str = ', '.join([f"{t}:{w:.1%}" for t, w in top_3])
        print(f"   GC-HRP Portfolio: {weights_str}")
    if result.integrated_signals:
        print(f"   Integrated Signals: {len(result.integrated_signals)}")
    if result.volume_anomalies:
        high_sev = len([a for a in result.volume_anomalies if a.get('severity') in ['HIGH', 'CRITICAL']])
        print(f"   Volume Anomalies: {len(result.volume_anomalies)} detected ({high_sev} high severity)")
    if result.whitening_summary:
        print(f"   Whitening: {result.whitening_summary[:60]}...")
    if result.fact_check_grade != "N/A":
        print(f"   Fact Check Grade: {result.fact_check_grade}")
    print()
    print("ğŸ¯ FINAL RECOMMENDATION")
    print(f"   Action: {result.final_recommendation}")
    print(f"   Confidence: {result.confidence:.0%}")
    print(f"   Risk Level: {result.risk_level}")

    if result.warnings:
        print()
        print("âš ï¸  WARNINGS")
        for w in result.warnings:
            print(f"   - {w}")

    print()
    print(f"Results saved:")
    print(f"   JSON: {output_file}")
    print(f"   MD:   {md_file}")
    if report_path:
        print(f"   AI Report: {report_path}")
    print("=" * 70)

    return result


# ============================================================================
# CLI
# ============================================================================

def parse_args():
    parser = argparse.ArgumentParser(
        description='EIMAS - Economic Intelligence Multi-Agent System',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog='''
Examples:
    python main.py                         # ì „ì²´ íŒŒì´í”„ë¼ì¸ (ê¸°ë³¸)
    python main.py --mode full             # ì „ì²´ ë¶„ì„
    python main.py --mode quick            # ë¹ ë¥¸ ë¶„ì„ (Phase 2.3-2.10 ìŠ¤í‚µ)
    python main.py --mode full --target NVDA   # NVDA ì¤‘ì‹¬ ë¶„ì„
    python main.py --report                # AI ì œì•ˆì„œ ìƒì„± í¬í•¨
    python main.py --realtime --duration 60    # 60ì´ˆ ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°
    python main.py --cron                  # ì„œë²„ ìë™í™” ëª¨ë“œ (ë°±ê·¸ë¼ìš´ë“œ)
    python main.py --cron --output /data/reports  # ì§€ì • ë””ë ‰í† ë¦¬ì— ì €ì¥

Terminal Automation:
    # ë§¤ì¼ ì˜¤ì „ 9ì‹œ ìë™ ì‹¤í–‰ (crontab)
    0 9 * * * cd /path/to/eimas && python main.py --cron >> /var/log/eimas.log 2>&1
        '''
    )

    # ëª¨ë“œ ì„ íƒ
    parser.add_argument(
        '--mode', '-m',
        choices=['full', 'quick', 'report'],
        default='full',
        help='Analysis mode: full (default), quick (fast), report (includes AI report)'
    )

    # íƒ€ê²Ÿ í‹°ì»¤ (ì„ íƒ)
    parser.add_argument(
        '--target', '-t',
        type=str,
        default=None,
        help='Target ticker for focused analysis (e.g., NVDA, AAPL)'
    )

    # ì„œë²„ ìë™í™” ëª¨ë“œ (cron)
    parser.add_argument(
        '--cron',
        action='store_true',
        help='Cron mode: no visualization, background execution, markdown report only'
    )

    # ì¶œë ¥ ë””ë ‰í† ë¦¬
    parser.add_argument(
        '--output', '-o',
        type=str,
        default='outputs',
        help='Output directory for reports (default: outputs)'
    )

    # ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°
    parser.add_argument(
        '--realtime', '-r',
        action='store_true',
        help='Enable real-time Binance streaming'
    )

    parser.add_argument(
        '--duration', '-d',
        type=int,
        default=30,
        help='Real-time streaming duration in seconds (default: 30)'
    )

    # ë¹ ë¥¸ ëª¨ë“œ (í•˜ìœ„ í˜¸í™˜)
    parser.add_argument(
        '--quick', '-q',
        action='store_true',
        help='Quick mode (alias for --mode quick)'
    )

    # AI ë¦¬í¬íŠ¸ ìƒì„± (í•˜ìœ„ í˜¸í™˜)
    parser.add_argument(
        '--report',
        action='store_true',
        help='Generate AI-powered investment report (alias for --mode report)'
    )

    # ìƒì„¸ ë¡œê¹…
    parser.add_argument(
        '--verbose', '-v',
        action='store_true',
        help='Verbose logging'
    )

    # ë²„ì „ ì •ë³´
    parser.add_argument(
        '--version',
        action='version',
        version='EIMAS v2.1.0 (Real-World Agent Edition)'
    )

    return parser.parse_args()


async def main():
    args = parse_args()

    # ë¡œê¹… ì„¤ì •
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)

    # Cron ëª¨ë“œ: ìµœì†Œ ì¶œë ¥
    if args.cron:
        logging.getLogger().setLevel(logging.WARNING)
        print(f"[CRON] EIMAS starting at {datetime.now().isoformat()}")

    # ëª¨ë“œ ê²°ì • (í•˜ìœ„ í˜¸í™˜ì„±)
    quick_mode = args.quick or args.mode == 'quick'
    generate_report = args.report or args.mode == 'report'

    # Cron ëª¨ë“œì—ì„œëŠ” realtime ë¹„í™œì„±í™”
    enable_realtime = args.realtime and not args.cron

    # íƒ€ê²Ÿ í‹°ì»¤ ì²˜ë¦¬
    target_ticker = args.target
    if target_ticker:
        print(f"[INFO] Focused analysis on: {target_ticker}")

    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ì„¤ì •
    output_dir = args.output
    if output_dir != 'outputs':
        print(f"[INFO] Output directory: {output_dir}")
        os.makedirs(output_dir, exist_ok=True)

    # íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
    result = await run_full_pipeline(
        enable_realtime=enable_realtime,
        realtime_duration=args.duration,
        quick_mode=quick_mode,
        generate_report=generate_report,
        target_ticker=target_ticker,
        output_dir=output_dir,
        cron_mode=args.cron
    )

    # Cron ëª¨ë“œ: ì™„ë£Œ ë©”ì‹œì§€
    if args.cron:
        print(f"[CRON] EIMAS completed at {datetime.now().isoformat()}")
        if result:
            print(f"[CRON] Recommendation: {result.final_recommendation}")
            print(f"[CRON] Confidence: {result.confidence:.0%}")
            print(f"[CRON] Risk Level: {result.risk_level}")

    return result


if __name__ == "__main__":
    asyncio.run(main())
